{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "zsh:1: no matches found: pinecone[grpc]\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: googleapis-common-protos in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (1.65.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from googleapis-common-protos) (5.28.2)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: protoc_gen_openapiv2 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (0.0.1)\n",
            "Requirement already satisfied: googleapis-common-protos in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from protoc_gen_openapiv2) (1.65.0)\n",
            "Requirement already satisfied: protobuf>=4.21.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from protoc_gen_openapiv2) (5.28.2)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: langchain_pinecone in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (0.2.0)\n",
            "Requirement already satisfied: numpy<2,>=1 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langchain_pinecone) (1.26.4)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langchain_pinecone) (0.3.5)\n",
            "Requirement already satisfied: pinecone-client<6.0.0,>=5.0.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langchain_pinecone) (5.0.1)\n",
            "Requirement already satisfied: aiohttp<3.10,>=3.9.5 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langchain_pinecone) (3.9.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from aiohttp<3.10,>=3.9.5->langchain_pinecone) (1.12.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from aiohttp<3.10,>=3.9.5->langchain_pinecone) (4.0.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from aiohttp<3.10,>=3.9.5->langchain_pinecone) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from aiohttp<3.10,>=3.9.5->langchain_pinecone) (6.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from aiohttp<3.10,>=3.9.5->langchain_pinecone) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from aiohttp<3.10,>=3.9.5->langchain_pinecone) (24.2.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4,>=0.3->langchain_pinecone) (0.1.128)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4,>=0.3->langchain_pinecone) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4,>=0.3->langchain_pinecone) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4,>=0.3->langchain_pinecone) (4.12.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4,>=0.3->langchain_pinecone) (24.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4,>=0.3->langchain_pinecone) (1.33)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4,>=0.3->langchain_pinecone) (2.9.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3->langchain_pinecone) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_pinecone) (0.27.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_pinecone) (2.32.3)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_pinecone) (3.10.7)\n",
            "Requirement already satisfied: idna in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_pinecone) (3.10)\n",
            "Requirement already satisfied: certifi in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_pinecone) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_pinecone) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_pinecone) (1.3.1)\n",
            "Requirement already satisfied: anyio in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_pinecone) (4.6.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_pinecone) (0.14.0)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from pinecone-client<6.0.0,>=5.0.0->langchain_pinecone) (4.66.5)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from pinecone-client<6.0.0,>=5.0.0->langchain_pinecone) (0.0.7)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from pinecone-client<6.0.0,>=5.0.0->langchain_pinecone) (2.2.3)\n",
            "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from pinecone-client<6.0.0,>=5.0.0->langchain_pinecone) (1.1.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3->langchain_pinecone) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3->langchain_pinecone) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_pinecone) (3.3.2)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_pinecone) (1.2.2)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: langchain-community in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (0.3.0)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langchain-community) (0.3.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langchain-community) (3.9.5)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langchain-community) (0.1.128)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langchain-community) (2.0.35)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langchain-community) (2.5.2)\n",
            "Requirement already satisfied: numpy<2,>=1 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langchain-community) (0.3.5)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.12.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langchain<0.4.0,>=0.3.0->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langchain<0.4.0,>=0.3.0->langchain-community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-community) (24.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langsmith<0.2.0,>=0.1.112->langchain-community) (3.10.7)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from langsmith<0.2.0,>=0.1.112->langchain-community) (0.27.2)\n",
            "Requirement already satisfied: anyio in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (4.6.0)\n",
            "Requirement already satisfied: idna in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (3.10)\n",
            "Requirement already satisfied: certifi in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (2024.8.30)\n",
            "Requirement already satisfied: sniffio in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.0->langchain-community) (2.23.4)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (1.2.2)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: poppler-utils in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (0.1.0)\n",
            "Requirement already satisfied: Click>=7.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from poppler-utils) (8.1.7)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "zsh:1: no matches found: unstructured[local-inference]\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
            "   command: /Library/Developer/CommandLineTools/usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-wheel-ibwwa0j4\n",
            "       cwd: /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/\n",
            "  Complete output (623 lines):\n",
            "  running bdist_wheel\n",
            "  /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/utils/cpp_extension.py:495: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "    warnings.warn(msg.format('we could not find ninja.'))\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib.macosx-10.9-universal2-3.9\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/tools\n",
            "  copying tools/lightning_train_net.py -> build/lib.macosx-10.9-universal2-3.9/tools\n",
            "  copying tools/convert-torchvision-to-d2.py -> build/lib.macosx-10.9-universal2-3.9/tools\n",
            "  copying tools/benchmark.py -> build/lib.macosx-10.9-universal2-3.9/tools\n",
            "  copying tools/visualize_data.py -> build/lib.macosx-10.9-universal2-3.9/tools\n",
            "  copying tools/plain_train_net.py -> build/lib.macosx-10.9-universal2-3.9/tools\n",
            "  copying tools/__init__.py -> build/lib.macosx-10.9-universal2-3.9/tools\n",
            "  copying tools/visualize_json_results.py -> build/lib.macosx-10.9-universal2-3.9/tools\n",
            "  copying tools/analyze_model.py -> build/lib.macosx-10.9-universal2-3.9/tools\n",
            "  copying tools/lazyconfig_train_net.py -> build/lib.macosx-10.9-universal2-3.9/tools\n",
            "  copying tools/train_net.py -> build/lib.macosx-10.9-universal2-3.9/tools\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2\n",
            "  copying detectron2/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/checkpoint\n",
            "  copying detectron2/checkpoint/catalog.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/checkpoint\n",
            "  copying detectron2/checkpoint/c2_model_loading.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/checkpoint\n",
            "  copying detectron2/checkpoint/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/checkpoint\n",
            "  copying detectron2/checkpoint/detection_checkpoint.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/checkpoint\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "  copying detectron2/layers/deform_conv.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "  copying detectron2/layers/shape_spec.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "  copying detectron2/layers/roi_align.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "  copying detectron2/layers/roi_align_rotated.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "  copying detectron2/layers/nms.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "  copying detectron2/layers/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "  copying detectron2/layers/aspp.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "  copying detectron2/layers/mask_ops.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "  copying detectron2/layers/wrappers.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "  copying detectron2/layers/losses.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "  copying detectron2/layers/blocks.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "  copying detectron2/layers/batch_norm.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "  copying detectron2/layers/rotated_boxes.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/structures\n",
            "  copying detectron2/structures/instances.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/structures\n",
            "  copying detectron2/structures/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/structures\n",
            "  copying detectron2/structures/boxes.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/structures\n",
            "  copying detectron2/structures/keypoints.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/structures\n",
            "  copying detectron2/structures/masks.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/structures\n",
            "  copying detectron2/structures/image_list.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/structures\n",
            "  copying detectron2/structures/rotated_boxes.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/structures\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/config\n",
            "  copying detectron2/config/config.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/config\n",
            "  copying detectron2/config/compat.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/config\n",
            "  copying detectron2/config/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/config\n",
            "  copying detectron2/config/instantiate.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/config\n",
            "  copying detectron2/config/defaults.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/config\n",
            "  copying detectron2/config/lazy.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/config\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/projects\n",
            "  copying detectron2/projects/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "  copying detectron2/utils/serialize.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "  copying detectron2/utils/colormap.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "  copying detectron2/utils/env.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "  copying detectron2/utils/analysis.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "  copying detectron2/utils/comm.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "  copying detectron2/utils/memory.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "  copying detectron2/utils/video_visualizer.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "  copying detectron2/utils/registry.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "  copying detectron2/utils/events.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "  copying detectron2/utils/collect_env.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "  copying detectron2/utils/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "  copying detectron2/utils/logger.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "  copying detectron2/utils/file_io.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "  copying detectron2/utils/testing.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "  copying detectron2/utils/visualizer.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/solver\n",
            "  copying detectron2/solver/build.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/solver\n",
            "  copying detectron2/solver/lr_scheduler.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/solver\n",
            "  copying detectron2/solver/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/solver\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo\n",
            "  copying detectron2/model_zoo/model_zoo.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo\n",
            "  copying detectron2/model_zoo/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/modeling\n",
            "  copying detectron2/modeling/test_time_augmentation.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling\n",
            "  copying detectron2/modeling/poolers.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling\n",
            "  copying detectron2/modeling/matcher.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling\n",
            "  copying detectron2/modeling/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling\n",
            "  copying detectron2/modeling/box_regression.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling\n",
            "  copying detectron2/modeling/mmdet_wrapper.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling\n",
            "  copying detectron2/modeling/anchor_generator.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling\n",
            "  copying detectron2/modeling/sampling.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling\n",
            "  copying detectron2/modeling/postprocessing.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/evaluation\n",
            "  copying detectron2/evaluation/fast_eval_api.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/evaluation\n",
            "  copying detectron2/evaluation/panoptic_evaluation.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/evaluation\n",
            "  copying detectron2/evaluation/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/evaluation\n",
            "  copying detectron2/evaluation/cityscapes_evaluation.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/evaluation\n",
            "  copying detectron2/evaluation/coco_evaluation.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/evaluation\n",
            "  copying detectron2/evaluation/sem_seg_evaluation.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/evaluation\n",
            "  copying detectron2/evaluation/pascal_voc_evaluation.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/evaluation\n",
            "  copying detectron2/evaluation/lvis_evaluation.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/evaluation\n",
            "  copying detectron2/evaluation/testing.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/evaluation\n",
            "  copying detectron2/evaluation/evaluator.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/evaluation\n",
            "  copying detectron2/evaluation/rotated_coco_evaluation.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/evaluation\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/export\n",
            "  copying detectron2/export/c10.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/export\n",
            "  copying detectron2/export/caffe2_export.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/export\n",
            "  copying detectron2/export/flatten.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/export\n",
            "  copying detectron2/export/caffe2_patch.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/export\n",
            "  copying detectron2/export/caffe2_modeling.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/export\n",
            "  copying detectron2/export/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/export\n",
            "  copying detectron2/export/shared.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/export\n",
            "  copying detectron2/export/api.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/export\n",
            "  copying detectron2/export/caffe2_inference.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/export\n",
            "  copying detectron2/export/torchscript_patch.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/export\n",
            "  copying detectron2/export/torchscript.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/export\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/data\n",
            "  copying detectron2/data/catalog.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data\n",
            "  copying detectron2/data/build.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data\n",
            "  copying detectron2/data/benchmark.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data\n",
            "  copying detectron2/data/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data\n",
            "  copying detectron2/data/detection_utils.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data\n",
            "  copying detectron2/data/dataset_mapper.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data\n",
            "  copying detectron2/data/common.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/engine\n",
            "  copying detectron2/engine/hooks.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/engine\n",
            "  copying detectron2/engine/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/engine\n",
            "  copying detectron2/engine/train_loop.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/engine\n",
            "  copying detectron2/engine/launch.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/engine\n",
            "  copying detectron2/engine/defaults.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/engine\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/meta_arch\n",
            "  copying detectron2/modeling/meta_arch/build.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/meta_arch\n",
            "  copying detectron2/modeling/meta_arch/rcnn.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/meta_arch\n",
            "  copying detectron2/modeling/meta_arch/panoptic_fpn.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/meta_arch\n",
            "  copying detectron2/modeling/meta_arch/dense_detector.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/meta_arch\n",
            "  copying detectron2/modeling/meta_arch/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/meta_arch\n",
            "  copying detectron2/modeling/meta_arch/fcos.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/meta_arch\n",
            "  copying detectron2/modeling/meta_arch/retinanet.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/meta_arch\n",
            "  copying detectron2/modeling/meta_arch/semantic_seg.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/meta_arch\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/proposal_generator\n",
            "  copying detectron2/modeling/proposal_generator/build.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/proposal_generator\n",
            "  copying detectron2/modeling/proposal_generator/rpn.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/proposal_generator\n",
            "  copying detectron2/modeling/proposal_generator/rrpn.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/proposal_generator\n",
            "  copying detectron2/modeling/proposal_generator/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/proposal_generator\n",
            "  copying detectron2/modeling/proposal_generator/proposal_utils.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/proposal_generator\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/roi_heads\n",
            "  copying detectron2/modeling/roi_heads/mask_head.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/roi_heads\n",
            "  copying detectron2/modeling/roi_heads/fast_rcnn.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/roi_heads\n",
            "  copying detectron2/modeling/roi_heads/box_head.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/roi_heads\n",
            "  copying detectron2/modeling/roi_heads/keypoint_head.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/roi_heads\n",
            "  copying detectron2/modeling/roi_heads/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/roi_heads\n",
            "  copying detectron2/modeling/roi_heads/rotated_fast_rcnn.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/roi_heads\n",
            "  copying detectron2/modeling/roi_heads/cascade_rcnn.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/roi_heads\n",
            "  copying detectron2/modeling/roi_heads/roi_heads.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/roi_heads\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/backbone\n",
            "  copying detectron2/modeling/backbone/build.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/backbone\n",
            "  copying detectron2/modeling/backbone/fpn.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/backbone\n",
            "  copying detectron2/modeling/backbone/regnet.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/backbone\n",
            "  copying detectron2/modeling/backbone/backbone.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/backbone\n",
            "  copying detectron2/modeling/backbone/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/backbone\n",
            "  copying detectron2/modeling/backbone/resnet.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/backbone\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/data/datasets\n",
            "  copying detectron2/data/datasets/coco.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/datasets\n",
            "  copying detectron2/data/datasets/register_coco.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/datasets\n",
            "  copying detectron2/data/datasets/cityscapes.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/datasets\n",
            "  copying detectron2/data/datasets/cityscapes_panoptic.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/datasets\n",
            "  copying detectron2/data/datasets/lvis_v1_categories.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/datasets\n",
            "  copying detectron2/data/datasets/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/datasets\n",
            "  copying detectron2/data/datasets/lvis.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/datasets\n",
            "  copying detectron2/data/datasets/coco_panoptic.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/datasets\n",
            "  copying detectron2/data/datasets/builtin.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/datasets\n",
            "  copying detectron2/data/datasets/pascal_voc.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/datasets\n",
            "  copying detectron2/data/datasets/lvis_v0_5_categories.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/datasets\n",
            "  copying detectron2/data/datasets/builtin_meta.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/datasets\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/data/transforms\n",
            "  copying detectron2/data/transforms/augmentation_impl.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/transforms\n",
            "  copying detectron2/data/transforms/augmentation.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/transforms\n",
            "  copying detectron2/data/transforms/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/transforms\n",
            "  copying detectron2/data/transforms/transform.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/transforms\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/data/samplers\n",
            "  copying detectron2/data/samplers/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/samplers\n",
            "  copying detectron2/data/samplers/grouped_batch_sampler.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/samplers\n",
            "  copying detectron2/data/samplers/distributed_sampler.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/samplers\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/projects/point_rend\n",
            "  copying projects/PointRend/point_rend/point_features.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/point_rend\n",
            "  copying projects/PointRend/point_rend/mask_head.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/point_rend\n",
            "  copying projects/PointRend/point_rend/config.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/point_rend\n",
            "  copying projects/PointRend/point_rend/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/point_rend\n",
            "  copying projects/PointRend/point_rend/point_head.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/point_rend\n",
            "  copying projects/PointRend/point_rend/roi_heads.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/point_rend\n",
            "  copying projects/PointRend/point_rend/color_augmentation.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/point_rend\n",
            "  copying projects/PointRend/point_rend/semantic_seg.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/point_rend\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/projects/deeplab\n",
            "  copying projects/DeepLab/deeplab/lr_scheduler.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/deeplab\n",
            "  copying projects/DeepLab/deeplab/config.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/deeplab\n",
            "  copying projects/DeepLab/deeplab/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/deeplab\n",
            "  copying projects/DeepLab/deeplab/loss.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/deeplab\n",
            "  copying projects/DeepLab/deeplab/resnet.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/deeplab\n",
            "  copying projects/DeepLab/deeplab/build_solver.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/deeplab\n",
            "  copying projects/DeepLab/deeplab/semantic_seg.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/deeplab\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/projects/panoptic_deeplab\n",
            "  copying projects/Panoptic-DeepLab/panoptic_deeplab/config.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/panoptic_deeplab\n",
            "  copying projects/Panoptic-DeepLab/panoptic_deeplab/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/panoptic_deeplab\n",
            "  copying projects/Panoptic-DeepLab/panoptic_deeplab/dataset_mapper.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/panoptic_deeplab\n",
            "  copying projects/Panoptic-DeepLab/panoptic_deeplab/panoptic_seg.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/panoptic_deeplab\n",
            "  copying projects/Panoptic-DeepLab/panoptic_deeplab/post_processing.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/panoptic_deeplab\n",
            "  copying projects/Panoptic-DeepLab/panoptic_deeplab/target_generator.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/panoptic_deeplab\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs\n",
            "  copying detectron2/model_zoo/configs/Base-RetinaNet.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs\n",
            "  copying detectron2/model_zoo/configs/Base-RCNN-FPN.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs\n",
            "  copying detectron2/model_zoo/configs/Base-RCNN-C4.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs\n",
            "  copying detectron2/model_zoo/configs/Base-RCNN-DilatedC5.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "  copying detectron2/model_zoo/configs/Misc/scratch_mask_rcnn_R_50_FPN_9x_syncbn.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "  copying detectron2/model_zoo/configs/Misc/mask_rcnn_R_50_FPN_3x_dconv_c3-c5.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "  copying detectron2/model_zoo/configs/Misc/semantic_R_50_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "  copying detectron2/model_zoo/configs/Misc/scratch_mask_rcnn_R_50_FPN_3x_gn.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "  copying detectron2/model_zoo/configs/Misc/panoptic_fpn_R_101_dconv_cascade_gn_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "  copying detectron2/model_zoo/configs/Misc/scratch_mask_rcnn_R_50_FPN_9x_gn.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "  copying detectron2/model_zoo/configs/Misc/mask_rcnn_R_50_FPN_3x_gn.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "  copying detectron2/model_zoo/configs/Misc/mask_rcnn_R_50_FPN_1x_dconv_c3-c5.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "  copying detectron2/model_zoo/configs/Misc/cascade_mask_rcnn_R_50_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "  copying detectron2/model_zoo/configs/Misc/cascade_mask_rcnn_X_152_32x8d_FPN_IN5k_gn_dconv.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "  copying detectron2/model_zoo/configs/Misc/mask_rcnn_R_50_FPN_1x_cls_agnostic.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "  copying detectron2/model_zoo/configs/Misc/cascade_mask_rcnn_R_50_FPN_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "  copying detectron2/model_zoo/configs/Misc/mask_rcnn_R_50_FPN_3x_syncbn.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Keypoints\n",
            "  copying detectron2/model_zoo/configs/COCO-Keypoints/keypoint_rcnn_X_101_32x8d_FPN_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Keypoints\n",
            "  copying detectron2/model_zoo/configs/COCO-Keypoints/keypoint_rcnn_R_50_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Keypoints\n",
            "  copying detectron2/model_zoo/configs/COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Keypoints\n",
            "  copying detectron2/model_zoo/configs/COCO-Keypoints/Base-Keypoint-RCNN-FPN.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Keypoints\n",
            "  copying detectron2/model_zoo/configs/COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Keypoints\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/PascalVOC-Detection\n",
            "  copying detectron2/model_zoo/configs/PascalVOC-Detection/faster_rcnn_R_50_FPN.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/PascalVOC-Detection\n",
            "  copying detectron2/model_zoo/configs/PascalVOC-Detection/faster_rcnn_R_50_C4.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/PascalVOC-Detection\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Detectron1-Comparisons\n",
            "  copying detectron2/model_zoo/configs/Detectron1-Comparisons/mask_rcnn_R_50_FPN_noaug_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Detectron1-Comparisons\n",
            "  copying detectron2/model_zoo/configs/Detectron1-Comparisons/keypoint_rcnn_R_50_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Detectron1-Comparisons\n",
            "  copying detectron2/model_zoo/configs/Detectron1-Comparisons/faster_rcnn_R_50_FPN_noaug_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Detectron1-Comparisons\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "  copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "  copying detectron2/model_zoo/configs/COCO-Detection/rpn_R_50_C4_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "  copying detectron2/model_zoo/configs/COCO-Detection/rpn_R_50_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "  copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_101_C4_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "  copying detectron2/model_zoo/configs/COCO-Detection/retinanet_R_101_FPN_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "  copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "  copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_50_DC5_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "  copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_50_DC5_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "  copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_50_C4_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "  copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_50_C4_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "  copying detectron2/model_zoo/configs/COCO-Detection/fast_rcnn_R_50_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "  copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "  copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "  copying detectron2/model_zoo/configs/COCO-Detection/retinanet_R_50_FPN_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "  copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_101_DC5_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "  copying detectron2/model_zoo/configs/COCO-Detection/retinanet_R_50_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-PanopticSegmentation\n",
            "  copying detectron2/model_zoo/configs/COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-PanopticSegmentation\n",
            "  copying detectron2/model_zoo/configs/COCO-PanopticSegmentation/Base-Panoptic-FPN.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-PanopticSegmentation\n",
            "  copying detectron2/model_zoo/configs/COCO-PanopticSegmentation/panoptic_fpn_R_50_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-PanopticSegmentation\n",
            "  copying detectron2/model_zoo/configs/COCO-PanopticSegmentation/panoptic_fpn_R_50_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-PanopticSegmentation\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/LVISv1-InstanceSegmentation\n",
            "  copying detectron2/model_zoo/configs/LVISv1-InstanceSegmentation/mask_rcnn_R_101_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/LVISv1-InstanceSegmentation\n",
            "  copying detectron2/model_zoo/configs/LVISv1-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/LVISv1-InstanceSegmentation\n",
            "  copying detectron2/model_zoo/configs/LVISv1-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/LVISv1-InstanceSegmentation\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/LVISv0.5-InstanceSegmentation\n",
            "  copying detectron2/model_zoo/configs/LVISv0.5-InstanceSegmentation/mask_rcnn_R_101_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/LVISv0.5-InstanceSegmentation\n",
            "  copying detectron2/model_zoo/configs/LVISv0.5-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/LVISv0.5-InstanceSegmentation\n",
            "  copying detectron2/model_zoo/configs/LVISv0.5-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/LVISv0.5-InstanceSegmentation\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  copying detectron2/model_zoo/configs/quick_schedules/keypoint_rcnn_R_50_FPN_inference_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_FPN_training_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_C4_training_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  copying detectron2/model_zoo/configs/quick_schedules/cascade_mask_rcnn_R_50_FPN_inference_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_C4_GCV_instant_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_C4_instant_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_FPN_instant_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  copying detectron2/model_zoo/configs/quick_schedules/panoptic_fpn_R_50_instant_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  copying detectron2/model_zoo/configs/quick_schedules/retinanet_R_50_FPN_inference_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_DC5_inference_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  copying detectron2/model_zoo/configs/quick_schedules/retinanet_R_50_FPN_instant_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_FPN_pred_boxes_training_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  copying detectron2/model_zoo/configs/quick_schedules/panoptic_fpn_R_50_training_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  copying detectron2/model_zoo/configs/quick_schedules/rpn_R_50_FPN_instant_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  copying detectron2/model_zoo/configs/quick_schedules/semantic_R_50_FPN_training_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  copying detectron2/model_zoo/configs/quick_schedules/keypoint_rcnn_R_50_FPN_normalized_training_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  copying detectron2/model_zoo/configs/quick_schedules/panoptic_fpn_R_50_inference_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  copying detectron2/model_zoo/configs/quick_schedules/cascade_mask_rcnn_R_50_FPN_instant_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  copying detectron2/model_zoo/configs/quick_schedules/fast_rcnn_R_50_FPN_inference_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  copying detectron2/model_zoo/configs/quick_schedules/semantic_R_50_FPN_inference_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  copying detectron2/model_zoo/configs/quick_schedules/rpn_R_50_FPN_inference_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  copying detectron2/model_zoo/configs/quick_schedules/keypoint_rcnn_R_50_FPN_instant_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  copying detectron2/model_zoo/configs/quick_schedules/keypoint_rcnn_R_50_FPN_training_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_C4_inference_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  copying detectron2/model_zoo/configs/quick_schedules/semantic_R_50_FPN_instant_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_FPN_inference_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  copying detectron2/model_zoo/configs/quick_schedules/fast_rcnn_R_50_FPN_instant_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Cityscapes\n",
            "  copying detectron2/model_zoo/configs/Cityscapes/mask_rcnn_R_50_FPN.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Cityscapes\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "  copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "  copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x_giou.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "  copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "  copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "  copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "  copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_C4_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "  copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_C4_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "  copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "  copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_DC5_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "  copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_DC5_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "  copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_101_DC5_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "  copying detectron2/model_zoo/configs/Misc/mmdet_mask_rcnn_R_50_FPN_1x.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "  copying detectron2/model_zoo/configs/Misc/torchvision_imagenet_R_50.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "  copying detectron2/model_zoo/configs/COCO-Keypoints/keypoint_rcnn_R_50_FPN_1x.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Keypoints\n",
            "  copying detectron2/model_zoo/configs/COCO-Detection/fcos_R_50_FPN_1x.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "  copying detectron2/model_zoo/configs/COCO-Detection/retinanet_R_50_FPN_1x.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common\n",
            "  copying detectron2/model_zoo/configs/common/coco_schedule.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common\n",
            "  copying detectron2/model_zoo/configs/common/train.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common\n",
            "  copying detectron2/model_zoo/configs/common/optim.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common/models\n",
            "  copying detectron2/model_zoo/configs/common/models/panoptic_fpn.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common/models\n",
            "  copying detectron2/model_zoo/configs/common/models/keypoint_rcnn_fpn.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common/models\n",
            "  copying detectron2/model_zoo/configs/common/models/fcos.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common/models\n",
            "  copying detectron2/model_zoo/configs/common/models/mask_rcnn_c4.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common/models\n",
            "  copying detectron2/model_zoo/configs/common/models/retinanet.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common/models\n",
            "  copying detectron2/model_zoo/configs/common/models/cascade_rcnn.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common/models\n",
            "  copying detectron2/model_zoo/configs/common/models/mask_rcnn_fpn.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common/models\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common/data\n",
            "  copying detectron2/model_zoo/configs/common/data/coco.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common/data\n",
            "  copying detectron2/model_zoo/configs/common/data/coco_panoptic_separated.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common/data\n",
            "  copying detectron2/model_zoo/configs/common/data/coco_keypoint.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common/data\n",
            "  copying detectron2/model_zoo/configs/COCO-PanopticSegmentation/panoptic_fpn_R_50_1x.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-PanopticSegmentation\n",
            "  creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "  copying detectron2/model_zoo/configs/new_baselines/mask_rcnn_R_101_FPN_200ep_LSJ.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "  copying detectron2/model_zoo/configs/new_baselines/mask_rcnn_R_50_FPN_50ep_LSJ.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "  copying detectron2/model_zoo/configs/new_baselines/mask_rcnn_regnetx_4gf_dds_FPN_100ep_LSJ.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "  copying detectron2/model_zoo/configs/new_baselines/mask_rcnn_regnetx_4gf_dds_FPN_400ep_LSJ.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "  copying detectron2/model_zoo/configs/new_baselines/mask_rcnn_regnety_4gf_dds_FPN_400ep_LSJ.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "  copying detectron2/model_zoo/configs/new_baselines/mask_rcnn_R_50_FPN_200ep_LSJ.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "  copying detectron2/model_zoo/configs/new_baselines/mask_rcnn_regnety_4gf_dds_FPN_100ep_LSJ.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "  copying detectron2/model_zoo/configs/new_baselines/mask_rcnn_R_50_FPN_100ep_LSJ.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "  copying detectron2/model_zoo/configs/new_baselines/mask_rcnn_regnety_4gf_dds_FPN_200ep_LSJ.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "  copying detectron2/model_zoo/configs/new_baselines/mask_rcnn_R_50_FPN_400ep_LSJ.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "  copying detectron2/model_zoo/configs/new_baselines/mask_rcnn_R_101_FPN_400ep_LSJ.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "  copying detectron2/model_zoo/configs/new_baselines/mask_rcnn_R_101_FPN_100ep_LSJ.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "  copying detectron2/model_zoo/configs/new_baselines/mask_rcnn_regnetx_4gf_dds_FPN_200ep_LSJ.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "  copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_regnetx_4gf_dds_fpn_1x.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "  copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_regnety_4gf_dds_fpn_1x.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "  copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "  copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_C4_1x.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "  running build_ext\n",
            "  building 'detectron2._C' extension\n",
            "  creating build/temp.macosx-10.9-universal2-3.9\n",
            "  creating build/temp.macosx-10.9-universal2-3.9/private\n",
            "  creating build/temp.macosx-10.9-universal2-3.9/private/var\n",
            "  creating build/temp.macosx-10.9-universal2-3.9/private/var/folders\n",
            "  creating build/temp.macosx-10.9-universal2-3.9/private/var/folders/tq\n",
            "  creating build/temp.macosx-10.9-universal2-3.9/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn\n",
            "  creating build/temp.macosx-10.9-universal2-3.9/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T\n",
            "  creating build/temp.macosx-10.9-universal2-3.9/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz\n",
            "  creating build/temp.macosx-10.9-universal2-3.9/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299\n",
            "  creating build/temp.macosx-10.9-universal2-3.9/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2\n",
            "  creating build/temp.macosx-10.9-universal2-3.9/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers\n",
            "  creating build/temp.macosx-10.9-universal2-3.9/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc\n",
            "  creating build/temp.macosx-10.9-universal2-3.9/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated\n",
            "  creating build/temp.macosx-10.9-universal2-3.9/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/box_iou_rotated\n",
            "  creating build/temp.macosx-10.9-universal2-3.9/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/cocoeval\n",
            "  creating build/temp.macosx-10.9-universal2-3.9/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/nms_rotated\n",
            "  clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/Headers -arch arm64 -arch x86_64 -Werror=implicit-function-declaration -Wno-error=unreachable-code -I/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc -I/Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include -I/Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/torch/csrc/api/include -I/Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/TH -I/Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/THC -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/include/python3.9 -c /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp -o build/temp.macosx-10.9-universal2-3.9/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_clang\" -DPYBIND11_STDLIB=\"_libcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1002\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:2:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/TensorUtils.h:4:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/EmptyTensor.h:2:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/TensorBase.h:8:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/core/Storage.h:6:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/core/StorageImpl.h:8:\n",
            "  /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/core/impl/COWDeleter.h:36:50: error: 'shared_mutex' is unavailable: introduced in macOS 10.12\n",
            "    using NotLastReference = std::shared_lock<std::shared_mutex>;\n",
            "                                                   ^\n",
            "  /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/shared_mutex:187:104: note: 'shared_mutex' has been explicitly marked unavailable here\n",
            "      _LIBCPP_AVAILABILITY_SHARED_MUTEX _LIBCPP_THREAD_SAFETY_ANNOTATION(__capability__(\"shared_mutex\")) shared_mutex {\n",
            "                                                                                                         ^\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:2:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/TensorUtils.h:4:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/EmptyTensor.h:2:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/TensorBase.h:8:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/core/Storage.h:6:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/core/StorageImpl.h:8:\n",
            "  /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/core/impl/COWDeleter.h:53:8: error: 'shared_mutex' is unavailable: introduced in macOS 10.12\n",
            "    std::shared_mutex mutex_;\n",
            "         ^\n",
            "  /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/shared_mutex:187:104: note: 'shared_mutex' has been explicitly marked unavailable here\n",
            "      _LIBCPP_AVAILABILITY_SHARED_MUTEX _LIBCPP_THREAD_SAFETY_ANNOTATION(__capability__(\"shared_mutex\")) shared_mutex {\n",
            "                                                                                                         ^\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:2:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/TensorUtils.h:5:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/Tensor.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/Tensor.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/TensorBody.h:28:\n",
            "  /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/util/OptionalArrayRef.h:165:34: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "      return wrapped_opt_array_ref.value();\n",
            "                                   ^\n",
            "  /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/util/OptionalArrayRef.h:227:13: note: in instantiation of member function 'c10::OptionalArrayRef<long long>::value' requested here\n",
            "    return a1.value() == other;\n",
            "              ^\n",
            "  /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1057:33: note: 'value' has been explicitly marked unavailable here\n",
            "      constexpr value_type const& value() const&\n",
            "                                  ^\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:2:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/TensorUtils.h:5:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/Tensor.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/Tensor.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/TensorBody.h:31:\n",
            "  /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/CheckMemoryFormat.h:11:35: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "        options.requires_grad_opt().value() == false,\n",
            "                                    ^\n",
            "  /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1075:28: note: 'value' has been explicitly marked unavailable here\n",
            "      constexpr value_type&& value() &&\n",
            "                             ^\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:2:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/TensorUtils.h:5:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/Tensor.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/Tensor.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/TensorBody.h:33:\n",
            "  /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/DeprecatedTypeProperties.h:114:34: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "        Device device = device_opt.value();\n",
            "                                   ^\n",
            "  /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1066:27: note: 'value' has been explicitly marked unavailable here\n",
            "      constexpr value_type& value() &\n",
            "                            ^\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:2:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/TensorUtils.h:5:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/Tensor.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/Tensor.h:3:\n",
            "  /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/TensorBody.h:442:26: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "        TORCH_CHECK(inputs.value().size() > 0, \"'inputs' argument to backward cannot be empty\")\n",
            "                           ^\n",
            "  /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1066:27: note: 'value' has been explicitly marked unavailable here\n",
            "      constexpr value_type& value() &\n",
            "                            ^\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:2:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/TensorUtils.h:5:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/Tensor.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/Tensor.h:3:\n",
            "  /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/TensorBody.h:443:30: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "        this->_backward(inputs.value(), gradient, retain_graph, create_graph);\n",
            "                               ^\n",
            "  /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1066:27: note: 'value' has been explicitly marked unavailable here\n",
            "      constexpr value_type& value() &\n",
            "                            ^\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:2:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/TensorUtils.h:5:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/Tensor.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/Tensor.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/TensorBody.h:28:\n",
            "  /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/util/OptionalArrayRef.h:137:34: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "      return wrapped_opt_array_ref.value();\n",
            "                                   ^\n",
            "  /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/TensorBody.h:5624:154: note: in instantiation of member function 'c10::OptionalArrayRef<long long>::operator*' requested here\n",
            "      return at::_ops::to_padded_tensor::call(const_cast<Tensor&>(*this), padding, output_size.has_value() ? ::std::make_optional(c10::fromIntArrayRefSlow(*output_size)) : ::std::nullopt);\n",
            "                                                                                                                                                           ^\n",
            "  /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1066:27: note: 'value' has been explicitly marked unavailable here\n",
            "      constexpr value_type& value() &\n",
            "                            ^\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:3:\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/ATen.h:7:\n",
            "  /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/Context.h:64:27: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "          ? opt_device_type.value()\n",
            "                            ^\n",
            "  /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1066:27: note: 'value' has been explicitly marked unavailable here\n",
            "      constexpr value_type& value() &\n",
            "                            ^\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:3:\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/ATen.h:7:\n",
            "  /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/Context.h:65:36: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "          : at::getAccelerator(true).value();\n",
            "                                     ^\n",
            "  /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1075:28: note: 'value' has been explicitly marked unavailable here\n",
            "      constexpr value_type&& value() &&\n",
            "                             ^\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:3:\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/ATen.h:9:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/DeviceGuard.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/IListRef.h:631:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/IListRef_inl.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/List.h:490:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/List_inl.h:4:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/ivalue.h:1581:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/ivalue_inl.h:12:\n",
            "  /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/jit_type.h:662:24: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "        prod *= shape[i].value();\n",
            "                         ^\n",
            "  /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1057:33: note: 'value' has been explicitly marked unavailable here\n",
            "      constexpr value_type const& value() const&\n",
            "                                  ^\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:3:\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/ATen.h:9:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/DeviceGuard.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/IListRef.h:631:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/IListRef_inl.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/List.h:490:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/List_inl.h:4:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/ivalue.h:1581:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/ivalue_inl.h:12:\n",
            "  /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/jit_type.h:1512:28: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "      const auto& n = name().value();\n",
            "                             ^\n",
            "  /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1057:33: note: 'value' has been explicitly marked unavailable here\n",
            "      constexpr value_type const& value() const&\n",
            "                                  ^\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:3:\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/ATen.h:9:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/DeviceGuard.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/IListRef.h:631:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/IListRef_inl.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/List.h:490:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/List_inl.h:4:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/ivalue.h:1581:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/ivalue_inl.h:12:\n",
            "  /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/jit_type.h:2129:20: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "      return reason_.value();\n",
            "                     ^\n",
            "  /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1057:33: note: 'value' has been explicitly marked unavailable here\n",
            "      constexpr value_type const& value() const&\n",
            "                                  ^\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:3:\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/ATen.h:9:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/DeviceGuard.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/IListRef.h:631:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/IListRef_inl.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/List.h:490:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/List_inl.h:4:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/ivalue.h:1581:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/ivalue_inl.h:16:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/core/DeviceGuard.h:5:\n",
            "  /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/core/impl/InlineDeviceGuard.h:229:33: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "        guard_.emplace(device_opt.value());\n",
            "                                  ^\n",
            "  /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1066:27: note: 'value' has been explicitly marked unavailable here\n",
            "      constexpr value_type& value() &\n",
            "                            ^\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:3:\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/ATen.h:9:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/DeviceGuard.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/IListRef.h:631:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/IListRef_inl.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/List.h:490:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/List_inl.h:4:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/ivalue.h:1581:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/ivalue_inl.h:16:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/core/DeviceGuard.h:5:\n",
            "  /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/core/impl/InlineDeviceGuard.h:241:39: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "        guard_.emplace(device_index_opt.value());\n",
            "                                        ^\n",
            "  /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1066:27: note: 'value' has been explicitly marked unavailable here\n",
            "      constexpr value_type& value() &\n",
            "                            ^\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:3:\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/ATen.h:9:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/DeviceGuard.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/IListRef.h:631:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/IListRef_inl.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/List.h:490:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/List_inl.h:4:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/ivalue.h:1581:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/ivalue_inl.h:20:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/core/StreamGuard.h:5:\n",
            "  /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/core/impl/InlineStreamGuard.h:144:33: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "        guard_.emplace(stream_opt.value());\n",
            "                                  ^\n",
            "  /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1066:27: note: 'value' has been explicitly marked unavailable here\n",
            "      constexpr value_type& value() &\n",
            "                            ^\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:3:\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/ATen.h:9:\n",
            "  /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/DeviceGuard.h:27:38: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "    return t.has_value() ? device_of(t.value()) : c10::nullopt;\n",
            "                                       ^\n",
            "  /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1057:33: note: 'value' has been explicitly marked unavailable here\n",
            "      constexpr value_type const& value() const&\n",
            "                                  ^\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:3:\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/ATen.h:18:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/TensorIndexing.h:13:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/NativeFunctions.h:37:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/ops/_addmm_activation_native.h:15:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/ops/_addmm_activation_meta.h:12:\n",
            "  /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/TensorIterator.h:324:34: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "      return operands_[arg].device.value();\n",
            "                                   ^\n",
            "  /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1057:33: note: 'value' has been explicitly marked unavailable here\n",
            "      constexpr value_type const& value() const&\n",
            "                                  ^\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:3:\n",
            "  In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
            "  In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/ATen.h:18:\n",
            "  /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/TensorIndexing.h:48:37: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "        step_ = std::move(step_index).value();\n",
            "                                      ^\n",
            "  /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1075:28: note: 'value' has been explicitly marked unavailable here\n",
            "      constexpr value_type&& value() &&\n",
            "                             ^\n",
            "  fatal error: too many errors emitted, stopping now [-ferror-limit=]\n",
            "  20 errors generated.\n",
            "  error: command '/usr/bin/clang' failed with exit code 1\n",
            "  ----------------------------------------\u001b[0m\n",
            "\u001b[31m  ERROR: Failed building wheel for detectron2\u001b[0m\n",
            "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
            "     command: /Library/Developer/CommandLineTools/usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-record-t56nxge3/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers /Users/pranavsuresh/Library/Python/3.9/include/python3.9/detectron2\n",
            "         cwd: /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/\n",
            "    Complete output (623 lines):\n",
            "    running install\n",
            "    running build\n",
            "    running build_py\n",
            "    creating build\n",
            "    creating build/lib.macosx-10.9-universal2-3.9\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/tools\n",
            "    copying tools/lightning_train_net.py -> build/lib.macosx-10.9-universal2-3.9/tools\n",
            "    copying tools/convert-torchvision-to-d2.py -> build/lib.macosx-10.9-universal2-3.9/tools\n",
            "    copying tools/benchmark.py -> build/lib.macosx-10.9-universal2-3.9/tools\n",
            "    copying tools/visualize_data.py -> build/lib.macosx-10.9-universal2-3.9/tools\n",
            "    copying tools/plain_train_net.py -> build/lib.macosx-10.9-universal2-3.9/tools\n",
            "    copying tools/__init__.py -> build/lib.macosx-10.9-universal2-3.9/tools\n",
            "    copying tools/visualize_json_results.py -> build/lib.macosx-10.9-universal2-3.9/tools\n",
            "    copying tools/analyze_model.py -> build/lib.macosx-10.9-universal2-3.9/tools\n",
            "    copying tools/lazyconfig_train_net.py -> build/lib.macosx-10.9-universal2-3.9/tools\n",
            "    copying tools/train_net.py -> build/lib.macosx-10.9-universal2-3.9/tools\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2\n",
            "    copying detectron2/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/checkpoint\n",
            "    copying detectron2/checkpoint/catalog.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/checkpoint\n",
            "    copying detectron2/checkpoint/c2_model_loading.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/checkpoint\n",
            "    copying detectron2/checkpoint/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/checkpoint\n",
            "    copying detectron2/checkpoint/detection_checkpoint.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/checkpoint\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "    copying detectron2/layers/deform_conv.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "    copying detectron2/layers/shape_spec.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "    copying detectron2/layers/roi_align.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "    copying detectron2/layers/roi_align_rotated.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "    copying detectron2/layers/nms.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "    copying detectron2/layers/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "    copying detectron2/layers/aspp.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "    copying detectron2/layers/mask_ops.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "    copying detectron2/layers/wrappers.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "    copying detectron2/layers/losses.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "    copying detectron2/layers/blocks.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "    copying detectron2/layers/batch_norm.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "    copying detectron2/layers/rotated_boxes.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/layers\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/structures\n",
            "    copying detectron2/structures/instances.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/structures\n",
            "    copying detectron2/structures/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/structures\n",
            "    copying detectron2/structures/boxes.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/structures\n",
            "    copying detectron2/structures/keypoints.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/structures\n",
            "    copying detectron2/structures/masks.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/structures\n",
            "    copying detectron2/structures/image_list.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/structures\n",
            "    copying detectron2/structures/rotated_boxes.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/structures\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/config\n",
            "    copying detectron2/config/config.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/config\n",
            "    copying detectron2/config/compat.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/config\n",
            "    copying detectron2/config/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/config\n",
            "    copying detectron2/config/instantiate.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/config\n",
            "    copying detectron2/config/defaults.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/config\n",
            "    copying detectron2/config/lazy.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/config\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/projects\n",
            "    copying detectron2/projects/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "    copying detectron2/utils/serialize.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "    copying detectron2/utils/colormap.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "    copying detectron2/utils/env.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "    copying detectron2/utils/analysis.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "    copying detectron2/utils/comm.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "    copying detectron2/utils/memory.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "    copying detectron2/utils/video_visualizer.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "    copying detectron2/utils/registry.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "    copying detectron2/utils/events.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "    copying detectron2/utils/collect_env.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "    copying detectron2/utils/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "    copying detectron2/utils/logger.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "    copying detectron2/utils/file_io.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "    copying detectron2/utils/testing.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "    copying detectron2/utils/visualizer.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/utils\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/solver\n",
            "    copying detectron2/solver/build.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/solver\n",
            "    copying detectron2/solver/lr_scheduler.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/solver\n",
            "    copying detectron2/solver/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/solver\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo\n",
            "    copying detectron2/model_zoo/model_zoo.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo\n",
            "    copying detectron2/model_zoo/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/modeling\n",
            "    copying detectron2/modeling/test_time_augmentation.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling\n",
            "    copying detectron2/modeling/poolers.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling\n",
            "    copying detectron2/modeling/matcher.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling\n",
            "    copying detectron2/modeling/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling\n",
            "    copying detectron2/modeling/box_regression.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling\n",
            "    copying detectron2/modeling/mmdet_wrapper.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling\n",
            "    copying detectron2/modeling/anchor_generator.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling\n",
            "    copying detectron2/modeling/sampling.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling\n",
            "    copying detectron2/modeling/postprocessing.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/evaluation\n",
            "    copying detectron2/evaluation/fast_eval_api.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/evaluation\n",
            "    copying detectron2/evaluation/panoptic_evaluation.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/evaluation\n",
            "    copying detectron2/evaluation/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/evaluation\n",
            "    copying detectron2/evaluation/cityscapes_evaluation.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/evaluation\n",
            "    copying detectron2/evaluation/coco_evaluation.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/evaluation\n",
            "    copying detectron2/evaluation/sem_seg_evaluation.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/evaluation\n",
            "    copying detectron2/evaluation/pascal_voc_evaluation.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/evaluation\n",
            "    copying detectron2/evaluation/lvis_evaluation.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/evaluation\n",
            "    copying detectron2/evaluation/testing.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/evaluation\n",
            "    copying detectron2/evaluation/evaluator.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/evaluation\n",
            "    copying detectron2/evaluation/rotated_coco_evaluation.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/evaluation\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/export\n",
            "    copying detectron2/export/c10.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/export\n",
            "    copying detectron2/export/caffe2_export.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/export\n",
            "    copying detectron2/export/flatten.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/export\n",
            "    copying detectron2/export/caffe2_patch.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/export\n",
            "    copying detectron2/export/caffe2_modeling.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/export\n",
            "    copying detectron2/export/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/export\n",
            "    copying detectron2/export/shared.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/export\n",
            "    copying detectron2/export/api.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/export\n",
            "    copying detectron2/export/caffe2_inference.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/export\n",
            "    copying detectron2/export/torchscript_patch.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/export\n",
            "    copying detectron2/export/torchscript.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/export\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/data\n",
            "    copying detectron2/data/catalog.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data\n",
            "    copying detectron2/data/build.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data\n",
            "    copying detectron2/data/benchmark.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data\n",
            "    copying detectron2/data/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data\n",
            "    copying detectron2/data/detection_utils.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data\n",
            "    copying detectron2/data/dataset_mapper.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data\n",
            "    copying detectron2/data/common.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/engine\n",
            "    copying detectron2/engine/hooks.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/engine\n",
            "    copying detectron2/engine/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/engine\n",
            "    copying detectron2/engine/train_loop.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/engine\n",
            "    copying detectron2/engine/launch.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/engine\n",
            "    copying detectron2/engine/defaults.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/engine\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/meta_arch\n",
            "    copying detectron2/modeling/meta_arch/build.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/meta_arch\n",
            "    copying detectron2/modeling/meta_arch/rcnn.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/meta_arch\n",
            "    copying detectron2/modeling/meta_arch/panoptic_fpn.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/meta_arch\n",
            "    copying detectron2/modeling/meta_arch/dense_detector.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/meta_arch\n",
            "    copying detectron2/modeling/meta_arch/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/meta_arch\n",
            "    copying detectron2/modeling/meta_arch/fcos.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/meta_arch\n",
            "    copying detectron2/modeling/meta_arch/retinanet.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/meta_arch\n",
            "    copying detectron2/modeling/meta_arch/semantic_seg.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/meta_arch\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/proposal_generator\n",
            "    copying detectron2/modeling/proposal_generator/build.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/proposal_generator\n",
            "    copying detectron2/modeling/proposal_generator/rpn.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/proposal_generator\n",
            "    copying detectron2/modeling/proposal_generator/rrpn.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/proposal_generator\n",
            "    copying detectron2/modeling/proposal_generator/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/proposal_generator\n",
            "    copying detectron2/modeling/proposal_generator/proposal_utils.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/proposal_generator\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/roi_heads\n",
            "    copying detectron2/modeling/roi_heads/mask_head.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/roi_heads\n",
            "    copying detectron2/modeling/roi_heads/fast_rcnn.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/roi_heads\n",
            "    copying detectron2/modeling/roi_heads/box_head.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/roi_heads\n",
            "    copying detectron2/modeling/roi_heads/keypoint_head.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/roi_heads\n",
            "    copying detectron2/modeling/roi_heads/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/roi_heads\n",
            "    copying detectron2/modeling/roi_heads/rotated_fast_rcnn.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/roi_heads\n",
            "    copying detectron2/modeling/roi_heads/cascade_rcnn.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/roi_heads\n",
            "    copying detectron2/modeling/roi_heads/roi_heads.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/roi_heads\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/backbone\n",
            "    copying detectron2/modeling/backbone/build.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/backbone\n",
            "    copying detectron2/modeling/backbone/fpn.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/backbone\n",
            "    copying detectron2/modeling/backbone/regnet.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/backbone\n",
            "    copying detectron2/modeling/backbone/backbone.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/backbone\n",
            "    copying detectron2/modeling/backbone/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/backbone\n",
            "    copying detectron2/modeling/backbone/resnet.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/modeling/backbone\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/data/datasets\n",
            "    copying detectron2/data/datasets/coco.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/datasets\n",
            "    copying detectron2/data/datasets/register_coco.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/datasets\n",
            "    copying detectron2/data/datasets/cityscapes.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/datasets\n",
            "    copying detectron2/data/datasets/cityscapes_panoptic.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/datasets\n",
            "    copying detectron2/data/datasets/lvis_v1_categories.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/datasets\n",
            "    copying detectron2/data/datasets/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/datasets\n",
            "    copying detectron2/data/datasets/lvis.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/datasets\n",
            "    copying detectron2/data/datasets/coco_panoptic.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/datasets\n",
            "    copying detectron2/data/datasets/builtin.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/datasets\n",
            "    copying detectron2/data/datasets/pascal_voc.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/datasets\n",
            "    copying detectron2/data/datasets/lvis_v0_5_categories.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/datasets\n",
            "    copying detectron2/data/datasets/builtin_meta.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/datasets\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/data/transforms\n",
            "    copying detectron2/data/transforms/augmentation_impl.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/transforms\n",
            "    copying detectron2/data/transforms/augmentation.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/transforms\n",
            "    copying detectron2/data/transforms/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/transforms\n",
            "    copying detectron2/data/transforms/transform.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/transforms\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/data/samplers\n",
            "    copying detectron2/data/samplers/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/samplers\n",
            "    copying detectron2/data/samplers/grouped_batch_sampler.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/samplers\n",
            "    copying detectron2/data/samplers/distributed_sampler.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/data/samplers\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/projects/point_rend\n",
            "    copying projects/PointRend/point_rend/point_features.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/point_rend\n",
            "    copying projects/PointRend/point_rend/mask_head.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/point_rend\n",
            "    copying projects/PointRend/point_rend/config.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/point_rend\n",
            "    copying projects/PointRend/point_rend/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/point_rend\n",
            "    copying projects/PointRend/point_rend/point_head.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/point_rend\n",
            "    copying projects/PointRend/point_rend/roi_heads.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/point_rend\n",
            "    copying projects/PointRend/point_rend/color_augmentation.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/point_rend\n",
            "    copying projects/PointRend/point_rend/semantic_seg.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/point_rend\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/projects/deeplab\n",
            "    copying projects/DeepLab/deeplab/lr_scheduler.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/deeplab\n",
            "    copying projects/DeepLab/deeplab/config.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/deeplab\n",
            "    copying projects/DeepLab/deeplab/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/deeplab\n",
            "    copying projects/DeepLab/deeplab/loss.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/deeplab\n",
            "    copying projects/DeepLab/deeplab/resnet.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/deeplab\n",
            "    copying projects/DeepLab/deeplab/build_solver.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/deeplab\n",
            "    copying projects/DeepLab/deeplab/semantic_seg.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/deeplab\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/projects/panoptic_deeplab\n",
            "    copying projects/Panoptic-DeepLab/panoptic_deeplab/config.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/panoptic_deeplab\n",
            "    copying projects/Panoptic-DeepLab/panoptic_deeplab/__init__.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/panoptic_deeplab\n",
            "    copying projects/Panoptic-DeepLab/panoptic_deeplab/dataset_mapper.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/panoptic_deeplab\n",
            "    copying projects/Panoptic-DeepLab/panoptic_deeplab/panoptic_seg.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/panoptic_deeplab\n",
            "    copying projects/Panoptic-DeepLab/panoptic_deeplab/post_processing.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/panoptic_deeplab\n",
            "    copying projects/Panoptic-DeepLab/panoptic_deeplab/target_generator.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/projects/panoptic_deeplab\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs\n",
            "    copying detectron2/model_zoo/configs/Base-RetinaNet.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs\n",
            "    copying detectron2/model_zoo/configs/Base-RCNN-FPN.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs\n",
            "    copying detectron2/model_zoo/configs/Base-RCNN-C4.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs\n",
            "    copying detectron2/model_zoo/configs/Base-RCNN-DilatedC5.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "    copying detectron2/model_zoo/configs/Misc/scratch_mask_rcnn_R_50_FPN_9x_syncbn.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "    copying detectron2/model_zoo/configs/Misc/mask_rcnn_R_50_FPN_3x_dconv_c3-c5.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "    copying detectron2/model_zoo/configs/Misc/semantic_R_50_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "    copying detectron2/model_zoo/configs/Misc/scratch_mask_rcnn_R_50_FPN_3x_gn.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "    copying detectron2/model_zoo/configs/Misc/panoptic_fpn_R_101_dconv_cascade_gn_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "    copying detectron2/model_zoo/configs/Misc/scratch_mask_rcnn_R_50_FPN_9x_gn.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "    copying detectron2/model_zoo/configs/Misc/mask_rcnn_R_50_FPN_3x_gn.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "    copying detectron2/model_zoo/configs/Misc/mask_rcnn_R_50_FPN_1x_dconv_c3-c5.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "    copying detectron2/model_zoo/configs/Misc/cascade_mask_rcnn_R_50_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "    copying detectron2/model_zoo/configs/Misc/cascade_mask_rcnn_X_152_32x8d_FPN_IN5k_gn_dconv.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "    copying detectron2/model_zoo/configs/Misc/mask_rcnn_R_50_FPN_1x_cls_agnostic.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "    copying detectron2/model_zoo/configs/Misc/cascade_mask_rcnn_R_50_FPN_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "    copying detectron2/model_zoo/configs/Misc/mask_rcnn_R_50_FPN_3x_syncbn.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Keypoints\n",
            "    copying detectron2/model_zoo/configs/COCO-Keypoints/keypoint_rcnn_X_101_32x8d_FPN_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Keypoints\n",
            "    copying detectron2/model_zoo/configs/COCO-Keypoints/keypoint_rcnn_R_50_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Keypoints\n",
            "    copying detectron2/model_zoo/configs/COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Keypoints\n",
            "    copying detectron2/model_zoo/configs/COCO-Keypoints/Base-Keypoint-RCNN-FPN.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Keypoints\n",
            "    copying detectron2/model_zoo/configs/COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Keypoints\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/PascalVOC-Detection\n",
            "    copying detectron2/model_zoo/configs/PascalVOC-Detection/faster_rcnn_R_50_FPN.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/PascalVOC-Detection\n",
            "    copying detectron2/model_zoo/configs/PascalVOC-Detection/faster_rcnn_R_50_C4.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/PascalVOC-Detection\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Detectron1-Comparisons\n",
            "    copying detectron2/model_zoo/configs/Detectron1-Comparisons/mask_rcnn_R_50_FPN_noaug_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Detectron1-Comparisons\n",
            "    copying detectron2/model_zoo/configs/Detectron1-Comparisons/keypoint_rcnn_R_50_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Detectron1-Comparisons\n",
            "    copying detectron2/model_zoo/configs/Detectron1-Comparisons/faster_rcnn_R_50_FPN_noaug_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Detectron1-Comparisons\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "    copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "    copying detectron2/model_zoo/configs/COCO-Detection/rpn_R_50_C4_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "    copying detectron2/model_zoo/configs/COCO-Detection/rpn_R_50_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "    copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_101_C4_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "    copying detectron2/model_zoo/configs/COCO-Detection/retinanet_R_101_FPN_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "    copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "    copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_50_DC5_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "    copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_50_DC5_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "    copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_50_C4_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "    copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_50_C4_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "    copying detectron2/model_zoo/configs/COCO-Detection/fast_rcnn_R_50_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "    copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "    copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "    copying detectron2/model_zoo/configs/COCO-Detection/retinanet_R_50_FPN_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "    copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_101_DC5_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "    copying detectron2/model_zoo/configs/COCO-Detection/retinanet_R_50_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-PanopticSegmentation\n",
            "    copying detectron2/model_zoo/configs/COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-PanopticSegmentation\n",
            "    copying detectron2/model_zoo/configs/COCO-PanopticSegmentation/Base-Panoptic-FPN.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-PanopticSegmentation\n",
            "    copying detectron2/model_zoo/configs/COCO-PanopticSegmentation/panoptic_fpn_R_50_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-PanopticSegmentation\n",
            "    copying detectron2/model_zoo/configs/COCO-PanopticSegmentation/panoptic_fpn_R_50_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-PanopticSegmentation\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/LVISv1-InstanceSegmentation\n",
            "    copying detectron2/model_zoo/configs/LVISv1-InstanceSegmentation/mask_rcnn_R_101_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/LVISv1-InstanceSegmentation\n",
            "    copying detectron2/model_zoo/configs/LVISv1-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/LVISv1-InstanceSegmentation\n",
            "    copying detectron2/model_zoo/configs/LVISv1-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/LVISv1-InstanceSegmentation\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/LVISv0.5-InstanceSegmentation\n",
            "    copying detectron2/model_zoo/configs/LVISv0.5-InstanceSegmentation/mask_rcnn_R_101_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/LVISv0.5-InstanceSegmentation\n",
            "    copying detectron2/model_zoo/configs/LVISv0.5-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/LVISv0.5-InstanceSegmentation\n",
            "    copying detectron2/model_zoo/configs/LVISv0.5-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/LVISv0.5-InstanceSegmentation\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    copying detectron2/model_zoo/configs/quick_schedules/keypoint_rcnn_R_50_FPN_inference_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_FPN_training_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_C4_training_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    copying detectron2/model_zoo/configs/quick_schedules/cascade_mask_rcnn_R_50_FPN_inference_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_C4_GCV_instant_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_C4_instant_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_FPN_instant_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    copying detectron2/model_zoo/configs/quick_schedules/panoptic_fpn_R_50_instant_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    copying detectron2/model_zoo/configs/quick_schedules/retinanet_R_50_FPN_inference_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_DC5_inference_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    copying detectron2/model_zoo/configs/quick_schedules/retinanet_R_50_FPN_instant_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_FPN_pred_boxes_training_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    copying detectron2/model_zoo/configs/quick_schedules/panoptic_fpn_R_50_training_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    copying detectron2/model_zoo/configs/quick_schedules/rpn_R_50_FPN_instant_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    copying detectron2/model_zoo/configs/quick_schedules/semantic_R_50_FPN_training_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    copying detectron2/model_zoo/configs/quick_schedules/keypoint_rcnn_R_50_FPN_normalized_training_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    copying detectron2/model_zoo/configs/quick_schedules/panoptic_fpn_R_50_inference_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    copying detectron2/model_zoo/configs/quick_schedules/cascade_mask_rcnn_R_50_FPN_instant_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    copying detectron2/model_zoo/configs/quick_schedules/fast_rcnn_R_50_FPN_inference_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    copying detectron2/model_zoo/configs/quick_schedules/semantic_R_50_FPN_inference_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    copying detectron2/model_zoo/configs/quick_schedules/rpn_R_50_FPN_inference_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    copying detectron2/model_zoo/configs/quick_schedules/keypoint_rcnn_R_50_FPN_instant_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    copying detectron2/model_zoo/configs/quick_schedules/keypoint_rcnn_R_50_FPN_training_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_C4_inference_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    copying detectron2/model_zoo/configs/quick_schedules/semantic_R_50_FPN_instant_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_FPN_inference_acc_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    copying detectron2/model_zoo/configs/quick_schedules/fast_rcnn_R_50_FPN_instant_test.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/quick_schedules\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Cityscapes\n",
            "    copying detectron2/model_zoo/configs/Cityscapes/mask_rcnn_R_50_FPN.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Cityscapes\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "    copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "    copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x_giou.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "    copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "    copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "    copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "    copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_C4_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "    copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_C4_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "    copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "    copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_DC5_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "    copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_DC5_1x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "    copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_101_DC5_3x.yaml -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "    copying detectron2/model_zoo/configs/Misc/mmdet_mask_rcnn_R_50_FPN_1x.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "    copying detectron2/model_zoo/configs/Misc/torchvision_imagenet_R_50.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/Misc\n",
            "    copying detectron2/model_zoo/configs/COCO-Keypoints/keypoint_rcnn_R_50_FPN_1x.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Keypoints\n",
            "    copying detectron2/model_zoo/configs/COCO-Detection/fcos_R_50_FPN_1x.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "    copying detectron2/model_zoo/configs/COCO-Detection/retinanet_R_50_FPN_1x.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-Detection\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common\n",
            "    copying detectron2/model_zoo/configs/common/coco_schedule.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common\n",
            "    copying detectron2/model_zoo/configs/common/train.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common\n",
            "    copying detectron2/model_zoo/configs/common/optim.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common/models\n",
            "    copying detectron2/model_zoo/configs/common/models/panoptic_fpn.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common/models\n",
            "    copying detectron2/model_zoo/configs/common/models/keypoint_rcnn_fpn.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common/models\n",
            "    copying detectron2/model_zoo/configs/common/models/fcos.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common/models\n",
            "    copying detectron2/model_zoo/configs/common/models/mask_rcnn_c4.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common/models\n",
            "    copying detectron2/model_zoo/configs/common/models/retinanet.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common/models\n",
            "    copying detectron2/model_zoo/configs/common/models/cascade_rcnn.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common/models\n",
            "    copying detectron2/model_zoo/configs/common/models/mask_rcnn_fpn.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common/models\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common/data\n",
            "    copying detectron2/model_zoo/configs/common/data/coco.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common/data\n",
            "    copying detectron2/model_zoo/configs/common/data/coco_panoptic_separated.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common/data\n",
            "    copying detectron2/model_zoo/configs/common/data/coco_keypoint.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/common/data\n",
            "    copying detectron2/model_zoo/configs/COCO-PanopticSegmentation/panoptic_fpn_R_50_1x.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-PanopticSegmentation\n",
            "    creating build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "    copying detectron2/model_zoo/configs/new_baselines/mask_rcnn_R_101_FPN_200ep_LSJ.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "    copying detectron2/model_zoo/configs/new_baselines/mask_rcnn_R_50_FPN_50ep_LSJ.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "    copying detectron2/model_zoo/configs/new_baselines/mask_rcnn_regnetx_4gf_dds_FPN_100ep_LSJ.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "    copying detectron2/model_zoo/configs/new_baselines/mask_rcnn_regnetx_4gf_dds_FPN_400ep_LSJ.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "    copying detectron2/model_zoo/configs/new_baselines/mask_rcnn_regnety_4gf_dds_FPN_400ep_LSJ.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "    copying detectron2/model_zoo/configs/new_baselines/mask_rcnn_R_50_FPN_200ep_LSJ.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "    copying detectron2/model_zoo/configs/new_baselines/mask_rcnn_regnety_4gf_dds_FPN_100ep_LSJ.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "    copying detectron2/model_zoo/configs/new_baselines/mask_rcnn_R_50_FPN_100ep_LSJ.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "    copying detectron2/model_zoo/configs/new_baselines/mask_rcnn_regnety_4gf_dds_FPN_200ep_LSJ.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "    copying detectron2/model_zoo/configs/new_baselines/mask_rcnn_R_50_FPN_400ep_LSJ.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "    copying detectron2/model_zoo/configs/new_baselines/mask_rcnn_R_101_FPN_400ep_LSJ.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "    copying detectron2/model_zoo/configs/new_baselines/mask_rcnn_R_101_FPN_100ep_LSJ.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "    copying detectron2/model_zoo/configs/new_baselines/mask_rcnn_regnetx_4gf_dds_FPN_200ep_LSJ.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/new_baselines\n",
            "    copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_regnetx_4gf_dds_fpn_1x.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "    copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_regnety_4gf_dds_fpn_1x.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "    copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "    copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_C4_1x.py -> build/lib.macosx-10.9-universal2-3.9/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "    running build_ext\n",
            "    /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/utils/cpp_extension.py:495: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "      warnings.warn(msg.format('we could not find ninja.'))\n",
            "    building 'detectron2._C' extension\n",
            "    creating build/temp.macosx-10.9-universal2-3.9\n",
            "    creating build/temp.macosx-10.9-universal2-3.9/private\n",
            "    creating build/temp.macosx-10.9-universal2-3.9/private/var\n",
            "    creating build/temp.macosx-10.9-universal2-3.9/private/var/folders\n",
            "    creating build/temp.macosx-10.9-universal2-3.9/private/var/folders/tq\n",
            "    creating build/temp.macosx-10.9-universal2-3.9/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn\n",
            "    creating build/temp.macosx-10.9-universal2-3.9/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T\n",
            "    creating build/temp.macosx-10.9-universal2-3.9/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz\n",
            "    creating build/temp.macosx-10.9-universal2-3.9/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299\n",
            "    creating build/temp.macosx-10.9-universal2-3.9/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2\n",
            "    creating build/temp.macosx-10.9-universal2-3.9/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers\n",
            "    creating build/temp.macosx-10.9-universal2-3.9/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc\n",
            "    creating build/temp.macosx-10.9-universal2-3.9/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated\n",
            "    creating build/temp.macosx-10.9-universal2-3.9/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/box_iou_rotated\n",
            "    creating build/temp.macosx-10.9-universal2-3.9/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/cocoeval\n",
            "    creating build/temp.macosx-10.9-universal2-3.9/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/nms_rotated\n",
            "    clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/Headers -arch arm64 -arch x86_64 -Werror=implicit-function-declaration -Wno-error=unreachable-code -I/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc -I/Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include -I/Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/torch/csrc/api/include -I/Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/TH -I/Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/THC -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/include/python3.9 -c /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp -o build/temp.macosx-10.9-universal2-3.9/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_clang\" -DPYBIND11_STDLIB=\"_libcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1002\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:2:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/TensorUtils.h:4:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/EmptyTensor.h:2:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/TensorBase.h:8:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/core/Storage.h:6:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/core/StorageImpl.h:8:\n",
            "    /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/core/impl/COWDeleter.h:36:50: error: 'shared_mutex' is unavailable: introduced in macOS 10.12\n",
            "      using NotLastReference = std::shared_lock<std::shared_mutex>;\n",
            "                                                     ^\n",
            "    /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/shared_mutex:187:104: note: 'shared_mutex' has been explicitly marked unavailable here\n",
            "        _LIBCPP_AVAILABILITY_SHARED_MUTEX _LIBCPP_THREAD_SAFETY_ANNOTATION(__capability__(\"shared_mutex\")) shared_mutex {\n",
            "                                                                                                           ^\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:2:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/TensorUtils.h:4:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/EmptyTensor.h:2:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/TensorBase.h:8:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/core/Storage.h:6:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/core/StorageImpl.h:8:\n",
            "    /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/core/impl/COWDeleter.h:53:8: error: 'shared_mutex' is unavailable: introduced in macOS 10.12\n",
            "      std::shared_mutex mutex_;\n",
            "           ^\n",
            "    /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/shared_mutex:187:104: note: 'shared_mutex' has been explicitly marked unavailable here\n",
            "        _LIBCPP_AVAILABILITY_SHARED_MUTEX _LIBCPP_THREAD_SAFETY_ANNOTATION(__capability__(\"shared_mutex\")) shared_mutex {\n",
            "                                                                                                           ^\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:2:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/TensorUtils.h:5:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/Tensor.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/Tensor.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/TensorBody.h:28:\n",
            "    /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/util/OptionalArrayRef.h:165:34: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "        return wrapped_opt_array_ref.value();\n",
            "                                     ^\n",
            "    /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/util/OptionalArrayRef.h:227:13: note: in instantiation of member function 'c10::OptionalArrayRef<long long>::value' requested here\n",
            "      return a1.value() == other;\n",
            "                ^\n",
            "    /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1057:33: note: 'value' has been explicitly marked unavailable here\n",
            "        constexpr value_type const& value() const&\n",
            "                                    ^\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:2:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/TensorUtils.h:5:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/Tensor.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/Tensor.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/TensorBody.h:31:\n",
            "    /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/CheckMemoryFormat.h:11:35: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "          options.requires_grad_opt().value() == false,\n",
            "                                      ^\n",
            "    /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1075:28: note: 'value' has been explicitly marked unavailable here\n",
            "        constexpr value_type&& value() &&\n",
            "                               ^\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:2:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/TensorUtils.h:5:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/Tensor.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/Tensor.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/TensorBody.h:33:\n",
            "    /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/DeprecatedTypeProperties.h:114:34: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "          Device device = device_opt.value();\n",
            "                                     ^\n",
            "    /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1066:27: note: 'value' has been explicitly marked unavailable here\n",
            "        constexpr value_type& value() &\n",
            "                              ^\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:2:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/TensorUtils.h:5:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/Tensor.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/Tensor.h:3:\n",
            "    /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/TensorBody.h:442:26: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "          TORCH_CHECK(inputs.value().size() > 0, \"'inputs' argument to backward cannot be empty\")\n",
            "                             ^\n",
            "    /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1066:27: note: 'value' has been explicitly marked unavailable here\n",
            "        constexpr value_type& value() &\n",
            "                              ^\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:2:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/TensorUtils.h:5:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/Tensor.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/Tensor.h:3:\n",
            "    /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/TensorBody.h:443:30: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "          this->_backward(inputs.value(), gradient, retain_graph, create_graph);\n",
            "                                 ^\n",
            "    /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1066:27: note: 'value' has been explicitly marked unavailable here\n",
            "        constexpr value_type& value() &\n",
            "                              ^\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:2:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/TensorUtils.h:5:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/Tensor.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/Tensor.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/TensorBody.h:28:\n",
            "    /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/util/OptionalArrayRef.h:137:34: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "        return wrapped_opt_array_ref.value();\n",
            "                                     ^\n",
            "    /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/TensorBody.h:5624:154: note: in instantiation of member function 'c10::OptionalArrayRef<long long>::operator*' requested here\n",
            "        return at::_ops::to_padded_tensor::call(const_cast<Tensor&>(*this), padding, output_size.has_value() ? ::std::make_optional(c10::fromIntArrayRefSlow(*output_size)) : ::std::nullopt);\n",
            "                                                                                                                                                             ^\n",
            "    /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1066:27: note: 'value' has been explicitly marked unavailable here\n",
            "        constexpr value_type& value() &\n",
            "                              ^\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:3:\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/ATen.h:7:\n",
            "    /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/Context.h:64:27: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "            ? opt_device_type.value()\n",
            "                              ^\n",
            "    /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1066:27: note: 'value' has been explicitly marked unavailable here\n",
            "        constexpr value_type& value() &\n",
            "                              ^\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:3:\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/ATen.h:7:\n",
            "    /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/Context.h:65:36: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "            : at::getAccelerator(true).value();\n",
            "                                       ^\n",
            "    /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1075:28: note: 'value' has been explicitly marked unavailable here\n",
            "        constexpr value_type&& value() &&\n",
            "                               ^\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:3:\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/ATen.h:9:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/DeviceGuard.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/IListRef.h:631:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/IListRef_inl.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/List.h:490:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/List_inl.h:4:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/ivalue.h:1581:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/ivalue_inl.h:12:\n",
            "    /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/jit_type.h:662:24: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "          prod *= shape[i].value();\n",
            "                           ^\n",
            "    /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1057:33: note: 'value' has been explicitly marked unavailable here\n",
            "        constexpr value_type const& value() const&\n",
            "                                    ^\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:3:\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/ATen.h:9:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/DeviceGuard.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/IListRef.h:631:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/IListRef_inl.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/List.h:490:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/List_inl.h:4:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/ivalue.h:1581:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/ivalue_inl.h:12:\n",
            "    /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/jit_type.h:1512:28: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "        const auto& n = name().value();\n",
            "                               ^\n",
            "    /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1057:33: note: 'value' has been explicitly marked unavailable here\n",
            "        constexpr value_type const& value() const&\n",
            "                                    ^\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:3:\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/ATen.h:9:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/DeviceGuard.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/IListRef.h:631:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/IListRef_inl.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/List.h:490:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/List_inl.h:4:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/ivalue.h:1581:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/ivalue_inl.h:12:\n",
            "    /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/jit_type.h:2129:20: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "        return reason_.value();\n",
            "                       ^\n",
            "    /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1057:33: note: 'value' has been explicitly marked unavailable here\n",
            "        constexpr value_type const& value() const&\n",
            "                                    ^\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:3:\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/ATen.h:9:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/DeviceGuard.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/IListRef.h:631:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/IListRef_inl.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/List.h:490:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/List_inl.h:4:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/ivalue.h:1581:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/ivalue_inl.h:16:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/core/DeviceGuard.h:5:\n",
            "    /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/core/impl/InlineDeviceGuard.h:229:33: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "          guard_.emplace(device_opt.value());\n",
            "                                    ^\n",
            "    /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1066:27: note: 'value' has been explicitly marked unavailable here\n",
            "        constexpr value_type& value() &\n",
            "                              ^\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:3:\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/ATen.h:9:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/DeviceGuard.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/IListRef.h:631:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/IListRef_inl.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/List.h:490:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/List_inl.h:4:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/ivalue.h:1581:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/ivalue_inl.h:16:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/core/DeviceGuard.h:5:\n",
            "    /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/core/impl/InlineDeviceGuard.h:241:39: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "          guard_.emplace(device_index_opt.value());\n",
            "                                          ^\n",
            "    /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1066:27: note: 'value' has been explicitly marked unavailable here\n",
            "        constexpr value_type& value() &\n",
            "                              ^\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:3:\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/ATen.h:9:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/DeviceGuard.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/IListRef.h:631:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/IListRef_inl.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/List.h:490:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/List_inl.h:4:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/ivalue.h:1581:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/core/ivalue_inl.h:20:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/core/StreamGuard.h:5:\n",
            "    /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/c10/core/impl/InlineStreamGuard.h:144:33: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "          guard_.emplace(stream_opt.value());\n",
            "                                    ^\n",
            "    /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1066:27: note: 'value' has been explicitly marked unavailable here\n",
            "        constexpr value_type& value() &\n",
            "                              ^\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:3:\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/ATen.h:9:\n",
            "    /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/DeviceGuard.h:27:38: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "      return t.has_value() ? device_of(t.value()) : c10::nullopt;\n",
            "                                         ^\n",
            "    /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1057:33: note: 'value' has been explicitly marked unavailable here\n",
            "        constexpr value_type const& value() const&\n",
            "                                    ^\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:3:\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/ATen.h:18:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/TensorIndexing.h:13:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/NativeFunctions.h:37:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/ops/_addmm_activation_native.h:15:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/ops/_addmm_activation_meta.h:12:\n",
            "    /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/TensorIterator.h:324:34: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "        return operands_[arg].device.value();\n",
            "                                     ^\n",
            "    /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1057:33: note: 'value' has been explicitly marked unavailable here\n",
            "        constexpr value_type const& value() const&\n",
            "                                    ^\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:3:\n",
            "    In file included from /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
            "    In file included from /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/ATen.h:18:\n",
            "    /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages/torch/include/ATen/TensorIndexing.h:48:37: error: 'value' is unavailable: introduced in macOS 10.13\n",
            "          step_ = std::move(step_index).value();\n",
            "                                        ^\n",
            "    /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/optional:1075:28: note: 'value' has been explicitly marked unavailable here\n",
            "        constexpr value_type&& value() &&\n",
            "                               ^\n",
            "    fatal error: too many errors emitted, stopping now [-ferror-limit=]\n",
            "    20 errors generated.\n",
            "    error: command '/usr/bin/clang' failed with exit code 1\n",
            "    ----------------------------------------\u001b[0m\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /Library/Developer/CommandLineTools/usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-install-kr3pwxsz/detectron2_3b5952fd4e624c8b83f368c849b6d299/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/tq/qh_2v9x13qj_15wl4lftdtpr0000gn/T/pip-record-t56nxge3/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers /Users/pranavsuresh/Library/Python/3.9/include/python3.9/detectron2 Check the logs for full command output.\u001b[0m\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: sentence-transformers in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (3.1.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (0.25.1)\n",
            "Requirement already satisfied: scikit-learn in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: Pillow in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: scipy in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (2.4.1)\n",
            "Requirement already satisfied: tqdm in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: filelock in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: networkx in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: sympy in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/pranavsuresh/Library/Python/3.9/lib/python/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pinecone-client -q\n",
        "%pip install pinecone[grpc]\n",
        "%pip install googleapis-common-protos\n",
        "%pip install protoc_gen_openapiv2\n",
        "%pip install langchain_pinecone\n",
        "%pip install langchain-community\n",
        "%pip install tiktoken -q\n",
        "%pip install poppler-utils  \n",
        "%pip install unstructured -q\n",
        "%pip install unstructured[local-inference] -q\n",
        "%pip install detectron2@git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2 -q\n",
        "%pip install -U sentence-transformers\n",
        "%pip install --upgrade langchain openai  -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fulYnj9nZr3n",
        "outputId": "358cd3d8-43a3-41dd-efd5-60e2679a91b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "442"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "\n",
        "directory = '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs'\n",
        "\n",
        "def load_docs(directory):\n",
        "  loader = PyPDFDirectoryLoader(directory)\n",
        "  documents = loader.load()\n",
        "  return documents\n",
        "\n",
        "docs = load_docs(directory)\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwaCosqsogzw"
      },
      "source": [
        "https://python.langchain.com/en/latest/modules/indexes/text_splitters/getting_started.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "def chunk_data(doc, chunk_size=500, chunk_overlap=50):\n",
        "    textsplitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    doc=textsplitter.split_documents(doc)\n",
        "    return doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3406"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents = chunk_data(docs)\n",
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 0}, page_content='Computer Stereo Vision for Autonomous\\nDriving\\nRui Fan, Li Wang, Mohammud Junaid Bocus, Ioannis Pitas\\nAbstract As an important component of autonomous systems, autonomous car\\nperception has had a big leap with recent advances in parallel computing archi-\\ntectures. With the use of tiny but full-feature embedded supercomputers, com-\\nputer stereo vision has been prevalently applied in autonomous cars for depth\\nperception. The two key aspects of computer stereo vision are speed and ac-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 0}, page_content='curacy. They are both desirable but conﬂicting properties, as the algorithms\\nwith better disparity accuracy usually have higher computational complexity.\\nTherefore, the main aim of developing a computer stereo vision algorithm for\\nresource-limited hardware is to improve the trade-oﬀ between speed and accu-\\nracy. In this chapter, we introduce both the hardware and software aspects of\\ncomputer stereo vision for autonomous car systems. Then, we discuss four au-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 0}, page_content='tonomous car perception tasks, including 1) visual feature detection, description\\nand matching, 2) 3D information acquisition, 3) object detection/recognition\\nand 4) semantic image segmentation. The principles of computer stereo vision\\nand parallel computing on multi-threading CPU and GPU architectures are then\\ndetailed.\\nRui Fan\\nUC San Diego, e-mail:rfan@ucsd.edu\\nLi Wang\\nATG Robotics, e-mail:li.wang@ieee.org\\nMohammud Junaid Bocus\\nUniversity of Bristol, e-mail:junaid.bocus@bristol.ac.uk'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 0}, page_content='Ioannis Pitas\\nAristotle University of Thessaloniki, e-mail:pitas@csd.auth.gr\\n1\\narXiv:2012.03194v2  [cs.CV]  17 Dec 2020'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 1}, page_content='2 Rui Fan, Li Wang, Mohammud Junaid Bocus, Ioannis Pitas\\n1 Introduction\\nAutonomous car systems enable self-driving cars to navigate in complicated envi-\\nronments,withoutanyinterventionofhumandrivers.Anexampleofautonomous\\ncar system architecture is illustrated in Fig 1. Its hardware (HW) mainly in-\\ncludes: 1) car sensors, such as cameras, LIDARs, and Radars; and 2) car chassis,\\nsuch as throttle, brake, and wheel. On the other hand, the software (SW) is'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 1}, page_content='comprised of four main functional modules [1]: 1) perception, 2) localization and\\nmapping, 3) prediction and planning and 4) control. Computer stereo vision is\\nan important component of the perception module. It enables self-driving cars\\nto perceive environment in 3D.\\nFig. 1 Autonomous car system architecture.\\nThis chapter ﬁrst introduces the HW/SW aspects of an autonomous car sys-\\ntem. Then, four autonomous car perception tasks are discussed, including: 1) vi-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 1}, page_content='sual feature detection, description and matching, 2) 3D information acquisition,\\n3) object detection/recognition and 4) semantic image segmentation. Finally, the\\nprinciples of computer stereo vision and parallel computing on multi-threading\\nCPU and GPU are detailed.\\n2 Autonomous Car System\\n2.1 Hardware\\n2.1.1 Car Sensors\\nThe most commonly used car sensors include: a) passive sensors, such as cam-\\neras; b) active sensors, such as LIDARs, Radars and ultrasonic transceivers;'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 1}, page_content='and c) other types of sensors, such as global positioning systems (GPS), inertial\\nmeasurement unit (IMU), among others. When choosing them, we need to con-\\nsider many diﬀerent factors, such as sampling rate, ﬁeld of view (FoV), accuracy,\\nrange, cost and overall system complexity1.\\n1 autonomous-driving.org/2019/01/25/positioning-sensors-for-autonomous-vehicles'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 2}, page_content='Computer Stereo Vision for Autonomous Driving 3\\nCameras capture 2D images, by collecting light reﬂected on 3D objects. Im-\\nages captured from diﬀerent views can be utilized to reconstruct the 3D driving\\nscene geometry. Most autonomous car perception tasks, such as visual semantic\\ndriving scene segmentation and object detection/recognition, are developed for\\nimages. In Sec. 3, we provide readers with a comprehensive overview of these'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 2}, page_content='tasks. The perspective (or pinhole) camera model and the mathematical princi-\\nples of multi-view geometry are discussed in Sec. 4. Acquired image quality is\\nalways subject to environmental conditions, such as weather and illumination [2].\\nTherefore, the visual information fusion from other sensors is typically required\\nfor robust autonomous car perception.\\nLIDAR illuminates a target with pulsed laser light and measures the source-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 2}, page_content='target distance, by analyzing the reﬂected laser pulses [3]. Due to its ability to\\ngenerate highly accurate 3D driving scene geometry models, LIDARs are gen-\\nerally mounted on autonomous cars for depth perception. Current industrial\\nautonomous car localization and mapping systems are generally based on LI-\\nDARs. Furthermore, Radars can measure both the range and radial velocity\\nof an object, by transmitting an electromagnetic wave and analyzing its re-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 2}, page_content='ﬂections [4]. Radars have already been established in the automotive industry,\\nand they have been prevalently employed to enable intelligent vehicle advanced\\ndriver assistance system (ADAS) features, such as adaptive cruise control and\\nautonomousemergencybraking 1.SimilartoRadar,ultrasonictransceiverscalcu-\\nlate the source-object distance, by measuring the time between transmitting an\\nultrasonic signal and receiving its echo [5]. Ultrasonic transceivers are commonly'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 2}, page_content='used for autonomous car localization and navigation.\\nIn addition to the aforementioned passive and active sensors, GPS and IMU\\nsystems are commonly used to enhance autonomous car localization and map-\\nping performance [6]. GPS can provide both time and geolocation information\\nfor autonomous cars. However, its signals can become very weak, when GPS re-\\nception is blocked by obstacles in GPS-denied regions, such as urban regions [7].'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 2}, page_content='Hence, GPS and IMU information fusion is widely adopted to provide continuous\\nautonomous car position and velocity information [6].\\n2.1.2 Car Chassis\\nCar chassis technologies, especially Drive-by-Wire (DbW), are required for build-\\ning autonomous vehicles. DbW technology refers to the electronic systems that\\ncan replace traditional mechanical controls [8]. DbW systems can perform vehi-\\ncle functions, which are traditionally achieved by mechanical linkages, through'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 2}, page_content='electrical or electro-mechanical systems. There are three main vehicle control\\nsystems that are commonly replaced with electronic controls: 1) throttle, 2)\\nbraking and 3) steering.\\nA Throttle-by-Wire (TbW) system helps accomplish vehicle propulsion via an\\nelectronic throttle, without any cables from the accelerator pedal to the engine\\nthrottle valve. In electric vehicles, TbW system controls the electric motors,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 3}, page_content='4 Rui Fan, Li Wang, Mohammud Junaid Bocus, Ioannis Pitas\\nby sensing accelerator pedal for a pressure (input) and sending signal to the\\npower inverter modules. Compared to traditional hydraulic brakes, which pro-\\nvide braking eﬀort, by building hydraulic pressure in the brake lines, a Brake-\\nby-Wire (BbW) system completely eliminates the need for hydraulics, by using\\nelectronic motors to activate calipers. Furthermore, in vehicles that are equipped'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 3}, page_content='with Steer-by-Wire (SbW) technology, there is no physical connection between\\nthe steering wheel and the tires. The control of wheels’ direction is established\\nthrough electric motor(s), which are actuated by electronic control units moni-\\ntoring steering wheel inputs.\\nIn comparison to traditional throttle systems, electronic throttle systems are\\nmuch lighter, hence greatly reducing modern car weight. In addition, they are'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 3}, page_content='easier to service and tune, as a technician can simply connect a laptop to perform\\ntuning. Moreover, an electronic control system allows more accurate control of\\nthrottle opening, compared to a cable control that stretches over time. Further-\\nmore, since the steering wheel can be bypassed as an input device, safety can be\\nimproved by providing computer controlled intervention of vehicle controls with\\nsystems, such as Adaptive Cruise Control and Electronic Stability Control.\\n2.2 Software'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 3}, page_content='2.2 Software\\nAutonomous car perception module analyzes the raw data collected by car sen-\\nsors (see Sec. 2.1.1) and outputs its understanding to the environment. This\\nprocess is similar to human visual cognition. We discuss diﬀerent autonomous\\ncar perception tasks in Sec. 3.\\nPerception module outputs are then used by other modules. The localization\\nand mapping module not only estimates autonomous car location, but also con-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 3}, page_content='structs and updates the 3D environment map [9]. This topic has become very\\npopular, since the concept ofSimultaneous Localization and Mapping (SLAM)\\nwas introduced in 1986 [10].\\nPrediction and planning module ﬁrst analyzes the motion patterns of other\\ntraﬃc agents and predicts their future trajectories. Such prediction outputs are\\nthen used to determine possible safe autonomous car navigation routes [11] using\\ndiﬀerent path planning techniques, such as Dijkstra [12], A-star (or simply A*)'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 3}, page_content='[13], etc.\\nFinally, autonomous car control module sends appropriate commands to car\\ncontrollers (see Sec. 2.1.2), based on its predicted trajectory and the estimated\\ncar state. This enables the autonomous car to follow the planned trajectory,\\nas closely as possible. Traditional controllers, such as proportional-integral-\\nderivative (PID) [14], linear-quadratic regulator (LQR) [15] and model predictive\\ncontrol (MPC) [16] are still the most commonly used ones in autonomous car\\ncontrol module.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 4}, page_content='Computer Stereo Vision for Autonomous Driving 5\\n3 Autonomous Car Perception\\nThe autonomous car perception module has four main functionalities:\\n1. visual feature detection, description and matching;\\n2. 3D information acquisition;\\n3. objection detection/recognition;\\n4. semantic image segmentation.\\nVisual feature detectors and descriptors have become very popular research\\ntopics in the computer vision and robotics communities. They have been applied'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 4}, page_content='in many application domains [17], such as image classiﬁcation [18], 3D scene\\nreconstruction [19], object recognition [20] and visual tracking [21]. The matched\\nvisual feature correspondences between two (or more) images can be utilized to\\nestablish image relationships [17]. The most well-known visual features are scale-\\ninvariant feature transform (SIFT) [22], speeded up robust feature (SURF) [23],\\noriented FAST and rotated BRIEF (ORB) [24], binary robust invariant scalable'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 4}, page_content='keypoints (BRISK) [25], and so forth.\\nStereo Camera\\nW\\nSingle Camera\\n Stereo Camera\\nW\\nSingle Camera\\nReal Scene Reconstructed Scene\\nFig. 2 3D scene reconstruction, where W presents the world coordinate system (WCS).\\nThe digital images captured by cameras are essentially 2D [26]. In order to\\nextrapolate the 3D information from a given driving scene, images from multiple\\nviews are required [27]. These images can be captured using either a single'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 4}, page_content='moving camera [28] or an array of synchronized cameras, as shown in Fig. 2. The\\nformeristypicallyknownas structure from motion (SfM)[29]or optical ﬂow[28],\\nwhile the latter is typically referred to asstereo vision or binocular vision (in\\ncase two cameras are used) [26]. SfM methods estimate both camera poses and\\nthe 3D points of interest from images captured from multiple views, which are\\nlinked by a collection of visual features. They also leverage bundle adjustment'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 4}, page_content='(BA) [30] technique to reﬁne the estimated camera poses and 3D point locations,\\nbyminimizingacostfunctionknownastotalre-projectionerror[31].Opticalﬂow\\ndescribes the motion of pixels between consecutive frames of a video sequence\\n[31]. It is also regarded as an eﬀective tool for dynamic object detection [28].\\nStereo vision acquires depth information by ﬁnding the horizontal positional\\ndiﬀerences (disparities) of the visual feature correspondence pairs between two'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 5}, page_content='6 Rui Fan, Li Wang, Mohammud Junaid Bocus, Ioannis Pitas\\nsynchronously captured images. More details on computer stereo vision will be\\ngiven in Sec. 4.3.\\nFig. 3 Object detection/recognition.\\nObject detection/recognition refers to the recognition and localization of par-\\nticular objects in images/videos [32]. It can be used in various autonomous car\\nperception subtasks, such as pedestrian detection [33], vehicle detection [31],'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 5}, page_content='traﬃc sign detection [34], cyclist detection [35],etc., as shown in Fig. 3. Object\\ndetection approaches can be classiﬁed as either computer vision-based or ma-\\nchine/deep learning-based. The former typically consists of three steps [32]: 1)\\ninformative region selection (scanning the whole image by sliding windows with\\nparticular templates to produce candidate regions); 2) visual feature extraction,\\nas discussed above; and 3) object classiﬁcation (distinguishing a target object'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 5}, page_content='from all the other categories using a classiﬁer). With recent advances in ma-\\nchine/deep learning, a large number of convolutional neural networks (CNNs)\\nhave been proposed to recognize objects from images/videos. Such CNN-based\\napproacheshaveachievedveryimpressiveresults.Themostpopularonesinclude:\\nregions with CNN features (R-CNN) [36], fast R-CNN [37], faster R-CNN [38],\\nyou only look once (YOLO) [39], YOLOv3 [40], YOLOv4 [41],etc.\\nFig. 4 Semantic image segmentation.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 5}, page_content='Fig. 4 Semantic image segmentation.\\nSemantic image segmentation labels every pixel in the image with a given ob-\\nject class [42], such as lane marking, vehicle, collision-free space, or pedestrian,\\nas illustrated in Fig. 4. The state-of-the-art semantic image segmentation ap-\\nproachesaremainlycategorizedintotwomaingroups[43]:1)single-modaland2)\\ndata-fusion.TheformertypicallysegmentsRGBimageswithanencoder-decoder\\nCNN architecture [44]. In recent years, many popular single-model semantic im-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 5}, page_content='age segmentation algorithms, such as Fully Convolutional Network (FCN) [45],\\nU-Net [46], SegNet [47], DeepLabv3+ [48], DenseASPP [49], DUpsampling [50],\\netc., have been proposed. Data-fusion semantic image segmentation approaches\\ngenerally learn features from two diﬀerent types of vision data [51], such as\\nRGB and depth images in FuseNet [52], RGB and surface normal images [53]'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 6}, page_content='Computer Stereo Vision for Autonomous Driving 7\\nin SNE-RoadSeg [42], RGB and transformed disparity [54–56] images in AA-\\nRTFNet [51], or RGB and thermal images in MFNet [57]. The learned feature\\nmaps are then fused to provide a better semantic prediction.\\nPlease note: a given autonomous car perception application can always be\\nsolved by diﬀerent types of techniques. For instance, lane marking detection can\\nbe formulated as a linear/quadratic/quadruplicate pattern recognition problem'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 6}, page_content='and solved using straight line detection algorithms [58] or dynamic programming\\nalgorithms [59–61]. On the other hand, it can also be formulated as a semantic\\nimage segmentation problem and solved with CNNs.\\n4 Computer Stereo Vision\\n4.1 Preliminaries\\n1. Skew-symmetric matrix\\nIn linear algebra, askew-symmetric matrixA satisﬁes the following property: its\\ntranspose is identical to its negative,i.e., A>=\\x00A. In 3D computer vision, the'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 6}, page_content='skew-symmetric matrix»a¼\\x02 of a vectora =»𝑎1\\x96𝑎2\\x96𝑎3¼> can be written as [27]:\\n»a¼\\x02=\\n266664\\n0 \\x00𝑎3 𝑎2\\n𝑎3 0 \\x00𝑎1\\n\\x00𝑎2 𝑎1 0\\n377775\\n\\x95 (1)\\nA skew-symmetric matrix has two important properties:\\na>»a¼\\x02=0>\\x96 »a¼\\x02a =0\\x96 (2)\\nwhere 0 = »0\\x960\\x960¼> is a zero vector. Furthermore, the cross product of two\\nvectors a and b can be formulated as a matrix multiplication process [27]:\\na \\x02b =»a¼\\x02b =\\x00»b¼\\x02a\\x95 (3)\\nThese properties are generally used to simplify the equations related to vector\\ncross-product.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 6}, page_content='cross-product.\\n2. Lie group SO(3) and SE(3)\\nA 3D pointx1 = »𝑥1\\x96𝑦1\\x96𝑧1¼> 2R3\\x021 can be transformed into another 3D point\\nx2 = »𝑥2\\x96𝑦2\\x96𝑧2¼> 2R3\\x021 using a rotation matrix R 2R3\\x023 and a translation\\nvector t 2R3\\x021:\\nx2 =Rx1 ¸t\\x95 (4)\\nR satisﬁes matrix orthogonality:'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 7}, page_content='8 Rui Fan, Li Wang, Mohammud Junaid Bocus, Ioannis Pitas\\nRR>=R>R =I and jdet¹Rºj=1\\x96 (5)\\nwhere I is an identity matrix and det¹Rºrepresents the determinant ofR. The\\ngroup containing all rotation matrices is referred to as aspecial orthogonal group\\nand is denoted as SO(3).~ x1 = »x1>\\x961¼> and ~ x2 = »x2>\\x961¼>, the homogeneous\\ncoordinates of x1 and x2, can be used to describe rotation and translation, as\\nfollows:\\n~ x2 =P~ x1\\x96 (6)\\nwhere\\nP =\\n\\x14 R t\\n0> 1\\n\\x15\\n\\x96 (7)'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 7}, page_content='~ x2 =P~ x1\\x96 (6)\\nwhere\\nP =\\n\\x14 R t\\n0> 1\\n\\x15\\n\\x96 (7)\\nP is a homogeneous transformation matrix2. The group containing all homoge-\\nneous transformation matrices is referred to as aspecial Euclidean groupand is\\ndenoted as SE(3).\\n4.2 Multi-View Geometry\\n4.2.1 Perspective Camera Model\\nThe perspective (or pinhole) camera model, as illustrated in Fig. 5, is the most\\ncommon geometric camera model describing the relationship between a 3D point'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 7}, page_content='pC =»𝑥C\\x96𝑦C\\x96𝑧C¼>in the camera coordinate system (CCS) and its projection\\x16 p=\\n»𝑥\\x96𝑦\\x96 𝑓¼> on the image planeΠ. oC is the camera center. The distance between\\nΠ and oC is the camera focal length 𝑓. ^pC = »𝑥C\\n𝑧C \\x96 𝑦C\\n𝑧C \\x961¼> are the normalized\\ncoordinates of pC = »𝑥C\\x96𝑦C\\x96𝑧C¼>. Optical axis is the ray originating fromoC\\nand passing perpendicularly throughΠ. The relationship betweenpC and \\x16p is\\nas follows [62]:\\n\\x16 p= 𝑓^pC = 𝑓\\n𝑧C pC\\x95 (8)\\nFig. 5 Perspective camera model.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 7}, page_content='𝑧C pC\\x95 (8)\\nFig. 5 Perspective camera model.\\n2 seas.upenn.edu/~meam620/slides/kinematicsI.pdf'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 8}, page_content='Computer Stereo Vision for Autonomous Driving 9\\n4.2.2 Intrinsic Matrix\\nSince lens distortion does not exist in a perspective camera model,\\x16 p=»𝑥\\x96𝑦\\x96 𝑓¼>\\non the image planeΠ can be transformed into a pixelp = »𝑢\\x96𝑣¼> in the image\\nusing [26]:\\n𝑢=𝑢𝑜 ¸𝑠𝑥𝑥\\x96 𝑣 =𝑣𝑜 ¸𝑠𝑦 𝑦\\x96 (9)\\nwhere p𝑜 = »𝑢𝑜\\x96𝑣𝑜¼> is the principal point; and𝑠𝑥 and 𝑠𝑦 are the eﬀective size\\nmeasured (in pixels per millimeter) in the horizontal and vertical directions,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 8}, page_content='respectively [62]. To simplify the expression of the camera intrinsic matrixK,\\ntwo notations 𝑓𝑥 = 𝑓𝑠𝑥 and 𝑓𝑦 = 𝑓𝑠𝑦 are introduced. 𝑢𝑜, 𝑣𝑜, 𝑓, 𝑠𝑥 and 𝑠𝑦 [27]\\nare ﬁve camera intrinsic parameters. Combining (8) and (9), a 3D pointpC in\\nthe CCS can be transformed into a pixelp in the image using [63]:\\n~ p= 1\\n𝑧C KpC = 1\\n𝑧C\\n266664\\n𝑓𝑥 0 𝑢𝑜\\n0 𝑓𝑦 𝑣𝑜\\n0 0 1\\n377775\\n266664\\n𝑥C\\n𝑦C\\n𝑧C\\n377775\\n\\x96 (10)\\nwhere ~p = »p>\\x961¼> = »𝑢\\x96𝑣\\x96 1¼> denotes the homogeneous coordinates of p ='),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 8}, page_content='»𝑢\\x96𝑣¼>. Plugging (10) into (8) results in:\\n^pC =K\\x001~ p= \\x16 p\\n𝑓 = pC\\n𝑧C \\x95 (11)\\nTherefore, an arbitrary 3D point lying on the ray, which goes from oC and\\nthrough pC, is always projected at\\x16p on the image plane.\\n4.2.3 Lens Distortion\\nIn order to get better imaging results, a lens is usually installed in front of the\\ncamera [62]. However, this introduces image distortions. The optical aberration\\ncaused by the installed lens typically deforms the physically straight lines pro-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 8}, page_content='vided by projective geometry to curves in the images [64], as shown in Fig. 6(a).\\nWe can observe in Fig. 6(b) that the bent checkerboard grids become straight\\nwhen the lens distortion is corrected.\\nFig. 6 Distorted image correction: (a) original image; (b) corrected image.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 9}, page_content='10 Rui Fan, Li Wang, Mohammud Junaid Bocus, Ioannis Pitas\\nLens distortion can be categorized into two main types: 1) radial distortion\\nand 2) tangential distortion [26]. The presence of radial distortion is due to the\\nfact that geometric lens shape aﬀects straight lines. Tangential distortion occurs\\nbecause the lens is not perfectly parallel to the image plane [62]. In practical\\nexperiments, the image geometry is aﬀected by radial distortion to a much higher'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 9}, page_content='extent than by tangential distortion. Therefore, the latter is sometimes neglected\\nin the process of distorted image correction.\\nRadial distortion\\nRadial distortion mainly includes 1) barrel distortion, 2) pincushion distortion\\nand 3) mustache distortion, as illustrated in Fig. 7. It can be observed that a)\\nFig. 7 Radial distortion types.\\nradial distortions are symmetric about the image center and b) straight lines\\nare no longer preserved. In barrel distortion, the image magniﬁcation decreases'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 9}, page_content='with the distance from the optical axis (lines curve outwards). In contrast, the\\npincushion distortion pinches the image (lines curve inwards). Mustache dis-\\ntortion is a mixture of the above two distortion types. It starts out as barrel\\ndistortion close to the optical axis and gradually turns into pincushion distor-\\ntion close to image periphery. Barrel distortion is commonly applied in ﬁsh-eye\\nlenses to produce wide-angle/panoramic images, while pincushion distortion is'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 9}, page_content='often associated with telephoto/zoom lenses. Radial distortions can be corrected\\nusing3:\\n𝑥undist =𝑥dist¹1 ¸𝑘1𝑟2 ¸𝑘2𝑟4 ¸𝑘3𝑟6º\\x96\\n𝑦undist =𝑦dist¹1 ¸𝑘1𝑟2 ¸𝑘2𝑟4 ¸𝑘3𝑟6º\\x96\\n(12)\\nwhere the corrected point will bepundist = »𝑥undist\\x96𝑦undist¼>; 𝑟2 =𝑥dist2 ¸𝑦dist2;\\n𝑥dist = 𝑥C\\n𝑧C = 𝑢\\x00𝑢𝑜\\n𝑓𝑥\\nand 𝑦dist = 𝑦C\\n𝑧C = 𝑣\\x00𝑣𝑜\\n𝑓𝑦\\n4 can be obtained from the distorted\\nimage. 𝑘1, 𝑘2 and 𝑘3 are three intrinsic parameters used for radial distortion\\ncorrection. They can be estimated using a collection of images containing a'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 9}, page_content='planar checkerboard pattern.\\n3 docs.opencv.org/2.4/doc/tutorials/calib3d/camera_calibration/camera_calibration.\\nhtml\\n4 docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 10}, page_content='Computer Stereo Vision for Autonomous Driving 11\\nTangential distortion\\nSimilar to radial distortion, tangential distortion can also be corrected using:\\n𝑥undist =𝑥dist ¸\\n\\x02\\n2𝑝1𝑥dist𝑦dist ¸𝑝2 ¹𝑟2 ¸2𝑥dist\\n2º\\n\\x03\\n\\x96\\n𝑦undist =𝑦dist ¸\\n\\x02\\n𝑝1 ¹𝑟2 ¸2𝑦dist\\n2º¸2𝑝2𝑥dist𝑦dist\\n\\x03\\n\\x96\\n(13)\\nwhere 𝑝1 and 𝑝2 are two intrinsic parameters, which can also be estimated using\\na collection of images containing a planar checkerboard pattern.\\n4.2.4 Epipolar Geometry'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 10}, page_content='4.2.4 Epipolar Geometry\\nThe generic geometry of stereo vision is known asepipolar geometry. An example\\nof the epipolar geometry is shown in Fig. 8.ΠL and ΠR represent the left and\\nFig. 8 Epipolar geometry.\\nright image planes, respectively.oC\\nL and oC\\nR denote the origins of the left camera\\ncoordinate system (LCCS) and the right camera coordinate system (RCCS),\\nrespectively. The 3D pointpW = »𝑥W\\x96𝑦W\\x96𝑧W¼> in the WCS, is projected at'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 10}, page_content='\\x16 pL = »𝑥L\\x96𝑦L\\x96 𝑓L¼> on ΠL and at \\x16 pR = »𝑥R\\x96𝑦R\\x96 𝑓R¼> on ΠR, respectively. 𝑓L\\nand 𝑓R are the focal lengths of the left and right cameras, respectively; The\\nrepresentations ofpW in the LCCS and RCCS arepC\\nL =»𝑥C\\nL \\x96𝑦C\\nL \\x96𝑧C\\nL ¼>= 𝑧C\\nL\\n𝑓L\\n\\x16 pL and\\npC\\nR =»𝑥C\\nR\\x96𝑦C\\nR\\x96𝑧C\\nR¼>= 𝑧C\\nR\\n𝑓R\\n\\x16 pR, respectively. According to (4),pC\\nL can be transformed\\ninto pC\\nR using:\\npC\\nR =RpC\\nL ¸t\\x96 (14)\\nwhere R 2R3\\x023 is a rotation matrix andt 2R3\\x021 is a translation vector.eC\\nL and\\neC'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 10}, page_content='L and\\neC\\nR denote the left and rightepipoles, respectively. Theepipolar planeis uniquely\\ndeﬁned byoC\\nL , oC\\nR and pW. It intersectsΠL and ΠR giving rise to twoepipolar\\nlines, as shown in Fig. 8. Using (11),pC\\nL and pC\\nR can be normalized using:'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 11}, page_content='12 Rui Fan, Li Wang, Mohammud Junaid Bocus, Ioannis Pitas\\n^ pC\\nL = pC\\nL\\n𝑧C\\nL\\n=KL\\n\\x001~ pL\\x96 ^ pC\\nR = pC\\nR\\n𝑍C\\nR\\n=KR\\n\\x001~ pR\\x96 (15)\\nwhere KL and KR denote the intrinsic matrices of the left and right cameras,\\nrespectively.~ pL =»pL>\\x961¼>and ~ pR =»pR>\\x961¼>are the homogeneous coordinates\\nof the image pixelspL =»𝑢L\\x96𝑣L¼> and pR =»𝑢R\\x96𝑣R¼>, respectively.\\n4.2.5 Essential Matrix\\nEssential matrix E 2R3\\x023 was ﬁrst introduced by Longuet-Higgins in 1981 [65].'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 11}, page_content='A simple way of introducing the deﬁning equation ofE is to multiply both sides\\nof (14) bypC\\nR\\n>»t¼\\x02:\\npC\\nR\\n>\\n»t¼\\x02pC\\nR =pC\\nR\\n>\\n»t¼\\x02¹RpC\\nL ¸tº\\x95 (16)\\nAccording to (3), (16) can be rewritten as follows:\\n\\x00pC\\nR\\n>\\n»pC\\nR¼\\x02t =pC\\nR\\n>\\n»t¼\\x02RpC\\nL ¸pC\\nR\\n>\\n»t¼\\x02t\\x95 (17)\\nApplying (2) to (17) yields:\\npC\\nR\\n>\\n»t¼\\x02RpC\\nL =pC\\nR\\n>\\nEpC\\nL =0\\x96 (18)\\nThe essential matrixE is deﬁned by:\\nE =»t¼\\x02R (19)\\nPlugging (15) into (18) results in:\\n^pC\\nR\\n>E^pC\\nL =0\\x96 (20)'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 11}, page_content='^pC\\nR\\n>E^pC\\nL =0\\x96 (20)\\nwhich depicts the relationship between each pair of normalized points^pC\\nR and\\n^pC\\nL lying on the same epipolar plane. It is important to note here thatE has ﬁve\\ndegrees of freedom: bothR and t have three degrees of freedom, but the overall\\nscale ambiguity causes the degrees of freedom to be reduced by one [27]. Hence,\\nin theory,E can be estimated with at least ﬁve pairs ofpC\\nL and pC\\nR. However,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 11}, page_content='L and pC\\nR. However,\\ndue to the non-linearity ofE, its estimation using ﬁve pairs of correspondences is\\nalways intractable. Therefore,E is commonly estimated with at least eight pairs\\nof pC\\nL and pC\\nR [62], as discussed in Sec. 4.2.6.\\n4.2.6 Fundamental Matrix\\nAs introduced in Sec. 4.2.5, the essential matrix creates a link between a given\\npair of corresponding 3D points in the LCCS and RCCS. When the intrinsic\\nmatrices of the two cameras in a stereo rig are unknown, the relationship between'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 12}, page_content='Computer Stereo Vision for Autonomous Driving 13\\neach pair of corresponding 2D image pixelspL = »𝑢L\\x96𝑣L¼> and pR = »𝑢R\\x96𝑣R¼>\\ncan be established, using thefundamental matrixF 2R3\\x023. It can be considered\\nas a generalization ofE, where the assumption of calibrated cameras is removed\\n[27]. Applying (15) to (20) yields:\\n~p>\\nRKR\\n\\x00>EKL\\n\\x001~pL =~p>\\nRF~pL =0\\x96 (21)\\nwhere the fundamental matrixF is deﬁned as:\\nF =KR\\n\\x00>EKL\\n\\x001\\x95 (22)\\nF has seven degrees of freedom: a 3\\x023 homogeneous matrix has eight indepen-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 12}, page_content='dent ratios, as there are nine entries, but the common scaling is not signiﬁcant.\\nHowever, F also satisﬁes the constraint det¹Fº= 0, which removes one degree\\nof freedom [27]. The most commonly used algorithm to estimateE and F is\\n“eight-point algorithm” (EPA), which was introduced by Hartley in 1997 [66].\\nThis algorithm is based on the scale invariance ofE and F: 𝜆EpC\\nR\\n>EpC\\nL =0 and\\n𝜆F~p>\\nRF~pL = 0, where𝜆E\\x96𝜆F ≠ 0. By setting one element inE and F to 1, eight'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 12}, page_content='unknown elements still need to be estimated. This can be done using at least\\neight correspondence pairs. If the intrinsic matricesKL and KR of the two cam-\\neras are known, the EPA only needs to be carried out once to estimate eitherE\\nor F, because the other one can be easily worked out using (21).\\n4.2.7 Homography Matrix\\nAn arbitrary 3D pointpW =»𝑥W\\x96𝑦W\\x96𝑧W¼> lying on a planar surface satisﬁes:\\nn>pW ¸𝑏=0\\x96 (23)\\nwhere n = »𝑛𝑥\\x96𝑛𝑦\\x96𝑛𝑧¼> is the normal vector of the planar surface. Its corre-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 12}, page_content='sponding pixelspL = »𝑢L\\x96𝑣L¼> and pR = »𝑢R\\x96𝑣R¼> in the left and right images,\\nrespectively, can be linked by ahomography matrixH 2R3\\x023. The expression of\\nthe planar surface can be rearranged as follows:\\n\\x00n>pW\\x9d𝑏=1\\x95 (24)\\nAssuming thatpC\\nL =pW and plugging (24) and (15) into (14) results in:\\npC\\nR =RpC\\nL \\x001\\n𝑏tn>pC\\nL =\\n\\x12\\nR \\x001\\n𝑏tn>\\n\\x13\\n𝑧C\\nL KL\\n\\x001~pL =𝑧C\\nRKR\\n\\x001~pR (25)\\nTherefore, ~pL and ~pR can be linked using:\\n~pR = 𝑧C\\nL\\n𝑧C\\nR\\nKR\\n\\x12\\nR \\x001\\n𝑏tn>\\n\\x13\\nKL\\n\\x001~pL =H~pL\\x95 (26)'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 13}, page_content='14 Rui Fan, Li Wang, Mohammud Junaid Bocus, Ioannis Pitas\\nThe homography matrix H is generally used to distinguish obstacles from a\\nplanar surface [67]. For a well-calibrated stereo vision system,R, t, KL as well\\nas KR are already known, and𝑧C\\nL is typically equal to𝑧C\\nR. Thus,H only relates\\nto n and 𝑏, and it can be estimated with at least four pairs of correspondences\\npL and pR [67].\\n4.3 Stereopsis\\n4.3.1 Stereo Rectiﬁcation'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 13}, page_content='4.3 Stereopsis\\n4.3.1 Stereo Rectiﬁcation\\n3D scene geometry reconstruction with a pair of synchronized cameras is based\\non determining pairs of correspondence pixels between the left and right images.\\nFor an uncalibrated stereo rig, ﬁnding the correspondence pairs is a 2D search\\nprocess (optical ﬂow estimation), which is extremely computationally intensive.\\nIf the stereo rig is calibrated, 1D search should be performed along the epipolar'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 13}, page_content='lines. An image transformation process, referred to asstereo rectiﬁcation, is al-\\nways performed beforehand to reduce the dimension of the correspondence pair\\nsearch. The stereo rectiﬁcation consists of four main steps [62]:\\nFig. 9 stereo rectiﬁcation.\\n1. Rotate the left camera byRrect so that the left image plane is parallel to the\\nvector t;\\n2. Apply the same rotation to the right camera to recover the original epipolar\\ngeometry;\\n3. Rotate the right camera byR\\x001;'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 13}, page_content='geometry;\\n3. Rotate the right camera byR\\x001;\\n4. Adjust the left and right image scales by allocating an identical intrinsic\\nmatrix to both cameras.\\nAfter the stereo rectiﬁcation, the left and right images appear as if they were\\ntaken by a pair of parallel cameras with the same intrinsic parameters, as shown'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 14}, page_content='Computer Stereo Vision for Autonomous Driving 15\\nin Fig. 9, where ΠL and ΠR are the original image planes; Π0\\nL and Π0\\nR are\\nthe rectiﬁed image planes. Also, each pair of conjugate epipolar lines become\\ncollinear and parallel to the horizontal image axis [62]. Hence, determining the\\ncorrespondence pairs is simpliﬁed to a 1D search problem.\\n4.3.2 Stereo Vision System\\nA well-rectiﬁed stereo vision system is illustrated in Fig. 10, which can be re-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 14}, page_content='Fig. 10 Basic stereo vision system.pW can be transformed to\\x16pC\\nL and \\x16pC\\nR using (29).\\ngarded as a special epipolar geometry introduced in Sec. 4.2.4, where the left and\\nright cameras are perfectly parallel to each other.𝑥C\\nL and 𝑥C\\nR axes are collinear.\\noC\\nL andoC\\nL are the left and right camera optical centers, respectively. The baseline\\nof the stereo rig𝑇𝑐, is deﬁned as the distance betweenoC\\nL and oC\\nR. The intrinsic\\nmatrices of the left and right cameras are given by:\\nKL =KR =K =\\n266664'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 14}, page_content='KL =KR =K =\\n266664\\n𝑓 0 𝑢𝑜\\n0 𝑓 𝑣𝑜\\n0 0 1\\n377775\\n\\x96 (27)\\nrespectively. Let pW = »𝑥W\\x96𝑦W\\x96𝑧W¼> be a 3D point of interest in the WCS.\\nIts representations in the LCCS and RCCS arepC\\nL = »𝑥C\\nL \\x96𝑦C\\nL \\x96𝑧C\\nL ¼> and pC\\nR =\\n»𝑥C\\nR\\x96𝑦C\\nR\\x96𝑧C\\nR¼>, respectively. Since the left and right cameras are considered to\\nbe exactly the same in a well-rectiﬁed stereo vision system,𝑠𝑥 and 𝑠𝑦 in (9)\\nare simply set to1 and 𝑓𝑥 = 𝑓𝑦 = 𝑓. pW is projected on ΠL and ΠR at \\x16pL ='),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 14}, page_content='»𝑥L\\x96𝑦L\\x96 𝑓¼> and \\x16pR = »𝑥R\\x96𝑦R\\x96 𝑓¼>, respectively.oW, the origin of the WCS, is\\nat the center of the line segment𝐿 = f𝑡oC\\nL ¸¹1 \\x00𝑡ºoC\\nR j𝑡 2»0\\x961¼g. 𝑧W axis is\\nparallel to the camera optical axes and perpendicular toΠL and ΠR. Therefore,\\nan arbitrary pointpW in the WCS can be transformed topC\\nL and pC\\nR using:\\npC\\nL =IpW ¸tL\\x96 pC\\nR =IpW ¸tR\\x96 (28)'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 15}, page_content='16 Rui Fan, Li Wang, Mohammud Junaid Bocus, Ioannis Pitas\\nwhere tL = »𝑇𝑐\\n2 \\x960\\x960¼> and tR = »\\x00𝑇𝑐\\n2 \\x960\\x960¼>; Applying (11) and (15) to (28)\\nresults in the following expressions:\\n𝑥L = 𝑓𝑥W ¸𝑇𝑐\\x9d2\\n𝑧W \\x96 𝑦 L = 𝑓 𝑦W\\n𝑧W \\x96\\n𝑥R = 𝑓𝑥W \\x00𝑇𝑐\\x9d2\\n𝑧W \\x96 𝑦 R = 𝑓 𝑦W\\n𝑧W \\x95\\n(29)\\nApplying (29) to (9) yields the following expressions:\\npL =\\n\\x14𝑢L\\n𝑣L\\n\\x15\\n=\\n\"\\n𝑓 𝑥W\\n𝑧W ¸𝑢𝑜 ¸𝑓 𝑇𝑐\\n2𝑧W\\n𝑓 𝑦W\\n𝑧W ¸𝑣𝑜\\n#\\n\\x96 pR =\\n\\x14𝑢R\\n𝑣R\\n\\x15\\n=\\n\"\\n𝑓 𝑥W\\n𝑧W ¸𝑢𝑜 \\x00𝑓 𝑇𝑐\\n2𝑧W\\n𝑓 𝑦W\\n𝑧W ¸𝑣𝑜\\n#\\n\\x95 (30)\\nTherelationshipbetweentheso-calleddisparity 𝑑anddepth 𝑧W isasfollows[26]:'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 15}, page_content='𝑑=𝑢L \\x00𝑢R = 𝑓 𝑇𝑐\\n𝑧W \\x95 (31)\\nIt can be observed that𝑑is inversely proportional to𝑧W. Therefore, for a distant\\n3D pointpW, pL and pR are close to each other. On the other hand, whenpW\\nlies near the stereo camera rig, the position diﬀerence betweenpL and pR is\\nlarge. Therefore, disparity estimation can be regarded as a task of 1) ﬁnding the\\ncorrespondence (pL and pR) pairs, which are on the same image row, on the left\\nand right images and 2) producing two disparity imagesDL and DR, as shown'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 15}, page_content='in Fig. 11.\\nFig. 11 (a) left image, (b) right image, (c) left disparity imageDL and (d) right disparity\\nimage DR.\\n4.3.3 Disparity Estimation\\nThe two key aspects of computer stereo vision are speed and accuracy [68]. Over\\nthe past two decades, a lot of research has been carried out to improve dispar-\\nity estimation accuracy while reducing computational complexity. However, the\\nstereo vision algorithms designed to achieve better disparity accuracy typically'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 15}, page_content='have higher computational complexity [26]. Hence, speed and accuracy are two\\ndesirable but conﬂicting properties. It is very challenging to achieve both of them\\nsimultaneously [68].'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 16}, page_content='Computer Stereo Vision for Autonomous Driving 17\\nIn general, the main motivation of designing a stereo vision algorithm is to\\nimprove the trade-oﬀ between speed and accuracy. In most circumstances, a de-\\nsirable trade-oﬀ entirely depends on the target application [68]. For instance,\\na real-time performance is required for stereo vision systems employed in au-\\ntonomous driving, because other systems, such as data-fusion semantic driving'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 16}, page_content='scene segmentation, usually take up only a small portion of the processing time,\\nand can be easily implemented in real-time if the 3D information is available [26].\\nAlthough stereo vision execution time can deﬁnitely be reduced with future HW\\nadvances, algorithm and SW improvements are also very important [68].\\nState-of-the-art stereo vision algorithms can be classiﬁed as either computer\\nvision-based or machine/deep learning-based. The former typically formulates'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 16}, page_content='disparity estimation as a local block matching problem or a global energy min-\\nimization problem [67], while the latter basically considers disparity estimation\\nas a regression problem [69].\\nComputer vision-based stereo vision algorithms\\nComputer vision-based disparity estimation algorithms are categorized as: 1) lo-\\ncal, 2) global and 3) semi-global [70]. Local algorithms simply select an image\\nblock from the left image and match it with a series of image blocks selected'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 16}, page_content='from the right image. Optimal disparity estimation corresponds to either the low-\\nest diﬀerence costs or the highest correlation costs. Global algorithms translate\\ndisparity estimation into a probability maximization problem or an energy min-\\nimization problem [71], which can be solved using Markov random ﬁeld (MRF)-\\nbased optimization algorithms [72]. Semi-global matching (SGM) [73] approxi-\\nmates MRF inferences by performing cost aggregation along all image directions,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 16}, page_content='which greatly improves both disparity estimation accuracy and eﬃciency. Gen-\\nerally, a computer vision-based disparity estimation algorithm consists of four\\nmain steps: 1) cost computation, 2) cost aggregation, 3) disparity optimization\\nand 4) disparity reﬁnement [74].\\n1. Cost computation\\nDisparity 𝑑 is a random variable with𝑁 possible discrete states, each of them\\nbeing associated with a matching cost𝑐. The two most commonly used pixel-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 16}, page_content='wise matching costs are the absolute diﬀerence (AD) cost𝑐AD and the squared\\ndiﬀerence (SD) cost 𝑐SD [74]. Since the left and right images are typically in\\ngray-scale format,𝑐AD and 𝑐SD can be computed using [75]:\\n𝑐AD¹p\\x96𝑑º=\\n\\x0c\\x0c𝑖L¹pº\\x00𝑖R¹p \\x00dº\\n\\x0c\\x0c\\x96\\n𝑐SD¹p\\x96𝑑º= \\x00𝑖L¹pº\\x00𝑖R¹p \\x00dº\\x012\\x96\\n(32)'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 17}, page_content='18 Rui Fan, Li Wang, Mohammud Junaid Bocus, Ioannis Pitas\\nwhere d = »𝑑\\x960¼>, 𝑖L¹pºdenotes the pixel intensity ofp = »𝑢\\x96𝑣¼> in the left\\nimage and𝑖R¹p \\x00dºrepresents the pixel intensity ofp \\x00d = »𝑢\\x00𝑑\\x96𝑣¼> in the\\nright image.\\n2. Cost aggregation\\nIn order to minimize incorrect matches, pixel-wise diﬀerence costs are often\\naggregated over all pixels within a support region [70]:\\n𝑐agg¹p\\x96𝑑º=𝑤¹p\\x96𝑑º\\x03𝐶¹p\\x96𝑑º\\x96 (33)\\nwhere the center of the support region is atp = »𝑢\\x96𝑣¼>. The corresponding dis-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 17}, page_content='parity is 𝑑. 𝑐agg denotes the aggregated cost.𝑤 is a kernel that represents the\\nsupport region. 𝐶 represents a neighborhood system containing the pixel-wise\\nmatching costs of all pixels within the support region.𝑐agg can be obtained by\\nperforming a convolution between𝑤 and 𝐶. A large support region can help\\nreduce disparity optimization uncertainties, but also increase the algorithm ex-\\necution time signiﬁcantly.\\nSince the support regions are always rectangular blocks, these algorithms are'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 17}, page_content='also known as stereo block matching [67]. When the convolution process is a\\nuniform box ﬁltering (all the elements in𝑤 are 1), the aggregations of𝑐AD and\\n𝑐SD are referred to as the sum of absolute diﬀerence (SAD) and the sum of\\nsquared diﬀerence (SSD), respectively, which can be written as [26]:\\n𝑐SAD¹p\\x96𝑑º=\\n∑︁\\nq2𝒩p\\n\\x0c\\x0c𝑖L¹qº\\x00𝑖R¹q \\x00dºº\\n\\x0c\\x0c\\x96\\n𝑐SSD¹p\\x96𝑑º=\\n∑︁\\nq2𝒩p\\n\\x00𝑖L¹qº\\x00𝑖R¹q \\x00dº\\x012\\x96\\n(34)\\nwhere 𝒩p is the support region (or neighborhood system) ofp. Although the'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 17}, page_content='SAD and the SSD are computationally eﬃcient, they are very sensitive to im-\\nage intensity noise. In this regard, some other cost or similarity functions, such\\nas the normalized cross-correlation (NCC), are more prevalently used for cost\\ncomputation and aggregation. The cost function of the NCC is as follows [67]:\\n𝑐NCC¹p\\x96𝑑º= 1\\n𝑛𝜎L𝜎R\\n∑︁\\nq2𝒩p\\n\\x10\\n𝑖L\\n\\x00q\\x01 \\x00𝜇L\\n\\x11\\x10\\n𝑖R\\n\\x00q \\x00d\\x01 \\x00𝜇R\\n\\x11\\n\\x96 (35)\\nwhere\\n𝜎L =\\nvt ∑︁\\nq2𝒩p\\n\\x10\\n𝑖L¹qº\\x00 𝜇L\\n\\x112\\n\\x9d𝑛\\x96 𝜎 R =\\nvt ∑︁\\nq2𝒩p\\n\\x10\\n𝑖R¹q \\x00dº\\x00 𝜇R\\n\\x112\\n\\x9d𝑛\\x96 (36)'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 17}, page_content='\\x9d𝑛\\x96 𝜎 R =\\nvt ∑︁\\nq2𝒩p\\n\\x10\\n𝑖R¹q \\x00dº\\x00 𝜇R\\n\\x112\\n\\x9d𝑛\\x96 (36)\\n𝜇L and 𝜇R represent the means of the pixel intensities within the left and right\\nimage block, respectively.𝜎L and 𝜎R denote the standard deviations of the left'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 18}, page_content='Computer Stereo Vision for Autonomous Driving 19\\nand right image block, respectively.𝑛 represents the number of pixels within\\neach image blocks. The NCC cost𝑐NCC 2»\\x001\\x961¼reﬂects the similarity between\\nthe given pair of left and right image blocks. A higher𝑐NCC corresponds to a\\nbetter block matching.\\nIn addition to the cost aggregation via uniform box ﬁltering, many adaptive\\ncost aggregation strategies have been proposed to improve disparity accuracy.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 18}, page_content='One of the most famous algorithms is fast bilateral stereo (FBS) [76,77], which\\nuses a bilateral ﬁlter to aggregate the matching costs adaptively. A general ex-\\npression of cost aggregation in FBS is as follows [78]:\\n𝑐agg¹p\\x96𝑑º=\\nÍ\\nq2𝒩q 𝜔𝑑 ¹qº𝜔𝑟 ¹qº𝑐¹q\\x96𝑑º\\nÍ\\nq2𝒩q 𝜔𝑑 ¹qº𝜔𝑟 ¹qº \\x96 (37)\\nwhere functions𝜔𝑑 and 𝜔𝑟 are based on spatial distance and color similarity, re-\\nspectively [77]. The costs𝑐within a rectangular block are aggregated adaptively\\nto produce𝑐agg.\\n3. Disparity Optimization'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 18}, page_content='to produce𝑐agg.\\n3. Disparity Optimization\\nThe local algorithms simply select the disparities that correspond to the low-\\nest diﬀerence costs or the highest correlation costs as the best disparities in a\\nWinner-Take-All (WTA) way.\\nUnlike WTA applied in the local algorithms, matching costs from neighboring\\npixels are also taken into account in the global algorithms,e.g., graph cuts (GC)\\n[79] and belief propagation (BP) [80]. The MRF is a commonly used graphical'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 18}, page_content='model in such algorithms. An example of the MRF model is depicted in Fig.\\n12. The graph𝒢 = ¹𝒫\\x96ℰºis a set of vertices𝒫 connected by edgesℰ, where\\n𝒫 = fp11\\x96p12\\x96\\x01\\x01\\x01 \\x96p𝑚𝑛gand ℰ = f¹p𝑖 𝑗\\x96p𝑠𝑡 º jp𝑖 𝑗\\x96p𝑠𝑡 2𝒫g. Two edges sharing\\none common vertex are called a pair of adjacent edges [81]. Since the MRF is\\nconsidered to be undirected,¹p𝑖 𝑗\\x96p𝑠𝑡 ºand ¹p𝑠𝑡 \\x96p𝑖 𝑗ºrefer to the same edge here.\\n𝒩𝑖 𝑗=fq1p𝑖 𝑗\\x96q2p𝑖 𝑗\\x96\\x01\\x01\\x01 \\x96q𝑘p𝑖 𝑗 jqp𝑖 𝑗 2𝒫gis a neighborhood system ofp𝑖 𝑗.\\nFig. 12 MRF model.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 18}, page_content='Fig. 12 MRF model.\\nFor stereo vision problems, 𝒫 is a 𝑚 \\x02𝑛 pixel disparity image and p𝑖 𝑗 is\\na graph vertex (or node) at the site of¹𝑖\\x96𝑗 ºwith a disparity node value𝑑𝑖 𝑗.\\nBecause the consideration of more candidates usually makes true disparity in-\\nference intractable, only the neighbors adjacent top𝑖 𝑗 are considered for stereo\\nmatching [72], in a pairwise MRF fashion, as the disparity of nodep𝑖 𝑗 tends to\\nhave a strong correlation with its vicinities, while it is linked implicitly to any'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 19}, page_content='20 Rui Fan, Li Wang, Mohammud Junaid Bocus, Ioannis Pitas\\nother random nodes in the disparity map. The joint MRF probability can be\\nwritten as [72]:\\n𝑃¹p\\x96𝑞º=\\nÖ\\np𝑖 𝑗2𝒫\\nΦ¹p𝑖 𝑗\\x96𝑞p𝑖 𝑗º\\nÖ\\nqp𝑖 𝑗2𝒩𝑖 𝑗\\nΨ¹p𝑖 𝑗\\x96qp𝑖 𝑗º\\x96 (38)\\nwhere𝑞p𝑖 𝑗 representsimageintensitydiﬀerences, Φ¹\\x01ºexpressesthecompatibility\\nbetween possible disparities and the corresponding image intensity diﬀerences,\\nwhile Ψ¹\\x01ºexpresses the compatibility betweenp𝑖 𝑗 and its neighborhood system.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 19}, page_content='Now, the aim of ﬁnding the best disparity is equivalent to maximizing𝑃¹p\\x96𝑞º\\nin (38), by formulating it as an energy function [63]:\\n𝐸¹pº=\\n∑︁\\np𝑖 𝑗2𝒫\\n𝐷¹p𝑖 𝑗\\x96𝑞p𝑖 𝑗º¸\\n∑︁\\nqp𝑖 𝑗2𝒩𝑖 𝑗\\n𝑉¹p𝑖 𝑗\\x96qp𝑖 𝑗º\\x96 (39)\\nwhere 𝐷¹\\x01ºand 𝑉¹\\x01ºare two energy functions.𝐷¹\\x01ºcorresponds to the matching\\ncost and𝑉¹\\x01ºdetermines the aggregation from the neighbors. In the MRF model,\\nthe method to formulate an adaptive𝑉¹\\x01ºis important, because image intensity'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 19}, page_content='in discontinuous areas usually varies greatly from that of its neighbors [26]. How-\\never, the process of minimizing (39) results in high computational complexities,\\nrendering real-time performance challenging. Therefore, SGM [73] breaks down\\n(39) into:\\n𝐸¹Dº=\\n∑︁\\np\\n\\x12\\n𝑐¹p\\x96𝑑pº¸\\n∑︁\\nq2𝒩p\\n𝜆1𝛿¹j𝑑p \\x00𝑑qj=1º¸\\n∑︁\\nq2𝒩p\\n𝜆2𝛿¹j𝑑p \\x00𝑑qj¡1º\\n\\x13\\n\\x96 (40)\\nwhere D is the disparity image,𝑐 is the matching cost,q is a pixel in the neigh-\\nborhood system𝒩p of p. 𝜆1 penalizes the neighboring pixels with small disparity'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 19}, page_content='diﬀerences, i.e., one pixel;𝜆2 penalizes the neighboring pixels with large dispar-\\nity diﬀerences,i.e., larger than one pixel.𝛿¹\\x01ºreturns 1 if its argument is true\\nand 0 otherwise.\\n4. Disparity Reﬁnement\\nDisparity reﬁnement usually involves several post-processing steps, such as the\\nleft-and-right disparity consistency check (LRDCC), subpixel enhancement and\\nweighted median ﬁltering [82]. The LRDCC can remove most of the occluded'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 19}, page_content='areas, which are only visible in one of the left/right image [67]. In addition, a\\ndisparity error larger than one pixel may result in a non-negligible 3D geome-\\ntry reconstruction error [67]. Therefore, subpixel enhancement provides an easy\\nway to increase disparity image resolution by simply interpolating the matching\\ncosts around the initial disparity [82]. Moreover, a median ﬁlter can be applied\\nto the disparity image to ﬁll the holes and remove the incorrect matches [82].'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 19}, page_content='However, the above disparity reﬁnement algorithms are not always necessary\\nand the sequential use of these steps depends entirely on the chosen algorithm\\nand application needs.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 20}, page_content='Computer Stereo Vision for Autonomous Driving 21\\nMachine/deep learning-based stereo vision algorithms\\nWith recent advances in machine/deep learning, CNNs have been prevalently\\nused for disparity estimation. For instance, Žbontar and LeCun [83] utilized a\\nCNN to compute patch-wise similarity scores, as shown in Fig. 13. It consists of\\na convolutional layer𝐿1 and seven fully-connected layers𝐿2-𝐿8. The inputs to\\nthis CNN are two 9\\x029-pixel gray-scale image patches.𝐿1 consists of 32 convolu-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 20}, page_content='Fig. 13 The architecture of the CNN proposed in [83] for stereo matching.\\ntion kernels of size 5\\x025\\x021. 𝐿2 and 𝐿3 have 200 neurons each. After𝐿3, the two\\n200-dimensional feature vectors are concatenated into a 400-dimensional vector\\nand passed through𝐿4-𝐿7 layers. Layer𝐿8 maps 𝐿7 output into two real num-\\nbers, which are then fed through a softmax function to produce a distribution\\nover the two classes: a) good match and b) bad match. Finally, they utilize'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 20}, page_content='computer vision-based cost aggregation and disparity optimization/reﬁnement\\ntechniques to produce the ﬁnal disparity images. Although this method has\\nachieved the state-of-the-art accuracy, it is limited by the employed matching\\ncost aggregation technique and can produce wrong predictions in occluded or\\ntexture-less/reﬂective regions [84].\\nIn this regard, some researchers have leveraged CNNs to improve computer\\nvision-based cost aggregation step. SGM-Nets [85] is one of the most well-known'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 20}, page_content='methods of this type. Its main contribution is a CNN-based technique for pre-\\ndicting SGM penalty parameters𝜆1 and 𝜆2 in (40) [73], as illustrated in Fig. 14.\\nA 5 \\x025-pixel gray-scale image patch and its normalized position are used as the\\nCNN inputs. It has a) two convolution layers, each followed by a rectiﬁed linear\\nunit (ReLU) layer; b) a concatenate layer for merging the two types of inputs;\\nc) two fully connected (FC) layers of size 128 each, followed by a ReLU layer'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 20}, page_content='and an exponential linear unit (ELU); d) a constant layer to keep SGM penalty\\nvalues positive. The costs can then be accumulated along four directions. The\\nCNN output values correspond to standard parameterization.\\nRecently, end-to-end deep CNNs have become very popular. For example,\\nMayeret al.[86] created three large synthetic datasets5 ( FlyingThings3D, Driv-\\n5 lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 21}, page_content='22 Rui Fan, Li Wang, Mohammud Junaid Bocus, Ioannis Pitas\\nFig. 14 SGM-Nets [85] architecture.\\ning and Monkaa) and proposed a CNN named DispNet for dense disparity esti-\\nmation.Lateron,Pang et al.[87]proposedatwo-stagecascadeCNNfordisparity\\nestimation. Its the ﬁrst stage enhances DispNet [86] by equipping it with extra\\nup-convolution modules and the second stage rectiﬁes the disparity initialized by\\nthe ﬁrst stage and generates residual signals across multiple scales. Furthermore,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 21}, page_content='GCNet [88] incorporated feature extraction (cost computation), cost aggregation\\nand disparity optimization/reﬁnement into a single end-to-end CNN model, and\\nit achieved the state-of-the-art accuracy on the FlyingThings3D benchmark [86]\\nas well as the KITTI stereo 2012 and 2015 benchmarks [89–91]. In 2018, Chang\\net al.[92] proposed Pyramid Stereo Matching Network (PSMNet), consisting of\\ntwo modules: a) spatial pyramid pooling and b) 3D CNN. The former aggre-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 21}, page_content='gates the context of diﬀerent scales and locations, while the latter regularizes\\nthe cost volume. Unlike PSMNet [92], guided aggregation net (GANet) [84] re-\\nplaces the widely used 3D CNN with two novel layers: a semi-global aggregation\\nlayer and a local guided aggregation layer, which help save a lot of memory and\\ncomputational cost.\\nAlthough the aforementioned CNN-based disparity estimation methods have\\nachieved compelling results, they usually have a huge number of learnable pa-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 21}, page_content='rameters, resulting in a long processing time. Therefore, current state-of-the-art\\nCNN-based disparity estimation algorithms have hardly been put into practical\\nuses in autonomous driving. We believe these methods will be applied in more\\nreal-world applications, with future advances in embedded computing HW.\\n4.3.4 Performance Evaluation\\nAs discussed above, disparity estimation speed and accuracy are two key prop-\\nerties and they are always pitted against each other. Therefore, the performance'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 21}, page_content='evaluation of a given stereo vision algorithm usually involves both of these two\\nproperties [68].\\nThe following two metrics are commonly used to evaluate the accuracy of an\\nestimated disparity image [93]:\\n1. Root mean squared (RMS) error𝑒RMS:'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 22}, page_content='Computer Stereo Vision for Autonomous Driving 23\\n𝑒RMS =\\n√︄\\n1\\n𝑁\\n∑︁\\np2𝒫\\njDE¹pº\\x00DG¹pºj2\\x96 (41)\\n2. Percentage of error pixels (PEP)𝑒PEP (tolerance: 𝛿𝑑 pixels):\\n𝑒PEP = 1\\n𝑁\\n∑︁\\np2𝒫\\n𝛿\\n\\x12\\njDE¹pº\\x00DG¹pºj¡𝛿 𝑑\\n\\x13\\n\\x02100%\\x96 (42)\\nwhere DE and DG represent the estimated and ground truth disparity images,\\nrespectively; 𝑁 denotes the total number of disparities used for evaluation;𝛿𝑑\\nrepresents the disparity evaluation tolerance.\\nAdditionally, a general way to depict the eﬃciency of an algorithm is given'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 22}, page_content='in millions of disparity evaluations per second Mde\\x9d𝑠 [68] as follows:\\nMde\\x9d𝑠= 𝑢max𝑣max𝑑max\\n𝑡 10\\x006\\x95 (43)\\nHowever, the speed of a disparity estimation algorithm typically varies across\\ndiﬀerent platforms, and it can be greatly boosted by exploiting the parallel\\ncomputing architecture.\\n5 Heterogeneous Computing\\nHeterogeneous computing systems use multiple types of processors or cores. In\\nthe past, heterogeneous computing meant that diﬀerent instruction-set architec-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 22}, page_content='tures (ISAs) had to be handled diﬀerently, while modern heterogeneous system\\narchitecture (HSA) systems allow users to utilize multiple processor types. As\\nillustrated in Fig. 15, a typical HSA system consists of two diﬀerent types of\\nGPU Multi-Threading\\nCPU\\nPCI Express Bus\\nMain Memory\\nIOMMU\\nIOMMU\\nMMU\\nPhysical Addresses\\nDevice Addresses Virtual Addresses\\nFig. 15 Heterogeneous system architecture.\\nprocessors: 1) a multi-threading central processing unit (CPU) and 2) a graph-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 22}, page_content='ics processing unit (GPU) [94], which are connected by a peripheral component\\ninterconnect (PCI) express bus. The CPU’s memory management unit (MMU)\\nand the GPU’s input/output memory management unit (IOMMU) comply with'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 23}, page_content='24 Rui Fan, Li Wang, Mohammud Junaid Bocus, Ioannis Pitas\\nthe HSA HW speciﬁcations. CPU runs the operating system and performs tra-\\nditional serial computing tasks, while GPU performs 3D graphics rendering and\\nCNN training.\\n5.1 Multi-Threading CPU\\nTheapplicationprogramminginterface(API)OpenMulti-Processing(OpenMP)\\nis typically used to break a serial code into independent chunks for parallel\\nprocessing. It supports multi-platform shared-memory multiprocessing program-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 23}, page_content='ming in C/C++ and Fortran [95]. An explicit parallelism programming model,\\ntypically known as a fork-join model, is illustrated in Fig. 16, where the compiler\\nMaster Thread\\nMaster Thread\\nTask 1 Task 2\\nTask 1 Task 2\\nA\\nB\\nC\\nA\\nB\\nA B C A B\\nFork\\nJoinSerial Processing\\nParallel Processing\\nFig. 16 Serial processing vs. parallel processing.\\ninstructs a section of the serial code to run in parallel [96]. The master thread (se-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 23}, page_content='rial execution on one core) forks a number of slave threads. The tasks are divided\\nto run in parallel amongst the slave threads on multiple cores. Synchronization\\nwaits until all slave threads ﬁnish their allocated tasks [58]. Finally, the slave\\nthreads join together at a subsequent point and resume sequential execution.\\n5.2 GPU\\nGPUs have been extensively used in computer vision and deep learning to accel-\\nerate the computationally intensive but parallelly-eﬃcient processing and CNN'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 23}, page_content='training. Compared with a CPU, which consists of a low number of cores opti-\\nmized for sequentially serial processing, GPU has a highly parallel architecture\\nwhich is composed of hundreds or thousands of light GPU cores to handle mul-\\ntiple tasks concurrently.\\nA typical GPU architecture is shown in Fig. 17, which consists of𝑁 stream-\\ning multiprocessors (SMs) with𝑀 streaming processors (SPs) on each of them.\\nThe single instruction multiple data (SIMD) architecture allows the SPs on the'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 23}, page_content='same SM to execute the same instruction but process diﬀerent data at each clock\\ncycle. The device has its own dynamic random access memory (DRAM) which\\nconsists of global memory, constant memory and texture memory. DRAM can\\ncommunicate with the host memory via the graphical/memory controller hub'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 24}, page_content='Computer Stereo Vision for Autonomous Driving 25\\nTable 1 GPU memory comparison [97].\\nMemory Location Cached Access Scope\\nregister on-chip n/a r/w one thread\\nshared on-chip n/a r/w all threads in a block\\nglobal oﬀ-chip no r/w all threads + host\\nconstant oﬀ-chip yes r all threads + host\\ntexture oﬀ-chip yes r all threads + host\\n(GMCH) and the I/O controller hub (ICH), which are also known as the Intel\\nnorthbridge and the Intel southbridge, respectively. Each SM has four types of'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 24}, page_content='on-chip memories: register, shared memory, constant cache and texture cache.\\nSince they are on-chip memories, the constant cache and texture cache are uti-\\nlized to speed up data fetching from the constant memory and texture memory,\\nrespectively. Due to the fact that the shared memory is small, it is used for the\\nduration of processing a block. The register is only visible to the thread. The\\ndetails of diﬀerent types of GPU memories are illustrated in Table 5.2.\\nFig. 17 GPU architecture [77].'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 24}, page_content='Fig. 17 GPU architecture [77].\\nIn CUDA C programming, the threads are grouped into a set of 3D thread\\nblocks which are then organized as a 3D grid. The kernels are deﬁned on the host\\nusing the CUDA C programming language. Then, the host issues the commands\\nthat submit the kernels to devices for execution. Only one kernel can be executed\\nat a given time. Once a thread block is distributed to an SM, the threads are\\ndivided into groups of 32 parallel threads which are executed by SPs. Each group'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 24}, page_content='of 32 parallel threads is known as a warp. Therefore, the size of a thread block\\nis usually chosen as a multiple of 32 to ensure eﬃcient data processing.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 25}, page_content='26 Rui Fan, Li Wang, Mohammud Junaid Bocus, Ioannis Pitas\\n6 Summary\\nIn this chapter, we ﬁrst introduced the autonomous car system, from both HW\\naspect (car sensors and car chassis) and SW aspect (perception, localization and\\nmapping, prediction and planning, and control). Particularly, we introduced the\\nautonomous car perception module, which has four main functionalities: 1) vi-\\nsual feature detection, description and matching, 2) 3D information acquisition,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 25}, page_content='3) object detection/recognition and 4) semantic image segmentation. Later on,\\nwe provided readers with the preliminaries for the epipolar geometry and intro-\\nduced computer stereo vision from theory to algorithms. Finally, heterogeneous\\ncomputing architecture, consisting of a multi-threading CPU and a GPU, was\\nintroduced.\\nAcknowledgments\\nThis chapter has received partial funding from the European Union’s Horizon\\n2020 research and innovation programme under grant agreement No. 871479'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 25}, page_content='(AERIALCORE).\\nReferences\\n1. R. Fan, J. Jiao, H. Ye, Y. Yu, I. Pitas, and M. Liu, “Key ingredients of self-driving cars,”\\n2019.\\n2. I. Pitas,Digital image processing algorithms and applications. John Wiley & Sons, 2000.\\n3. “Lidar–light detection and ranging–is a remote sensing method used to examine the surface\\nof the earth,”NOAA. Archived from the original on, vol. 4, 2013.\\n4. T. Bureau, “Radar deﬁnition,”Public Works and Government Services Canada, 2013.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 25}, page_content='5. W. J. Westerveld, “Silicon photonic micro-ring resonators to sense strain and ultrasound,”\\n2014.\\n6. L. Zheng, Y. Zhu, B. Xue, M. Liu, and R. Fan, “Low-cost gps-aided lidar state estimation\\nand map building,” in 2019 IEEE International Conference on Imaging Systems and\\nTechniques (IST). IEEE, 2019, pp. 1–6.\\n7. N. Samama,Global positioning: Technologies and performance. John Wiley & Sons, 2008,\\nvol. 7.\\n8. S. Liu, “Chassis technologies for autonomous robots and vehicles,” 2019.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 25}, page_content='9. M. U. M. Bhutta, M. Kuse, R. Fan, Y. Liu, and M. Liu, “Loop-box: Multiagent direct slam\\ntriggeredbysingleloopclosureforlarge-scalemapping,” IEEE transactions on cybernetics,\\n2020.\\n10. R. C. Smith and P. Cheeseman, “On the representation and estimation of spatial uncer-\\ntainty,”The international journal of Robotics Research, vol. 5, no. 4, pp. 56–68, 1986.\\n11. C.Katrakazas,M.Quddus,W.-H.Chen,andL.Deka,“Real-timemotionplanningmethods'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 25}, page_content='for autonomous on-road driving: State-of-the-art and future research directions,”Trans-\\nportation Research Part C: Emerging Technologies, vol. 60, pp. 416–442, 2015.\\n12. T. H. Cormen, “Section 24.3: Dijkstra’s algorithm,”Introduction to algorithms, pp. 595–\\n601, 2001.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 26}, page_content='Computer Stereo Vision for Autonomous Driving 27\\n13. D. Delling, P. Sanders, D. Schultes, and D. Wagner, “Engineering route planning algo-\\nrithms,” inAlgorithmics of large and complex networks. Springer, 2009, pp. 117–139.\\n14. M. Willis, “Proportional-integral-derivative control,”Dept. of Chemical and Process En-\\ngineering University of Newcastle, 1999.\\n15. G. C. Goodwin, S. F. Graebe, M. E. Salgadoet al., Control system design. Upper Saddle\\nRiver, NJ: Prentice Hall„ 2001.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 26}, page_content='River, NJ: Prentice Hall„ 2001.\\n16. C. E. Garcia, D. M. Prett, and M. Morari, “Model predictive control: theory and prac-\\ntice—a survey,” Automatica, vol. 25, no. 3, pp. 335–348, 1989.\\n17. M. Hassaballah, A. A. Abdelmgeid, and H. A. Alshazly, “Image features detection, de-\\nscription and matching,” inImage Feature Detectors and Descriptors. Springer, 2016,\\npp. 11–45.\\n18. S. Liu and X. Bai, “Discriminative features for image classiﬁcation and retrieval,”Pattern'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 26}, page_content='Recognition Letters, vol. 33, no. 6, pp. 744–751, 2012.\\n19. P. Moreels and P. Perona, “Evaluation of features detectors and descriptors based on 3d\\nobjects,”International journal of computer vision, vol. 73, no. 3, pp. 263–284, 2007.\\n20. P. Dollar, C. Wojek, B. Schiele, and P. Perona, “Pedestrian detection: An evaluation of\\nthe state of the art,”IEEE Trans. Pattern Anal. Mach.Intell., vol. 34, no. 4, pp. 743–761,\\n2011.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 26}, page_content='2011.\\n21. M.Danelljan,G.Häger,F.S.Khan,andM.Felsberg,“Discriminativescalespacetracking,”\\nIEEE Trans. Pattern Anal. Mach.Intell., vol. 39, no. 8, pp. 1561–1575, 2016.\\n22. D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”International\\njournal of computer vision, vol. 60, no. 2, pp. 91–110, 2004.\\n23. H. Bay, T. Tuytelaars, and L. Van Gool, “Surf: Speeded up robust features,” inECCV.\\nSpringer, 2006, pp. 404–417.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 26}, page_content='Springer, 2006, pp. 404–417.\\n24. E. Rublee, V. Rabaud, K. Konolige, and G. Bradski, “Orb: An eﬃcient alternative to sift\\nor surf,” in2011 International conference on computer vision. Ieee, 2011, pp. 2564–2571.\\n25. S. Leutenegger, M. Chli, and R. Y. Siegwart, “Brisk: Binary robust invariant scalable\\nkeypoints,” in2011 International conference on computer vision. Ieee, 2011, pp. 2548–\\n2555.\\n26. R. Fan, “Real-time computer stereo vision for automotive applications,” Ph.D. dissertation,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 26}, page_content='University of Bristol, 2018.\\n27. R. Hartley and A. Zisserman,Multiple view geometry in computer vision. Cambridge\\nuniversity press, 2003.\\n28. H.Wang,R.Fan,andM.Liu,“Cot-amﬂow:Adaptivemodulationnetworkwithco-teaching\\nstrategy for unsupervised optical ﬂow estimation,” 2020.\\n29. S. Ullman, “The interpretation of structure from motion,”Proceedings of the Royal Society\\nof London. Series B. Biological Sciences, vol. 203, no. 1153, pp. 405–426, 1979.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 26}, page_content='30. B. Triggs, P. F. McLauchlan, R. I. Hartley, and A. W. Fitzgibbon, “Bundle adjustment—a\\nmodern synthesis,” inInternational workshop on vision algorithms. Springer, 1999, pp.\\n298–372.\\n31. H. Wang, Y. Liu, H. Huang, Y. Pan, W. Yu, J. Jiang, D. Lyu, M. J. Bocus, M. Liu,\\nI. Pitaset al., “Atg-pvd: Ticketing parking violations on a drone,”European Conference\\non Computer Vision (ECCV) Workshops, 2020.\\n32. Z.-Q. Zhao, P. Zheng, S.-t. Xu, and X. Wu, “Object detection with deep learning: A'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 26}, page_content='review,”IEEE transactions on neural networks and learning systems, vol. 30, no. 11, pp.\\n3212–3232, 2019.\\n33. D. Wang, C. Devin, Q.-Z. Cai, F. Yu, and T. Darrell, “Deep object-centric policies for\\nautonomous driving,” in 2019 International Conference on Robotics and Automation\\n(ICRA). IEEE, 2019, pp. 8853–8859.\\n34. A. Mogelmose, M. M. Trivedi, and T. B. Moeslund, “Vision-based traﬃc sign detection and\\nanalysis for intelligent driver assistance systems: Perspectives and survey,”IEEE Trans-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 26}, page_content='actions on Intelligent Transportation Systems, vol. 13, no. 4, pp. 1484–1497, 2012.\\n35. B. Wu, F. Iandola, P. H. Jin, and K. Keutzer, “Squeezedet: Uniﬁed, small, low power\\nfully convolutional neural networks for real-time object detection for autonomous driving,”\\nin Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\\nWorkshops, 2017, pp. 129–137.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 27}, page_content='28 Rui Fan, Li Wang, Mohammud Junaid Bocus, Ioannis Pitas\\n36. R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies for accurate\\nobject detection and semantic segmentation,” inProceedings of the IEEE conference on\\ncomputer vision and pattern recognition, 2014, pp. 580–587.\\n37. R.Girshick,“Fastr-cnn,” in Proceedings of the IEEE international conference on computer\\nvision, 2015, pp. 1440–1448.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 27}, page_content='vision, 2015, pp. 1440–1448.\\n38. S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time object detection\\nwith region proposal networks,” inAdvances in neural information processing systems,\\n2015, pp. 91–99.\\n39. J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once: Uniﬁed, real-\\ntime object detection,” inProceedings of the IEEE conference on computer vision and\\npattern recognition, 2016, pp. 779–788.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 27}, page_content='pattern recognition, 2016, pp. 779–788.\\n40. J. Redmon and A. Farhadi, “Yolov3: An incremental improvement,” 2018.\\n41. A. Bochkovskiy, C.-Y. Wang, and H.-Y. M. Liao, “Yolov4: Optimal speed and accuracy of\\nobject detection,” 2020.\\n42. R. Fan, H. Wang, P. Cai, and M. Liu, “Sne-roadseg: Incorporating surface normal informa-\\ntion into semantic segmentation for accurate freespace detection,” inEuropean Conference\\non Computer Vision. Springer, 2020, pp. 340–356.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 27}, page_content='on Computer Vision. Springer, 2020, pp. 340–356.\\n43. H. Wang, R. Fan, Y. Sun, and M. Liu, “Applying surface normal information in drivable\\narea and road anomaly detection for ground mobile robots,” 2020.\\n44. R. Fan, H. Wang, P. Cai, J. Wu, M. J. Bocus, L. Qiao, and M. Liu, “Learning collision-free\\nspace detection from stereo images: Homography matrix brings better data augmentation,”\\n2020.\\n45. J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for semantic segmen-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 27}, page_content='tation,” inProceedings of the IEEE conference on computer vision and pattern recognition,\\n2015, pp. 3431–3440.\\n46. O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedi-\\ncal image segmentation,” inInternational Conference on Medical image computing and\\ncomputer-assisted intervention. Springer, 2015, pp. 234–241.\\n47. V. Badrinarayanan, A. Kendall, and R. Cipolla, “Segnet: A deep convolutional encoder-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 27}, page_content='decoder architecture for image segmentation,”IEEE Trans. Pattern Anal. Mach.Intell.,\\nvol. 39, no. 12, pp. 2481–2495, 2017.\\n48. L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroﬀ, and H. Adam, “Encoder-decoder with\\natrous separable convolution for semantic image segmentation,” inECCV, 2018, pp. 801–\\n818.\\n49. M. Yang, K. Yu, C. Zhang, Z. Li, and K. Yang, “Denseaspp for semantic segmentation in\\nstreet scenes,” inProceedings of the IEEE Conference on Computer Vision and Pattern'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 27}, page_content='Recognition, 2018, pp. 3684–3692.\\n50. Z. Tian, T. He, C. Shen, and Y. Yan, “Decoders matter for semantic segmentation: Data-\\ndependent decoding enables ﬂexible feature aggregation,” inProceedings of the IEEE Con-\\nference on Computer Vision and Pattern Recognition, 2019, pp. 3126–3135.\\n51. R. Fan, H. Wang, M. J. Bocus, and M. Liu, “We learn better road pothole detection: from\\nattention aggregation to adversarial domain adaptation,” 2020.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 27}, page_content='52. C. Hazirbas, L. Ma, C. Domokos, and D. Cremers, “Fusenet: Incorporating depth into se-\\nmantic segmentation via fusion-based cnn architecture,” inAsian conference on computer\\nvision. Springer, 2016, pp. 213–228.\\n53. R. Fan, H. Wang, B. Xue, H. Huang, Y. Wang, M. Liu, and I. Pitas, “Three-ﬁlters-to-\\nnormal: An accurate and ultrafast surface normal estimator,” 2020.\\n54. R. Fan, M. J. Bocus, and N. Dahnoun, “A novel disparity transformation algorithm for'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 27}, page_content='road segmentation,”Information Processing Letters, vol. 140, pp. 18–24, 2018.\\n55. R. Fan, U. Ozgunalp, B. Hosking, M. Liu, and I. Pitas, “Pothole detection based on dispar-\\nity transformation and road surface modeling,”IEEE Transactions on Image Processing,\\nvol. 29, pp. 897–908, 2019.\\n56. R. Fan and M. Liu, “Road damage detection based on unsupervised disparity map seg-\\nmentation,”IEEE Transactions on Intelligent Transportation Systems, 2019.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 28}, page_content='Computer Stereo Vision for Autonomous Driving 29\\n57. Q. Ha, K. Watanabe, T. Karasawa, Y. Ushiku, and T. Harada, “Mfnet: Towards real-\\ntime semantic segmentation for autonomous vehicles with multi-spectral scenes,” in2017\\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE,\\n2017, pp. 5108–5115.\\n58. R. Fan, V. Prokhorov, and N. Dahnoun, “Faster-than-real-time linear lane detection im-\\nplementation using soc dsp tms320c6678,” in2016 IEEE International Conference on'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 28}, page_content='Imaging Systems and Techniques (IST). IEEE, 2016, pp. 306–311.\\n59. U. Ozgunalp, R. Fan, X. Ai, and N. Dahnoun, “Multiple lane detection algorithm based on\\nnovel dense vanishing point estimation,”IEEE Transactions on Intelligent Transportation\\nSystems, vol. 18, no. 3, pp. 621–632, 2016.\\n60. R. Fan and N. Dahnoun, “Real-time stereo vision-based lane detection system,”Measure-\\nment Science and Technology, vol. 29, no. 7, p. 074005, 2018.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 28}, page_content='61. J. Jiao, R. Fan, H. Ma, and M. Liu, “Using dp towards a shortest path problem-related ap-\\nplication,” in2019 International Conference on Robotics and Automation (ICRA). IEEE,\\n2019, pp. 8669–8675.\\n62. E. Trucco and A. Verri,Introductory techniques for 3-D computer vision. Prentice Hall\\nEnglewood Cliﬀs, 1998, vol. 201.\\n63. R. Fan, J. Jiao, J. Pan, H. Huang, S. Shen, and M. Liu, “Real-time dense stereo embedded\\nin a uav for road inspection,” inComputer Vision and Pattern Recognition Workshops'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 28}, page_content='(CVPRW), 2019, pp. 535–543.\\n64. Z. Zhang, “A ﬂexible new technique for camera calibration,”IEEE Trans. Pattern Anal.\\nMach.Intell., vol. 22, no. 11, pp. 1330–1334, 2000.\\n65. H. C. Longuet-Higgins, “A computer algorithm for reconstructing a scene from two pro-\\njections,”Nature, vol. 293, no. 5828, pp. 133–135, 1981.\\n66. R. I. Hartley, “In defense of the eight-point algorithm,” IEEE Trans. Pattern Anal.\\nMach.Intell., vol. 19, no. 6, pp. 580–593, 1997.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 28}, page_content='Mach.Intell., vol. 19, no. 6, pp. 580–593, 1997.\\n67. R. Fan, X. Ai, and N. Dahnoun, “Road surface 3d reconstruction based on dense subpixel\\ndisparity map estimation,”IEEE Transactions on Image Processing, vol. 27, no. 6, pp.\\n3025–3035, 2018.\\n68. B. Tippetts, D. J. Lee, K. Lillywhite, and J. Archibald, “Review of stereo vision algorithms\\nandtheirsuitabilityforresource-limitedsystems,” Journal of Real-Time Image Processing,\\nvol. 11, no. 1, pp. 5–25, 2016.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 28}, page_content='vol. 11, no. 1, pp. 5–25, 2016.\\n69. W. Luo, A. G. Schwing, and R. Urtasun, “Eﬃcient deep learning for stereo matching,” in\\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016,\\npp. 5695–5703.\\n70. D. Scharstein and R. Szeliski, “High-accuracy stereo depth maps using structured light,” in\\n2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,\\n2003. Proceedings., vol. 1. IEEE, 2003, pp. I–I.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 28}, page_content='2003. Proceedings., vol. 1. IEEE, 2003, pp. I–I.\\n71. M. G. Mozerov and J. van de Weijer, “Accurate stereo matching by two-step energy min-\\nimization,”IEEE Transactions on Image Processing, vol. 24, no. 3, pp. 1153–1163, 2015.\\n72. M. F. Tappen and W. T. Freeman, “Comparison of graph cuts with belief propagation for\\nstereo, using identical mrf parameters,” innull. IEEE, 2003, p. 900.\\n73. H. Hirschmuller, “Stereo processing by semiglobal matching and mutual information,”'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 28}, page_content='IEEE Trans. Pattern Anal. Mach.Intell., vol. 30, no. 2, pp. 328–341, 2007.\\n74. J. Žbontar and Y. LeCun, “Stereo matching by training a convolutional neural network\\nto compare image patches,”The journal of machine learning research, vol. 17, no. 1, pp.\\n2287–2318, 2016.\\n75. H. Hirschmuller and D. Scharstein, “Evaluation of stereo matching costs on images with\\nradiometric diﬀerences,”IEEE Trans. Pattern Anal. Mach.Intell., vol. 31, no. 9, pp. 1582–\\n1599, 2008.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 28}, page_content='1599, 2008.\\n76. Q. Yang, L. Wang, R. Yang, H. Stewénius, and D. Nistér, “Stereo matching with color-\\nweighted correlation, hierarchical belief propagation, and occlusion handling,” IEEE\\nTrans. Pattern Anal. Mach.Intell., vol. 31, no. 3, pp. 492–504, 2008.\\n77. R. Fan, Y. Liu, M. J. Bocus, L. Wang, and M. Liu, “Real-time subpixel fast bilateral\\nstereo,” in2018 IEEE International Conference on Information and Automation (ICIA).\\nIEEE, 2018, pp. 1058–1065.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 29}, page_content='30 Rui Fan, Li Wang, Mohammud Junaid Bocus, Ioannis Pitas\\n78. R. Fan, Y. Liu, X. Yang, M. J. Bocus, N. Dahnoun, and S. Tancock, “Real-time stereo\\nvision for road surface 3-d reconstruction,” in2018 IEEE International Conference on\\nImaging Systems and Techniques (IST). IEEE, 2018, pp. 1–6.\\n79. Y. Boykov, O. Veksler, and R. Zabih, “Fast approximate energy minimization via graph\\ncuts,”IEEE Trans. Pattern Anal. Mach.Intell., vol. 23, no. 11, pp. 1222–1239, 2001.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 29}, page_content='80. A. T. Ihler, A. S. Willskyet al., “Loopy belief propagation: Convergence and eﬀects of\\nmessage errors,”Journal of Machine Learning Research, vol. 6, no. May, pp. 905–936,\\n2005.\\n81. A. Blake, P. Kohli, and C. Rother,Markov random ﬁelds for vision and image processing.\\nMit Press, 2011.\\n82. D. Scharstein and R. Szeliski, “A taxonomy and evaluation of dense two-frame stereo\\ncorrespondence algorithms,”International journal of computer vision, vol. 47, no. 1-3, pp.\\n7–42, 2002.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 29}, page_content='7–42, 2002.\\n83. J. Žbontar and Y. LeCun, “Computing the stereo matching cost with a convolutional\\nneural network,” inProceedings of the IEEE conference on computer vision and pattern\\nrecognition, 2015, pp. 1592–1599.\\n84. F. Zhang, V. Prisacariu, R. Yang, and P. H. Torr, “Ga-net: Guided aggregation net for\\nend-to-end stereo matching,” inProceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, 2019, pp. 185–194.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 29}, page_content='and Pattern Recognition, 2019, pp. 185–194.\\n85. A. Seki and M. Pollefeys, “Sgm-nets: Semi-global matching with neural networks,” in\\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017,\\npp. 231–240.\\n86. N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox, “A large\\ndataset to train convolutional networks for disparity, optical ﬂow, and scene ﬂow estima-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 29}, page_content='tion,” inProceedings of the IEEE conference on computer vision and pattern recognition,\\n2016, pp. 4040–4048.\\n87. J. Pang, W. Sun, J. S. Ren, C. Yang, and Q. Yan, “Cascade residual learning: A two-stage\\nconvolutional neural network for stereo matching,” inProceedings of the IEEE Interna-\\ntional Conference on Computer Vision Workshops, 2017, pp. 887–895.\\n88. A. Kendall, H. Martirosyan, S. Dasgupta, P. Henry, R. Kennedy, A. Bachrach, and A. Bry,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 29}, page_content='“End-to-end learning of geometry and context for deep stereo regression,” inProceedings\\nof the IEEE International Conference on Computer Vision, 2017, pp. 66–75.\\n89. A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving? the kitti\\nvision benchmark suite,” in 2012 IEEE Conference on Computer Vision and Pattern\\nRecognition. IEEE, 2012, pp. 3354–3361.\\n90. M. Menze, C. Heipke, and A. Geiger, “Joint 3d estimation of vehicles and scene ﬂow.”IS-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 29}, page_content='PRS Annals of Photogrammetry, Remote Sensing & Spatial Information Sciences, vol. 2,\\n2015.\\n91. C. Menze, Moritz; Heipke and A. Geiger, “Object scene ﬂow,”ISPRS Journal of Pho-\\ntogrammetry and Remote Sensing, vol. 140, pp. 60–76, 2018.\\n92. J.-R. Chang and Y.-S. Chen, “Pyramid stereo matching network,” inProceedings of the\\nIEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 5410–5418.\\n93. J. L. Barron, D. J. Fleet, and S. S. Beauchemin, “Performance of optical ﬂow techniques,”'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 29}, page_content='International journal of computer vision, vol. 12, no. 1, pp. 43–77, 1994.\\n94. S. Mittal and J. S. Vetter, “A survey of cpu-gpu heterogeneous computing techniques,”\\nACM Computing Surveys (CSUR), vol. 47, no. 4, pp. 1–35, 2015.\\n95. H. Jin, D. Jespersen, P. Mehrotra, R. Biswas, L. Huang, and B. Chapman, “High perfor-\\nmance computing using mpi and openmp on multi-core parallel systems,”Parallel Com-\\nputing, vol. 37, no. 9, pp. 562–575, 2011.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 29}, page_content='puting, vol. 37, no. 9, pp. 562–575, 2011.\\n96. R. Fan, S. Duanmu, Y. Liu, Y. Zhu, J. Jiao, M. J. Bocus, Y. Yu, L. Wang, and M. Liu,\\n“Real-time binocular vision implementation on an soc tms320c6678 dsp,” inInternational\\nConference on Computer Vision Systems. Springer, 2019, pp. 13–23.\\n97. R. Fan and N. Dahnoun, “Real-time implementation of stereo vision based on optimised\\nnormalised cross-correlation and propagated search range on a gpu,” in2017 IEEE In-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 29}, page_content='ternational Conference on Imaging Systems and Techniques (IST). IEEE, 2017, pp.\\n1–6.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 30}, page_content='Page 1 of 10 \\n \\nComputer Vision and Abnormal Patient Gait Assessment a Comparison of \\nMachine Learning Models \\nJasmin Hundal MD University of Connecticut, Benson A. Babu MD MBA Saint Johns’ Episcopal Hospital, \\nPlainview Medical Center Northwell Health  \\nAbstract \\nAbnormal gait, its associated falls and complications have high patient morbidity, mortality.  Computer \\nvision detects, predicts patient gait abnormalities, assesses fall risk and serves as clinical decision support'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 30}, page_content='tool for physicians . This paper performs a systematic review o f how computer vision, machine learning \\nmodels perform an abnormal patient’s gait assessment. Computer vision is beneficial in gait analysis, it helps \\ncapture the patient posture. Several literature suggests the use of different machine learning algorithms such \\nas SVM, ANN, K -Star, Random Forest, KNN , among others  to perform the classification on the features \\nextracted to study patient gait abnormalities.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 30}, page_content='extracted to study patient gait abnormalities. \\nKeywords: Gait analysis, computer vision, machine learning  \\n1.0. Introduction  \\nGait abnormalities falls and associated complications have high morbidity and mortality.   Monitoring of \\npatient mobility may identify fall risk and potential treatment options.  Advanced computer vision algorithms \\nand more efficient low cost sensors preve nt patient falls and its complications. Such preventable'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 30}, page_content='complications include pulmonary emboli, myocardial infarction, hip fractures and medical deconditioning.  \\nThus, patient gait assessment matters in medicine, and several research works performed to analyze patient \\ngait patterns. [1]. Gait associated with a stroke, dementia, frail, elderly examined in neurology, physical \\nmedicine rehabilitation, rheumatology, and orthopaedics regularly [2]. Analysis of gait, its parameters’ \\nperformed routinely in clinical practice  [3].'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 30}, page_content=\"performed routinely in clinical practice  [3].  \\nGait, which is a part of humans' common behaviour, could show mental disorders like depression [4] \\ndementia [5], intellectual disability [6], and musculoskeletal disorders like deformation of the joint [7]. The \\nmost significant factor used in carrying out and assessing the patient's therap y and rehabilitation is the \\naccurate measurement of the amount of exercise performed during everyday life. As a result, there are several\"),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 30}, page_content=\"bodies of research on gait [8]. \\nThe systematic study of human walk, its abnormalities, theories for its etiology, recommended treatment, is \\ngait analysis. Clinical applications usually use gait analysis in identifying medical conditions or for \\nmonitoring the state of the patients' clinical recovery [9].  Clinicians observe gait patterns of a patient as he \\nor she walks are one's responsible for carrying out the medical treatment plan. Gait analysis is a subjective,\"),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 30}, page_content=\"determined by the clinicians' experience and j udgment. Because of the subjective nature of gait analysis, it \\nimpacts diagnosis and treatment decisions, thus patient outcomes [10]. \\nTherefore, this project performs a systematic review o f how computer vision used along with m achine \\nlearning models to perform effective abnormal patient gait assessment. The research questions identified for \\nthis research are:  \\n\\uf0b7 RQ 1: What are the machine learning models used for the gait analysis?\"),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 30}, page_content='\\uf0b7 RQ 2: How can computer vision assist in gait analysis? \\nWe organise this paper in multiple sections: Section 2 discuss the background study performed. Section 3 \\ndiscuss the method adopted to perform the literature review. Section 4 shows the findings and analysis from \\nthe systematic review performed. Fina lly, section 5 discusses the conclusion of the work , along with \\nsuggested future works.  \\n2.0. Background Study'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 30}, page_content='suggested future works.  \\n2.0. Background Study \\nThis section provides a background study on the gait analysis, computer vision and machine learning \\nconcepts.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 31}, page_content='Page 2 of 10 \\n \\n2.1. Gait Analysis  \\nThe definition for gait [11] : \\n“The anthropomorphic upstanding self-displacement, in the stepping of two feet which alternates and that \\nis with no extra fulcra, and that always contains support on a slightly inclined or horizontal surface.”  \\nThe scientific study of animal locomotion, especially those of humans referred to as Gait Analysis  [12], and'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 31}, page_content='it primarily aims at determining the functions, categori sation and inconsistencies associated with gait, to \\nprovide better treatment for ambulatory patients. Gait analysis uses several approaches, including medical \\nimaging technique, acoustic tracking system, magnetic system, goniometric measurement system, \\nelectromyography, foot planter presser sensor , force shoes, force plate mechanism, inertial system, optical \\nsystem and utilities portable devices. \\n2.2. Computer Vision'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 31}, page_content='2.2. Computer Vision  \\nHuman vision refers to gazing at the world to understand it. Computer vision is similar; it, however, differs, \\nbecause it uses a machine, especially a camera, for getting information. We use the following features for \\nclassifying a computer vision’s entire applications [13]:  \\n\\uf0b7 Gauging: It relates to tolerance checking and dimensional characteristic measurement \\n\\uf0b7 Sorting: It relates to the recognition and identification of parts.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 31}, page_content='\\uf0b7 Inspection: It relates to detecting, identifying and classification of parts. \\n \\nWithin the past decade, they have conducted extensive  research on v ideo-based human motion capture . \\nVarious techniques in machine learning and computer vision are proposed for pose estimation and 3D human \\nmotion tracking [14]. A video-based technique used for carrying out joint k inematics assess while gait is \\nongoing developed by the work of Corazza et al. [15]. \\n2.3. Machine Learning'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 31}, page_content='2.3. Machine Learning  \\nAs deep learning approaches emerge and advance, DNN -based techniques are the standard in visions tasks \\nsuch as human motion tracking and pose estimation [16], human activity recognition [17] and fac e \\nrecognition [18]. Several hidden layers between the output and input layers, and the ones that can learn \\nsemantic and high-level features from the data to model complex non -linear relationships, make up DNNs.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 31}, page_content='The current techniques on DNN for 3D human pose estimation focus on a single view, and a complex setting \\n[14] [19]. Regarding clinical gait analysis, techniques in machine learning, which includes Logistic \\nRegression [20], Artificial Neural Networks (ANN)  [21], K-Star [22], Random Forest [23], K-nearest \\nneighbours (KNN) [24] and Support Vector Machines (SVM) [25] found in applications identify and classify \\nspecific gait patterns into the medical conditions [16].'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 31}, page_content='2.4. AI models for Gait Analysis \\nThere are several Artificial Intelligence (AI) models introduced in the literature to identify various \\nabnormalities in a particular patient by pose estimation and gender recognition. The following summarizes \\nthe AI models used for handling abnormalities:  \\n2.4.1. Post Estimation  \\nThe human body’s trunk and joints are parts, detected with human pose estimation  [26]. The human pose'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 31}, page_content='estimation can detect these parts using videos or images from a video or image detector. Key points used to \\ndescribe details of the human skeleton using human pose estimation  [27]. For instance, the key points of a \\nhuman skeleton’s coordinates generated using a pose estimation model, uses the human body’s photos as \\ninputs. The role of the human pose estimation is pivotal in the prediction of human behaviour and a'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 31}, page_content='description of the human posture. Human pose estimation is among computer vision’s basic algorithm and \\nis pivotal in several related domains, including gait recognition, cha racter tracking, action recognition and \\nbehaviour recognition [28].  \\nThe pictorial structures model [29], [30], [31] that expresses spatial relationships among the body parts as a \\nkinematic-priors-based tree -structured graphical model, which couples connected limbs, made up the'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 31}, page_content='classical approach to articulated pose estimation. These methods can make typical mistakes, including \\ncounting image evidence twice, which can take place because of the connections between variables that a'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 32}, page_content='Page 3 of 10 \\n \\ntree-structured model did not capture, even though they have been successful on images, in which the \\nperson’s entire limbs. They used the pictorial structure model in the work of [32]; however, its underlying \\ngraph representation is different. \\nHierarchical models [33] [34] signify how the parts relate at various sizes and scales in a hierarchical tree \\nstructure. Usually, the image structure easily detected, facilitates the location of harder-to-detect and smaller'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 32}, page_content='parts can be discriminative in the larger parts corresponding to the limbs, based on these models’ underlying \\nassumption. \\nInteractions which introduce loops so it can augment the tree structure with more edges capturing long-range, \\nocclusion, and symmetry correlations, incorporated in non-tree models [35] [36]. Typically, the approximate \\ninference required in these methods for learning, and test time. As a result, spatial relationships’ accurate'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 32}, page_content='modelling traded off with models which enable efficient inference, mostly with a straightforward parametric \\nform, to ensure fast inference. \\nContrariwise, sequential-prediction-framework-based approaches [37] use likely complicated correlations \\nbetween variables, for learning an implicit spatial model; they train an inference procedure directly, to \\nachieve that, as [38] [39] shows. \\nThe models that carry out articulated pose estimation [40] [41], using convolutional architectures have'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 32}, page_content='received much attention in recent times. The method that [42] used involves the use of of a standard \\nconvolutional architecture to carry out a direct regression of the Cartesian coordinates [43]. It regresses image \\nto confidence map in recent work and opts for graphical models that must use spatial probability priors’ \\nheuristic initialisation or energy functions designed by hand, for removing outliers on the regressed'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 32}, page_content='confidence maps. It uses a dedicated ne twork model in some of them for precision refinement [44], [45]. \\nThis paper shows that it is suitable to input the regressed confidence maps to additionally convolutional \\nnetworks that do not requir e using hand-designed priors, that has great receptive domains for learning and \\nattains high-level performance within the entire precision region. Further, it should not be carefully initialised'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 32}, page_content='and should have dedicated precision refinement. A network module with a large receptive field is used in the \\nwork of [46] for capturing implicit spatial models. [41] considered joint training’s advantages; hence, the \\nmodel we proposed can be trained globally because of convolutions differentiable attributes. \\nA deep network with the features of being able to use error feedback for training is seen in the work of [47].'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 32}, page_content='It also uses Cartesian representation, as seen in [48] that is incapable of preserving spatial improbability, and \\nthat reduces the high precision regime’s accuracy. \\nCarrying out the task of articulated pose e stimation using Convolutional Pose Machines (CPMs).  \\nConvolutional Pose Machines (CPM) inherit pose machine architecture’s benefits [37];integrating learning \\nand inference tightly, the learning of long-range dependencies between multi-part cues and image implicitly,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 32}, page_content='and a modular sequential design. It combines these with the benefits which convolutional architecture \\nprovides. CPMs also include advantages such as the capability of handling large training datasets efficiently, \\na differentiable architecture which makes joint training with backpropagation possible, and the ability to \\nlearn spatial and image context’s feature representations directly from data. Series of convolutional networks,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 32}, page_content='which constantly 2D maps for each part’s location, make up CPMs.  \\nCPMs [49] are robust, and they have a very high accuracy of detection on human pose estimations’ standard \\ndatasets, such as Leeds Sports Pose (LSP) data set [50], Human Pose data set [51], and Max-Planck-Institut \\nInformatik (MPII). The time required for training in CPMs is extensive, and its detection speed is low. This'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 32}, page_content='makes it challenging to be applied in real -time tasks. Based on human pose est imation’s standard datasets, \\nexcellent detection outputs are found in the Stacked Hourglass [52] of similar duration. The new modes using \\nthe enhanced Stacked Hourglass include the 2017 models such as Learning feature [53], Self-Adversarial \\nTraining [54] and Multi -context [55], and the 2018 excellent models’ further improved accuracy. Some'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 32}, page_content='metrics and contained in these models, and they elongate the time required for training, thereby making up \\na common limitation. To date, the model does not have a satisfactory accuracy. \\nThe ability to extract the low -level feature is enhanced, using the more convoluted network structures, and \\ndeeper network layers of the enhanced CPM model [56]; and afterwards, apply a system to fine-tune it. The'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 32}, page_content='enhanced CPM is proven to include an excellent image detection effect and high image classification'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 33}, page_content='Page 4 of 10 \\n \\naccuracy, and a good human pose estimation model for designing a new network and apply a system of fine-\\ntuning to increase the human pose estimation’s efficiency. \\n2.4.2. Gender Recognition  \\nUsing  Gait Energy Image (GEI), which is a combination of gait and a new spatiotemporal method for force \\nrepresentation, for marking human walking’s behaviour for individual recognition, proposed in the work of'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 33}, page_content='Han et al. The findings of the study shows the efficiency of the use of the combination of gait and GEI \\napproach for individual recognition, and the competitiveness of its performance [57]. [58] used the GEI \\napproach for studying individual recognition. They used various techniques and outlooks to present the GEI \\napproach as biased attributed in their survey. It is clear from the findings of their research that the system’s'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 33}, page_content='performance in real -time improved; hence, its application in real -word is possible. [59] used automated \\napproaches to combine psychological methods for improving accuracy quality, to classify human gait-based \\ngenders. According to the research, compared to other parts of the body, the major body parts for gender \\nrecognition process include the chest, back, hair and head. Even though the applicat ion process contains'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 33}, page_content='several impediments are because of the differences in how humans appear, and they include change of shoes \\nand clothes, or when they lift objects, the gait classification is possible in a controlled environment. \\nThe classification of hu man behaviour using 2 -Directional 2-dimensional principles component analysis \\n((2D)2PCA) and 2G (2D) 2PCA) Enhanced Gait Energy Image (EGEI) proposed in the work of [60]. The'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 33}, page_content='outcomes of the experiment revealed the simplicity o f the algorithm and its capacity for realising a higher \\nclassification accuracy within a short period. A system that can use gait classification based on the silhouette, \\nto recommend books to visitors according to their age or gender, and in real -time proposed in the work of \\n[61].The Super Vector Machine (SVM) approach with 77.5% accuracy rate, used in the classification'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 33}, page_content='process. [62] combined the denoised energy image (DEI) and GEI approach in the pre-processing process to \\npresent gender recognition’s initial design and outcomes from experiment from walking movements. The \\ntraining of the process, and the extraction of the feature, used the Support Vector Machine (SVM). It is clear \\nfrom the research’s findings that certain typical values will used in the proposed approach; it could be 100 \\npercent accurate.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 33}, page_content='percent accurate. \\nThe method of integrating information from the multi-view gait at the feature level proposed in the work of \\n[63], and it increases the effectiveness of the performance for the gender classification based on multi-view \\ngait. A study on the use of gait for human recognition was conducted in the work of  [64]. Gait image’s \\nfeatures that are founded on information theory sets referred to as image feature information gait are'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 33}, page_content='presented in this report. Gait information features, which are information set theory-based gait image features \\nare presented in this report. The concept of the information set was applied on the frames in a gait cycle, and \\ntwo elements referred to as Gait information image with Sigmoid feature (GIISF) extracted and Gait \\ninformation Image with Energy Feature (GII-EF) to derive the proposed Gait information Image (GII). The'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 33}, page_content='identification of the gait was made using Nearest Neighbour (NN) for the classification. The robust feature-\\nlevel fusion of directional vectors such as forward and backward diagonal, vertical, and horizontal vectors \\nused in the work of [65] to study gender recognition. The first construct for each image sequence was Gait \\nEnergy Image (GEI), followed by Gradient Gait Energy Image (GGEI), which is achieved using'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 33}, page_content='neighbourhood gradient computat ion. After that, differences in all the four directions were utilised as \\ndiscriminative gait features. Afterwards, SVM used in the classification process, while the largest multi-view \\nCASIA-B (Chinese Academy of Sciences) datasets were significantly used i n the testing process. The \\ninvestigators noted that the outcome could be potentially beneficial.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 33}, page_content='According to the literature review, the current most universal gait-based approaches to gender classification \\ninclude GEI and GII approaches. As a result, this research focuses on contrasting GII approaches with GEI \\napproaches to present a gait-based gender classification in real-time. The one with the highest accuracy will \\nbe beneficial for future study. \\n3.0. Methods  \\n3.1. Search criteria'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 33}, page_content='3.0. Methods  \\n3.1. Search criteria \\nThe systematic review aimed at reviewing published papers, as well as academic journals in a step yby step \\nmanner. It also intends to perform a systematic peer -review on academic-based journals.  It will use online'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 34}, page_content=\"Page 5 of 10 \\n \\nsearch engines such as IEEExplore1, PubMed2, Google Scholar 3, Cochrane4, CINAHL, Medline5, Web of \\nscience6, DBLP 7, and EMBase 8 to search for literature . The primary keywords used for the search are \\nComputer vision, Artificial Intelligence, Machine learning, Deep learning, CNN, Abnormal gait analysis, \\ngait analysis, Stroke, Parkinson's Disease, and Movement disorders. \\n3.2. Justification of the selection\"),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 34}, page_content='3.2. Justification of the selection \\nThe preliminary research produced one hundred articles. We considered only ten of them in this report. Out \\nof the ten articles, only five of them related to this report’s topic. The report set duration of from between \\n2009 and 2019 for the works of literature, to ensure that only up-to-date works of literature used. However, \\nsometimes, some earlier journals used. \\n4.0. Findings and Analysis  \\nThe key findings from the journals are provided in table 1.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 34}, page_content='Table 1: Key findings \\nSystem suggested  Year Computer Vision \\ntechnology used \\nMachine \\nlearning \\ntechnique used \\nThe abnormality \\nidentified using the \\nGait analysis \\nReference \\nAutomatic Health \\nProblem Detection \\n2018 Videos captured \\nusing digital cameras \\nDNN Parkinson’s disease \\nPose Stroke \\northopaedic problems \\n[16] \\nA vision -based proposal \\nfor classification of \\nnormal and abnormal gait \\n2016 RGB Camera KNN and SVM Dementia \\nfrailty  \\n \\n[2] \\nComputer Vision -Based \\nGait Analysis'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 34}, page_content='[2] \\nComputer Vision -Based \\nGait Analysis  \\n2018 Smart Phone KNN Senility \\nFrailty \\n[2] \\nExtracting Body \\nLandmarks from Videos \\n2019 Videos Suggested \\nfuture work for \\nclassification or \\nregression \\nalgorithms \\nParkinson disease [66] \\nSystem to support the \\nDiscrimination of Neuro -\\ndegenerative Diseases \\n2009 Videos SVM, Random \\nForest and KStar \\nAmyotrophic lateral \\nsclerosis, Parkinson’s \\ndisease and \\nHuntington’s disease \\n[67]'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 34}, page_content='disease and \\nHuntington’s disease \\n[67] \\n \\nSeveral measures identified in the gait analysis to study the abnormality of the patients , some of which we \\nprovide in table 2.  \\nTable 2: The measures identified in the gait analysis to study the abnormality of the patients [68] \\nPatient Abnormality Gait measures \\nSlow walking Gait speed \\nFrequency of steps \\nMuscle weakness Muscle force  \\nCrouch Gait Ankle joint angle \\nUnstable gait Gait stability measure \\nDouble support time'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 34}, page_content='Double support time \\nHigh stepped gait Step height \\nPelvis drop Hip flexion \\n5.0. Conclusion  \\nFrom this study, it is shows several machine learning algorithms  suggested in the literature perform the \\nclassification, include SVM, K-Star, Random Forest, KNN and DNN. The images and videos widely used \\n                                                           \\n1 https://ieeexplore.ieee.org/Xplore/home.jsp \\n2 https://www.ncbi.nlm.nih.gov/pubmed/ \\n3 https://scholar.google.com/'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 34}, page_content='3 https://scholar.google.com/ \\n4 https://www.cochranelibrary.com/ \\n5 https://www.ebsco.com/products/research-databases/medline \\n6 https://clarivate.com/webofsciencegroup/solutions/web-of-science/ \\n7 https://dblp.uni-trier.de/ \\n8 https://www.embase.com/login'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 35}, page_content='Page 6 of 10 \\n \\nin the literature to capture the human walk while performing the gait analysis. Therefore, the use of high \\ntechnologies of computer vision , such as smartphone cameras, surveillance cameras , among others, are \\nincreasing drastically. This research’s limitation includes its failure to perform in-depth research on the gait \\nanalysis and its functions. The comparison of the approach used to perform the gait analysis in the literature'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 35}, page_content='performed to know the in-depth information regarding the gait analysis and its usage.  \\nReferences \\n \\n[1]  P. Chinmilli, S. Redkar, W. Zhang and T. Sugar, “A review on wearable inertial tracking based \\nhuman gait analysis and control strategies of lower-limb exoskeletons.,” Int. Robot. Autom. J , vol. 3, \\nno. 7, p. 00080, 2017.  \\n[2]  M. Nieto-Hidalgo, F. J. Ferrández-Pastor, R. J. Valdivieso-Sarabia, J. Mora-Pascual and J. M.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 35}, page_content='García-Chamizo, “A vision based proposal for classification of normal and abnormal gait using RGB \\ncamera.,” Journal of biomedical informatics, vol. 63, pp. 82-89, 2016.  \\n[3]  F. I. Mahoney, “Functional evaluation: the Barthel index.,” Md. State Med. J, vol. 14, pp. 61-65, \\n1965.  \\n[4]  T. C. Brandler, C. Wang, M. Oh-Park, R. Holtzer and J. Verghese, “Depressive symptoms and gait \\ndysfunction in the elderly.,” The American Journal of Geriatric Psychiatry, vol. 20, no. 5, pp. 425-\\n432, 2012.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 35}, page_content=\"432, 2012.  \\n[5]  J. Verghese, R. B. Lipton, C. B. Hall, G. Kuslansky, M. J. Katz and H. Buschke, “Abnormality of \\ngait as a predictor of non-Alzheimer's dementia.,” New England Journal of Medicine, vol. 347, no. \\n22, pp. 1761-1768, 2002.  \\n[6]  C. A. Haynes and T. E. Lockhart, “Evaluation of gait and slip parameters for adults with intellectual \\ndisability,” Journal of biomechanics, vol. 45, no. 14, pp. 2337-2341, 2012.\"),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 35}, page_content='[7]  W. Pirker and R. Katzenschlager, “Gait disorders in adults and the elderly.,” Wiener Klinische \\nWochenschrift , vol. 129, no. 3-4, pp. 81-95, 2017.  \\n[8]  S.-S. Lee, S. T. Choi and S.-I. Choi, “Classification of gait type based on deep learning using various \\nsensors with smart insole.,” Sensors , vol. 19, no. 8, p. 1757, 2019.  \\n[9]  R. Mehrizi, P. Xi , S. Zhang, R. Liao and K. Li, “Automatic Health Problem Detection from Gait'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 35}, page_content='Videos Using Deep Neural Networks,” arXiv preprint arXiv, vol. 1906.01480 , 2019.  \\n[10]  A. Muro-De-La-Herran, B. Garcia-Zapirain and A. Mendez-Zorrilla, “Gait analysis methods: An \\noverview of wearable and non-wearable systems, highlighting clinical applications.,” Sensors , vol. \\n14, no. 2, pp. 3362-3394, 2014.  \\n[11]  M. Nieto-Hidalgo, F. J. Ferrández-Pastor, R. J. Valdivieso-Sarabia, J. Mora-Pascual and J. M.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 35}, page_content='García-Chamizo, “Vision based extraction of dynamic gait features focused on feet movement using \\nRGB camera.,” In Ambient Intelligence for Health, pp. 155-166, 2015.  \\n[12]  M. Akhtaruzzaman, A. S. Akramin and M. R. Khan, “Gait analysis: Systems, technologies, and \\nimportance.,” Journal of Mechanics in Medicine and Biology, vol. 16, no. 7, p. 1630003, 2016.  \\n[13]  N. Neethu and B. Anoop, “Role of Computer Vision in Automatic Inspection Systems,”'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 35}, page_content='International Journal of Computer Applications, vol. 123, no. 13, 2015.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 36}, page_content='Page 7 of 10 \\n \\n[14]  X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis and K. Daniilidis, “Sparseness meets deepness: 3D \\nhuman pose estimation from monocular video.,” In Proceedings of the IEEE conference on computer \\nvision and pattern recognition, pp. 4966-4975, 2016.  \\n[15]  S. Corazza, L. Muendermann, A. Chaudhari, T. Demattio, C. Cobelli and T. P. Andriacchi, “A \\nmarkerless motion capture system to study musculoskeletal biomechanics: visual hull and simulated'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 36}, page_content='annealing approach.,” Annals of biomedical engineering, vol. 34, no. 6, pp. 1019-1029, 2006.  \\n[16]  R. Mehrizi, X. Peng, Z. Tang, X. Xu, D. Metaxas and K. Li, “Toward marker-free 3D pose \\nestimation in lifting: A deep multi-view solution,” in In 2018 13th IEEE International Conference on \\nAutomatic Face & Gesture Recognition (FG 2018), 2018.  \\n[17]  J. Yang, M. N. Nguyen, P. P. San, X. L. Li and S. Krishnaswamy, “Deep convolutional neural'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 36}, page_content='networks on multichannel time series for human activity recognition.,” in In Twenty-Fourth \\nInternational Joint Conference on Artificial Intelligence, 2015.  \\n[18]  S. M. Iranmanesh, H. Kazemi, S. Soleymani, A. Dabouei and N. M. Nasrabadi, “Deep sketch-photo \\nface recognition assisted by facial attributes.,” In 2018 IEEE 9th International Conference on \\nBiometrics Theory Applications and Systems (BTAS), pp. 1-10, 2018.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 36}, page_content='[19]  G. Pavlakos, X. Zhou, K. G. Derpanis and K. Daniilidis, “Coarse-to-fine volumetric prediction for \\nsingle-image 3D human pose.,” in In Proceedings of the IEEE Conference on Computer Vision and \\nPattern Recognition, 2017.  \\n[20]  W. Jiang, J. Josse, M. Lavielle and TraumaBase Group, “Logistic regression with missing \\ncovariates—Parameter estimation, model selection and prediction within a joint-modeling \\nframework.,” Computational Statistics & Data Analysis , vol. 106907, 2019.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 36}, page_content='[21]  A. Khorasani and M. R. S. Yazdi, “Development of a dynamic surface roughness monitoring system \\nbased on artificial neural networks (ANN) in milling operation.,” The International Journal of \\nAdvanced Manufacturing Technology, vol. 93, no. 1-4 , pp. 141-151, 2017.  \\n[22]  B. Kapur, N. Ahluwalia and R. Sathyaraj, “Comparative study on marks prediction using data mining \\nand classification algorithms.,” International Journal of Advanced Research in Computer Science , \\nvol. 8, no. 3, 2017.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 36}, page_content='vol. 8, no. 3, 2017.  \\n[23]  P. Thanh Noi and M. Kappas, “Comparison of random forest, k-nearest neighbor, and support vector \\nmachine classifiers for land cover classification using Sentinel-2 imagery.,” Sensors , vol. 18, no. 1, \\np. 18, 2018.  \\n[24]  L.-Y. Hu, M.-W. Huang, S.-W. Ke and C.-F. Tsai, “The distance function effect on k-nearest \\nneighbor classification for medical datasets.,” SpringerPlus , vol. 5, no. 1, pp. 1-9, 2016.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 36}, page_content='[25]  G. Tezel and M. Buyukyildiz, “Monthly evaporation forecasting using artificial neural networks and \\nsupport vector machines.,” Theoretical and applied climatology, vol. 124, no. 2, pp. 69-80, 2016.  \\n[26]  B. Qiang, S. Zhang, Y. Zhan, W. Xie and T. Zhao, “Improved Convolutional Pose Machines for \\nHuman Pose Estimation Using Image Sensor Data.,” Sensors , vol. 19, no. 3, p. 718, 2019.  \\n[27]  D. Mehta, S. Sridhar, O. Sotnychenko, H. Rhodin, M. Shafiei, H.-P. Seidel, W. Xu, D. Casas and C.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 36}, page_content='Theobalt, “Vnect: Real-time 3d human pose estimation with a single rgb camera,” ACM Transactions \\non Graphics (TOG) , vol. 36, no. 4, pp. 1-14, 2017.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 37}, page_content='Page 8 of 10 \\n \\n[28]  L. Wang, J. Zang, Q. Zhang, Z. Niu, G. Hua and N. Zheng, “Action recognition by an attention-\\naware temporal weighted convolutional neural network.,” Sensors, vol. 18, no. 7, p. 1979, 2018.  \\n[29]  M. Andriluka, S. Roth and B. Schiele, “Pictorial structures revisited: People detection and articulated \\npose estimation,” in IEEE, 2009.  \\n[30]  G. Othmezouri, I. Sakata, B. Schiele, M. Andriluka and S. Roth, “Monocular 3D pose estimation and'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 37}, page_content='tracking by detection.”. Patent 8,958,600, 17 February 2015. \\n[31]  L. Pishchulin, M. Andriluka, P. Gehler and B. Schiele, “Poselet conditioned pictorial structures,” in \\nPishchulin, Leonid; Andriluka, Mykhaylo ; Gehler, Peter ; Schiele, Bernt ;, 2013.  \\n[32]  M. Kiefel and P. V. Gehler, “Human pose estimation with fields of parts.,” in In European \\nConference on Computer Vision, 2014.  \\n[33]  Y. Tian, C. L. Zitnick and S. G. Narasimhan, “Exploring the spatial hierarchy of mixture models for'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 37}, page_content='human pose estimation,” in Springer, Berlin, Heidelberg, 2012.  \\n[34]  M. Sun and S. Savarese, “Articulated part-based model for joint object detection and pose \\nestimation,” in In 2011 International Conference on Computer Vision, 2011.  \\n[35]  M. Dantone, J. Gall, C. Leistner and L. V. Gool, “Human pose estimation using body parts dependent \\njoint regressors,” in In Proceedings of the IEEE Conference on Computer Vision and Pattern \\nRecognition, 2013.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 37}, page_content='Recognition, 2013.  \\n[36]  L. Karlinsky and S. Ullman, “Using linking features in learning non-parametric part models.,” in In \\nEuropean Conference on Computer Vision, Berlin, Heidelberg, 2012.  \\n[37]  V. Ramakrishna, D. Munoz, M. Hebert, J. A. Bagnell and Y. Sheikh, “Pose machines: Articulated \\npose estimation via inference machines.,” in In European Conference on Computer Vision, 2014.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 37}, page_content='[38]  P. H. Pinheiro and R. Collobert, “Recurrent convolutional neural networks for scene labeling,” in In \\n31st International Conference on Machine Learning (ICML), 2014.  \\n[39]  S. Ross, D. Munoz, M. Hebert and J. A. Bagnell, “Learning message-passing inference machines for \\nstructured prediction,” in CVPR 2011, 2011.  \\n[40]  J. Carreira, P. Agrawal, K. Fragkiadaki and J. Malik, “Human pose estimation with iterative error'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 37}, page_content='feedback.,” in In Proceedings of the IEEE conference on computer vision and pattern recognition, \\n2016.  \\n[41]  J. J. Tompson, A. Jain, Y. LeCun and C. Bregler, “Joint training of a convolutional network and a \\ngraphical model for human pose estimation,” in In Advances in neural information processing \\nsystems, 2014.  \\n[42]  Z. Tu and X. Bai, “Auto-context and its application to high-level vision tasks and 3d brain image'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 37}, page_content='segmentation,” IEEE transactions on pattern analysis and machine intelligence, vol. 32, no. 10, pp. \\n1744-1757, 2009.  \\n[43]  A. Krizhevsky, I. Sutskever and G. E. Hinton, “Imagenet classification with deep convolutional \\nneural networks.,” in In Advances in neural information processing systems, 2012.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 38}, page_content='Page 9 of 10 \\n \\n[44]  L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler and B. Schiele, \\n“Deepcut: Joint subset partition and labeling for multi person pose estimation,” in In Proceedings of \\nthe IEEE conference on computer vision and pattern recognition, 2016.  \\n[45]  J. Tompson, R. Goroshin, A. Jain, Y. LeCun and C. Bregler, “Efficient object localization using \\nconvolutional networks.,” in In Proceedings of the IEEE Conference on Computer Vision and'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 38}, page_content='Pattern Recognition, 2015.  \\n[46]  T. Pfister, J. Charles and A. Zisserman, “Flowing convnets for human pose estimation in videos.,” in \\nIn Proceedings of the IEEE International Conference on Computer Vision, 2015.  \\n[47]  J. Carreira, P. Agrawal, K. Fragkiadaki and J. Malik, “Human pose estimation with iterative error \\nfeedback.,” in In Proceedings of the IEEE conference on computer vision and pattern recognition, \\n2016.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 38}, page_content='2016.  \\n[48]  A. Toshev and C. Szegedy, “Deeppose: Human pose estimation via deep neural networks,” in In \\nProceedings of the IEEE conference on computer vision and pattern recognition, 2014.  \\n[49]  S.-E. Wei, V. Ramakrishna, T. Kanade and Y. Sheikh, “Convolutional pose machines.,” in n \\nProceedings of the IEEE conference on Computer Vision and Pattern Recognition, 2016.  \\n[50]  I. Lifshitz, E. Fetaya and S. Ullman, “Human pose estimation using deep consensus voting.,” in'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 38}, page_content='European Conference on Computer Vision, 246-260.  \\n[51]  V. Belagiannis and A. Zisserman, “Recurrent human pose estimation.,” in In 2017 12th IEEE \\nInternational Conference on Automatic Face & Gesture Recognition (FG 2017), 2017.  \\n[52]  A. Newell, K. Yang and J. Deng, “Stacked hourglass networks for human pose estimation,” in In \\nEuropean conference on computer vision, 2016.  \\n[53]  W. Yang, S. Li, W. Ouyang, H. Li and X. Wang, “Learning feature pyramids for human pose'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 38}, page_content='estimation.,” in In proceedings of the IEEE international conference on computer vision, 2017.  \\n[54]  C.-J. Chou, J.-T. Chien and H.-T. Chen, “Self adversarial training for human pose estimation,” in In \\n2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference \\n(APSIPA ASC), 2018.  \\n[55]  X. Chu, W. Yang, W. Ouyang, C. Ma, A. L. Yuille and X. Wang, “Multi-context attention for human'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 38}, page_content='pose estimation.,” in In Proceedings of the IEEE Conference on Computer Vision and Pattern \\nRecognition, 2017.  \\n[56]  B. Qiang, S. Zhang, Y. Zhan, W. Xie and T. Zhao, “Improved Convolutional Pose Machines for \\nHuman Pose Estimation Using Image Sensor Data,” Sensors , vol. 19, no. 3, p. 718, 2019.  \\n[57]  J. Han and B. Bhanu, “Gait energy image representation: comparative performance evaluation on \\nUSF HumanID database.,” in In Proc. Joint Int’l Workshop VS-PETS, 2003.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 38}, page_content='[58]  P.-C. Chang, M.-C. Tien, J.-L. Wu and C.-S. Hu, “Real-time gender classification from human gait \\nfor arbitrary view angles,” in In 2009 11th IEEE International Symposium on Multimedia, 2009.  \\n[59]  S. Yu, T. Tan, K. Huang, K. Jia and X. Wu, “A study on gait-based gender classification,” IEEE \\nTransactions on image processing, vol. 18, no. 8, pp. 1905-1910, 2009.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 39}, page_content='Page 10 of 10 \\n \\n[60]  L. Chunli and W. Kejun, “A behavior classification based on enhanced gait energy image.,” In 2010 \\nInternational Conference on Networking and Digital Society, vol. 2, pp. 589-592, 2010.  \\n[61]  M. Mikawa, S. Izumi and K. Tanaka, “Book recommendation signage system using silhouette-based \\ngait classification,” In 2011 10th International Conference on Machine Learning and Applications \\nand Workshops, vol. 1, pp. 416-419, 2011.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 39}, page_content='and Workshops, vol. 1, pp. 416-419, 2011.  \\n[62]  L.-H. Juang, S.-A. Lin and M.-N. Wu, “Gender recognition studying by gait energy image \\nclassification.,” in In 2012 International Symposium on Computer, Consumer and Control, 2012.  \\n[63]  D. Zhang and Y. Wang, “Using multiple views for gait-based gender classification,” in In The 26th \\nChinese Control and Decision Conference (2014 CCDC), 2014.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 39}, page_content='[64]  P. Arora, M. Hanmandlu and S. Srivastava, “Gait based authentication using gait information image \\nfeatures,” Pattern Recognition Letters , vol. 68, pp. 336-342, 2015.  \\n[65]  V. M. Guru, V. Kamalesh and R. Dinesh, “Human gait recognition using four directional variations \\nof gradient gait energy image,” in In 2016 International Conference on Computing, Communication \\nand Automation (ICCCA), 2016.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 39}, page_content='and Automation (ICCCA), 2016.  \\n[66]  H. Fleyeh and J. Westin, “Extracting Body Landmarks from Videos for Parkinson Gait Analysis.,” in \\nIn 2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS), 2019.  \\n[67]  H. Zheng, M. Yang, H. Wang and S. McClean, “Machine learning and statistical approaches to \\nsupport the discrimination of neuro-degenerative diseases based on gait analysis.,” in In Intelligent \\npatient management, Berlin, Heidelberg, 2009.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 39}, page_content='patient management, Berlin, Heidelberg, 2009.  \\n[68]  S. Chen, J. Lach, B. Lo and G.-Z. Yang, “Toward pervasive gait analysis with wearable sensors: A \\nsystematic review.,” IEEE journal of biomedical and health informatics, vol. 20, no. 6, pp. 1521-\\n1537., 2016.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 40}, page_content='Vision Transformers in Medical Computer Vision - A\\nContemplative Retrospection\\nArshi Parvaiza, Muhammad Anwaar Khalida, Rukhsana Zafara, Huma Ameera,\\nMuhammad Alia and Muhammad Moazam Fraza,∗\\naSchool of Electrical Engineering and Computer Science,\\nNational University of Sciences and Technology (NUST), Islamabad, 44000, Pakistan\\nARTICLE INFO\\nKeywords:\\nVision Transformers\\nMedical Image Analytics\\nSelf Attention\\nMedical Computer Vision\\nDiagnostic Image Analysis\\nLiterature Survey\\nABSTRACT'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 40}, page_content='Literature Survey\\nABSTRACT\\nRecent escalation in the field of computer vision underpins a huddle of algorithms with the\\nmagnificentpotentialtounraveltheinformationcontainedwithinimages.Thesecomputervision\\nalgorithmsarebeingpracticedinmedicalimageanalysisandaretransfiguringtheperceptionand\\ninterpretationofImagingdata.Amongthesealgorithms,VisionTransformers(ViTs)areevolved\\nas one of the most contemporary and dominant architectures that are being used in the field of'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 40}, page_content='computervision.Theseareimmenselyutilizedbyaplentyofresearcherstoperformnewaswell\\nasformerexperiments.Here,inthisarticleweinvestigatetheintersectionofVisionTransformers\\nandMedicalimagesandprofferedanoverviewofvariousViTsbasedframeworksthatarebeing\\nusedbydifferentresearchersinordertodeciphertheobstaclesinMedicalComputerVision.We\\nsurveyed the application of Vision transformers in different areas of medical computer vision'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 40}, page_content='such as image-based disease classification, anatomical structure segmentation, registration,\\nregion-based lesion Detection, captioning, report generation, reconstruction using multiple\\nmedicalimagingmodalitiesthatgreatlyassistinmedicaldiagnosisandhencetreatmentprocess.\\nAlong with this, we also demystify several imaging modalities used in Medical Computer\\nVision. Moreover, to get more insight and deeper understanding, self-attention mechanism of'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 40}, page_content='transformers is also explained briefly. Conclusively, we also put some light on available data\\nsets, adopted methodology, their performance measures, challenges and their solutions in form\\nof discussion. We hope that this review article will open future directions for researchers in\\nmedical computer vision.\\n1. Introduction\\nAdvances in medical imaging modalities have made them indispensable in clinical practice. The analysis of these'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 40}, page_content='images by analysts is limited to human subjectivity, time constraints, and variation of interpretation, which leads to\\ndelusion[1,2].Medicalimagescontainanampleinformationthatisthekeyformedicaldiagnosisandhencetreatment.\\nThehealthcaredatacomprises90%ofimagingdata,soconsideredastheprimarysourceformedicalinterventionand\\nanalysis. Multiple medical imaging modalities such as Computed Tomography (CT), ultrasound, X-ray radiography,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 40}, page_content='MR Imaging (MRI), and pathology are commonly used for medical imaging diagnostics. Several challenging factors\\nassociated with medical imaging modalities such as expensive data acquisition [3], dense pixel resolution [4], lack of\\nstandardimageacquisitiontechniquesintermsoftoolandscanningsettings[5],modality-specificartefacts[6],hugely\\nimbalanced data in negative and positive classes [7], sparse and noisy annotated datasets [8] are major hindrance in'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 40}, page_content='translating AI based diagnosis into clinical practice.\\nSince its surge, deep learning has shown remarkable success in automatic image analyses of medical imaging\\nmodalities. The advancements in deep learning have been flourished and perfected with time, revolving primarily\\naround one algorithm called Convolutional Neural Networks (CNN). CNNs are potentially the most popular deep\\nlearning architecture for its distinguished capabilities to exploit the spatial and temporal relationship between the'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 40}, page_content='features of images, which need to be deciphered for extracting meaningful information hidden in images [9, 10, 11].\\nIt has achieved notable accomplishment in medical imaging applications [12, 13] such as, determining the presence\\nand then identifying the type of malignancy (Classification), locating the patient’s lesion (Detection), extracting the\\ndesiredobject(organ)fromamedicalimage(Segmentation),placingseparateimagesinacommonframeofreference\\nmoazam.fraz@seecs.edu.pk (M.M. Fraz)'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 40}, page_content='moazam.fraz@seecs.edu.pk (M.M. Fraz)\\nORCID(s):0000-0003-0495-463X (M.M. Fraz)\\n. : Page 1 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 41}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nfor comparing or integrating the information they contain (Registration), synthesizing images for balancing dataset\\n(Generative Modeling) [14].\\nDespite that CNNs are very good at feature extraction tasks, they fail to encode the relative position of different\\nfeatures. In a CNN, the deeper layers are limited to view at whatever the initial layers have passed to them. This way'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 41}, page_content='theylosetheglobalcontextofthefeatures.Increasingthenumberoffiltersimprovestherepresentationcapacitybutat\\nthe cost of computation [15]. Various architectural changes are suggested by researchers for an efficient solution over\\nthe course of time and leading to attention mechanisms [16]. Using attention mechanism, the regions of the image are\\ncaptured, to which the CNN should pay attention, and forwarded to the deeper layers. Researchers have demonstrated'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 41}, page_content='that replacing the convolutional layer with attention has improved performance [17, 18]. The breakthrough from\\nTransformer network [16] in Natural Language Processing (NLP) tasks has inspired researchers to leverage this\\narchitecture for various computer vision tasks. Dosovitskiy et al. [19] proposed an adaptation to the transformer,\\nknown as Vision Transformer (ViT) that can be applied directly to sequences of image patches for extracting fine-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 41}, page_content='grained features. In ViT, global attention is applied on 16x16 patches of the entire image, focusing on the global\\nsalient features of the image, which resolve the long-range dependency among image content. It gets the best out\\nof the attention mechanism to incorporate global context in the image features without compromising computational\\nefficiency.\\nThe potential of the vision transformers is further explored by many researchers for solving various problems.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 41}, page_content='However, in this survey we aim to highlight the contribution of vision transformers to circumvent the challenges in\\nautomatic diagnostic of diseases using medical imaging modalities and their applications in medical computer vision\\ntasks. Our intended audience for this review are research practitioners from medical and interdisciplinary fields of\\ncomputer vision. For their assistance, we have described commonly used terminologies and their description in table\\n1.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 41}, page_content='1.\\nThis review is organized into seven sections. Section 1 briefly discusses the role of deep learning, the emergence\\nof the transformers, and the replacement of CNNs by transformers in medical computer vision. Section 2 discuss the\\norganization and papers selection methodology and distribution of the Review. Section 3 discusses different medical\\nimaging modalities and their application in the diagnostic and treatment of various diseases. Section 4 discusses the'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 41}, page_content='emergence of vision transformers and lists all the publicly available datasets used by the reviewed paper in every\\nmodality and deep learning task. Section 5 discusses visual recognition tasks to established the domain knowledge\\nfor the audience of the interdisciplinary field. Section 6 gives the details about the reviewed techniques categorized\\naccording to each deep learning task such as classification, segmentation, detection clinical report generation, and'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 41}, page_content='Miscellaneous which also include image registration. Section 7 identifies research gaps in the review papers and\\ndiscusses the future directions for using transformers in medical computer vision.\\n1.1. Scope/Objective of the Review\\nThe aim of writing this review paper is to highlight and discuss the contribution of Vision Transformers in\\nmedical computer vision across different medical imaging modalities. For this purpose, we have searched out papers'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 41}, page_content='from different top Conferences and Journals, excluding pre-prints, within the time span of three years from 2019-\\n2022. The results achieved and the adopted methodology of each paper is reviewed comprehensively. The distinct\\ncategories that we reviewed belonging to the medical image analysis includes classification, detection, segmentation,\\nregistration,clinicalreportgeneration,imageenhancement,imagereconstructionsandimagesynthesis.Theliterature'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 41}, page_content='of these categories is further divide into different medical imaging modalities. Ultimately, in addition to accentuating\\ninteresting techniques in the literature, we also put some light on research gaps and future directions. We hope this\\nreview will bridge the gap between computer vision community and medical specialists to foster the future research\\nand development in medical computer vision This article is written keeping in mind the intended audience from the'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 41}, page_content='interdisciplinary fields, medical and AI. Publicly available datasets and downloadable links are listed in the table 3.\\n1.2. Comparison with other Reviews\\nAlthough there exists some reviews on transformers already which enfolds a significant amount of work, yet we\\nfeel that there is a lot of room for improvement. For example, no review is primarily focused on applications of vision\\ntransformersinmedicaldomain.Tobridgethisgap,wecomeupwiththissurveyinwhichourpointofconvergenceis'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 41}, page_content='to scrutinize the exertion of ViTs in medical computer vision. To initiate this process we collected a bunch of articles\\naddressingdifferenttransformersarchitectureandtheirutilizationonmultimodalmedicalimages.Weincludedalmost\\n80peerreviewedarticlesinoursurveyfromprestigiousplatformslikePubMed,Springer,IEEE,ScienceDirectwhich\\n. : Page 2 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 42}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nTable 1\\nList of accronyms and abbreviations used in paper\\nAcronyms Words Acronyms Words\\nKID Kernel & Inception Distance MFTE Multi-Branch Features Transformation\\nand Extraction\\nAHA Align Hierarchical Attention MA Microaneurysms\\nMSAM Multi modal Spatial Attention Module CLAM Clustering-Constrained-Attention\\nMultiple-Instance Learning\\nCAC Coronary Artery Calcium KFS Key Factor Sampling'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 42}, page_content='EMVT Efficient Multi Scale Fusion Trans-\\nformer MSTGANet Multi Scale Transformer Global Atten-\\ntion Network\\nNSCF NonLocal Sparse Net Fusion MCAT Multimodal Co-Attention Transformer\\nTETRIS Template Transformer Image Segmen-\\ntation FMNet Feature Mapping Sub-Network\\nRDLs Regionalized Dynamic Learners CRC Colorectal cancer\\nIDH Isocitrate Dehydrogenase MTTU Multi Task Transformer Unet\\nVITBIS Vision Transformer for Biomedical Im-\\nage Segmentation CIDEr Consensus-based Image Description\\nEvaluation'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 42}, page_content='Evaluation\\nDAFNet Disentangled Alligned and Fuse Net ASFT Adjacent Slices Feature Transformer\\nHYBRIDCTRM Hybrid Convolutional Transformer MTI Multi Text Indexer\\nVIF Visual Information Fidelity PCR Rpolymerase Chain Reaction\\nABVS Automated Breast Volume Scanner PRCC Papillary Renal Cell Carcinoma\\nFID Frechet Inception Distance GSM Genitourinary syndrome of menopause\\nCEDT Cross Encoder-Decoder Transformer GLVE Global-Local Visual Extractor'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 42}, page_content='makes our paper unique from other review articles. In addition to that we also explained different imaging modalities\\nused in medical computer comprehensively. Moreover, we have given a brief note on available medical data sets in\\ntabularform.Thedownloadablelinkstothesedatasetsarealsomentioninthesetables.Wealsodiscussedtheresultsof\\nstate-of-the-artapproachesinawellstructuredtabularforminwhichwedescribedtheperformancemetricesalongwith'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 42}, page_content='their results on the available datasets. In the end, we have also pointed out some challenges along with their insightful\\nfuture directions. For comparative analysis of our review with Khan et al. [20] and Kai et al. [21] we visualized the\\nmain points in Figure 1.\\n. : Page 3 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 43}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nFigure 1:Comparison with recent published reviews on Vision Transformers\\n2. Survey Methodology\\nIn this section we will discuss the study selection criteria based on which articles are chosen for the review, and\\ndistribution of the included articles according to venues (journals, conferences), medical imaging modalities, deep\\nlearning tasks (classification, segmentation, detection etc.) and impact factors.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 43}, page_content='2.1. Papers Selection\\nWe have demonstrated the details of the searched and included research papers in this review article through\\nPRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses). Figure 2 shows the summary of\\npapers selection. We searched our papers on PubMed, Springer, IEEE Xplore, Science direct, and finally on google\\nscholar. In the result of search queries, we have found 11060 papers. Among which 3060 were duplicate and were'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 43}, page_content='excluded from the study. We screened remaining 8000 and found 7,600 were not fulfilling the criteria of legitimacy\\nforthis surveyas someofthem wasonlyabout medicalapplication withouttransformersand someof themwasusing\\ntransformer word in the different context than vision transformer. We further screened the remaining 400 articles and\\nexcluded the preprints from our study. In the PRISMA we have shown the categorization of our included 80 papers'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 43}, page_content='according to their application in medical domain. We have also demonstrated the distribution of the papers according\\nto the medical imaging modalities.\\n2.2. Data Extraction Methods\\nWe searched different platforms such as PubMed, Springer, Science Direct, IEEE Xplore and google scholar for\\nextracting the research articles. We targeted top journals and conferences, in duration of last four years from 2019 to'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 43}, page_content='2022. For extracting relevant papers for our study, we used different key words and combine them with the logical\\noperators ‘AND’, ‘OR’ to get the better search results. The key words we used are:\\n. : Page 4 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 44}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nFigure 2:PRISMA - flow diagram of selected research articles for the review\\n• Computed tomography (CT) scans, Magnetic resonance images (MRI), Ultrasound, X-rays, Optical coher-\\nence tomography (OCT), Fundus images, Positron emission tomography-Computed tomography (PET-CT),\\nHistopathology, Histology, WSI, Whole Slide Images'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 44}, page_content='• Classification, Reconstruction, Segmentation, Registration, Detection, Report Generation, Enhancement\\n• Transformer, Vision Transformer\\nWeextractedthekeywordsfromallthearticlesonvisiontransformersincludedinourreviewandgeneratethetag\\ncloud,whichisshowninFigure3.ThetagcloudillustratesthetrendingtermsinViTapplicationsinmedicalcomputer\\nvision. As our central focus in this review is to recapitulate the application of vision transformers on medical imaging'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 44}, page_content='modalities, this word cloud mostly highlighting the applications (classification, segmentation, detection, denoising,\\ncaptioning), medical imaging modalities (X-rays, PET, OCT, Whole-slide, CT, Histopathological, Fundus), disease\\n(cancer,glaucoma,covid,diabetic,tumor,carcinoma),organs(retinal,chest,breast,pulmonary,brain)andotherdeep\\nlearning related terms like transformer, vision, attention, encoder, decoder, multi-model.\\n. : Page 5 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 45}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nFigure 3:A visual depiction of most frequently used keywords in the reviewed articles\\nTable 2\\nInclusion and exclusion criteria for papers selection\\nInclusion criteria Exclusion criteria\\nArticles that address the medical computer\\nvision tasks such as registration, segmentation,\\ndetection,classification, enactment, reconstruction\\nand report generation using medical imaging modalities\\nand vision transformer.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 45}, page_content='and vision transformer.\\nArticles which are not using medical imaging modalities\\nand vision transformer.\\nPapers with proper evaluation metrics and detailed sum-\\nmary of proposed architecture including training parame-\\nters.\\nArticles that are not peer-reviewed.\\nArticles that are based on vision transformers. Articles that are survey papers.\\nThe papers inclusion and exclusion criteria is given in table 2. Firstly, papers were selected on the basis of titles,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 45}, page_content='if it does not match the inclusion and exclusion criteria then we read the abstract, conclusion, and model diagram for\\nthe final selection.\\n2.3. Papers Distribution\\nIn this section we have shown the distribution of the published papers across journals, conferences, imaging\\nmodalities, impact factors, and medical computer vision tasks. The purpose of this section is to give the bird’s eye'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 45}, page_content='view of the published work, that how much literature is available in top journals and conferences, what is the impact\\nof the work, what imaging modalities are used and what is the progress of works across the years.\\nTheFigure4showsthedistributionofreviewedarticlesacrosstheyears.Itcanbeseeninthegraphthattheliterature\\nforvisiontransformershasgrownthroughouttheyearsfrom2019to2022astheapplicationsofvisiontransformersin'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 45}, page_content='medical imaging modalities started growing from 2019 onward, with a large number of publications in the year 2021,\\nin which more than 50 articles were published.\\n. : Page 6 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 46}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nFigure 4:A chronological distribution of vision transformers research publications in medical image analytic.\\nFigure 5 shows the categorization of research articles based on visual recognition tasks using vision transformers.\\nThe recapitulation of the reviewed article is categorized based on the tasks such as classification, segmentation,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 46}, page_content='detection, report generation, registration, and miscellaneous. Miscellaneous further contains different tasks such as\\nreconstruction,enhancementandvisualNeuralVisualContentgeneration.Thegraphshowsthatmostofthereviewed\\narticles applied vision transformers on classification task which is 31%, segmentation 25%, Miscellaneous 19%,\\nDetection 16%, Report generation 7%, and Registration 2%.\\nFigure 5:Distribution of reviewed articles based on visual recognition tasks.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 46}, page_content='As the review paper is focusing on the application of vision transformers in medical imaging modalities. Figure 6\\ndepictsthestatisticsofimagingmodalitiesusedinourreviewedarticles.Eighttypesofimagingmodalitiesareusedin\\nthis survey paper exploiting vision transformers include X-rays imaging modality which is 35%, CT Scans 26%, MRI\\nScans 13%, Histopathology Images 11%, OCT/Fundus Images 8%, PET 3%, Endoscopy 2%, and Microscopy 2%.\\n. : Page 7 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 47}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nFigure 6:Dispensation graph of medical imaging modalities among the articles that are reviewed.\\nFigure7showsthecountofvisiontransformerbasedmedicalimagingarticlestakenfromvarioustopjournalsand\\nconferences. Each bubble represents the number of a specific journal or a specific conference and number of articles\\ntaken from these journals and conferences.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 47}, page_content='taken from these journals and conferences.\\nFigure 7:Bubble graph representing number of articles chosen from top ranked journals and conferences.\\nFigure 8 shows the distribution of vision transformer based reviewed papers across various journals of various\\nimpact factors according to JCR year 2020. The bubble size shows the number of reviewed articles retrieved from\\neach journal. According to this figure the five papers are reviewed from IEEE Transaction on medical imaging with\\n. : Page 8 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 48}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nthe impact factor of 10.048. The highest impact factor journal included in this survey paper is Nature Biomedical\\nengineering which is 25.671.\\nFigure 8:Visual representation of selected research publications from top tier journals along their impact factor.\\n3. A Delineation of Medical Imaging Modalities\\nThissectionisaddedforassistingcomputervisionpractitionerstoestablishthebasicdomainknowledgeofmedical'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 48}, page_content='imaging modalities and their application in the diagnostic and treatment of various diseases. Medical images differ\\nfrom natural images as they have specialized acquisition techniques. Physical phenomena such as electromagnetic\\nradiation, sound, light, nuclear magnetic resonance, and radioactivity are used for generating medical images of\\nexternal or internal organs of human body. These imaging techniques can be applied as non-invasive methods to view'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 48}, page_content='inside the human body, without any surgical intervention. Because of their importance in medical diagnostic a lot\\nof advancement has taken place in image acquisition devices called image modalities. These image modalities play\\nan important role in patients follow-up, regarding the growth of the already diagnosed disease state or undergoing\\na treatment procedure as 90% of the health data comprises of images. These imaging modalities are very crucial in'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 48}, page_content='public health and preventive measures as they help in establishing the accurate diagnosis. These medical images can\\ncapturedifferentbodyregionssuchaseyes,chest,brain,heart,arms,andlegs.Therearedifferentmodalitiesofmedical\\nimages such as computed tomography (CT), ultrasound, X-ray radiography, MR imaging (MRI), Positron emission\\ntomography–computed tomography (PET-CT), pathology fundus images and Optical coherence tomography (OCT).'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 48}, page_content='The images acquired from these modalities are shown in Figure 9. Details about these image modalities are given in\\nthe subsequent section.\\n. : Page 9 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 49}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\n(a)\\n (b)\\n (c)\\n (d)\\n(e)\\n (f)\\n (g)\\n (h)\\nFigure 9: A catalogue of medical imaging modalities that vision transformers employed for assisted diagnosis.(a) Chest\\nX-rays that are widely used for COVID-19 or pneumonia detection.(b) Brain MRI scans that are being used for diagnosis of\\naneurysms and tumors.(c) Brain CT scans that are employed to locate injuries, tumors, or clots leading to stroke.(d) OCT'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 49}, page_content='images that are playing an important role in diagnosis of retinal diseases such as age-related macular degeneration (AMD)\\nand diabetic eye diseases.(e) Fundus images captured the rear of eye and are used for detection and grading of Hypertensive\\nRetinopathy (f) Liver Ultrasound(g) Whole Slide Images (WSIs) that are being widely used in computational pathology\\n(h) PET-CT scans that are responsible for detection and diagnosis of cancer, determining the spread or recurrence of\\ncancer or metastasis'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 49}, page_content='cancer or metastasis\\n3.1. X-ray Imaging\\nAccordingtoNationalInstituteofHealth(NIH),US[22],X-raysimagesarecapturednon-invasivelyusingradiation\\nthat is part of the electromagnetic spectrum. X-rays are mostly captured for diagnosing bone fracture [23], but chest\\nx-rays are also used for detecting pneumonia [24]. X-rays are also used by mammograms for breast cancer detection\\n[25]. Other most familiar uses of X-rays are for breast tumors [26], enlarged heart [27], blocked blood vessels [28],'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 49}, page_content='conditions affecting your lungs [29] , infections [30], osteoporosis [31], arthritis [32], tooth decay [33].\\n3.2. Computed Tomography (CT) Scans\\nNational Institute of Health (NIH), US [22] described computed tomography (CT) scan is a computerized x-ray\\nimagingtechniqueinwhichanarrowbeamofradiationisfocusedandthenquicklyrotatedaroundthebodytocapture\\nthe detailed internal images, called tomographic images, of the body’s slice non-invasively. CT Scan produces 2-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 49}, page_content='dimesionalaswell3-dimensionalimagesofsliceofthebody.Onceseveralimagesaretakentheseimagesaredigitally\\nstacked together to form three-dimensional images. CT Scans are used for identifying the various organs/slices of\\nthe body for example CT scan of the heart is used for detecting various types of heart disease or abnormalities [34].\\nCT Scans of the head, to locate injuries [35], tumors [36], clots leading to stroke, hemorrhage, and other conditions'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 49}, page_content='[37]. CT Scans of the lungs is used for detecting cancer [38], tumors excess fluid, pulmonary embolisms (blood clots)\\n[39],lung infections [40] and emphysema or pneumonia [41].\\n3.3. Optical Coherence Tomography (OCT) & Fundus Images\\nAccording to American Academy of Ophthalmology (AOA) [42], Optical coherence tomography (OCT) captures\\ninvasivecross-sectionimagesoftheretinausinglightwaves.OCTcanbeusedtoexaminetheretina’sdistinctivelayers'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 49}, page_content='whichhelpinmappingandmeasuringtheirthicknessandplayanimportantroleindiagnosisofretinaldiseasessuchas\\n. : Page 10 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 50}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nage-relatedmaculardegeneration(AMD)[43]anddiabeticeyedisease[44].OCTcanbe,further,helpfulindiagnosis\\nofglaucoma[45],macularpucker[46],macularedema[47],centralserousretinopathy[48],diabeticretinopathy[49],\\nmacular hole [50].\\nAnother type of images, discussed by [51], that can be helpful in the diagnosis of age-related macular degeneration'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 50}, page_content='(AMD) [52] are fundus images which capture the rear of the eye. It is 2D imaging modality and since glaucoma is a\\n“3D disease”, 3D image modality such as OCT is considered more efficient for diagnosis. Fundus images can also be\\nused for detection and grading of hypertensive retinopathy [53]\\n3.4. Magnetic Resonance Imaging (MRI)\\nMagneticResonanceImaging(MRI)modality,describedbytheNationalInstituteofHealth(NIH),US[22],capture'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 50}, page_content='3d anatomical images noninvasively. MRI scanning does not use any radiation which make it an ultimate choice of\\ncapturingwhenfrequentimagingisrequiredinthetreatmentprocessespeciallyinthebrain.MRIisparticularlysuitable\\nforcapturingthesofttissuesofthebody,butitismorecostlyascomparedtox-raysandCTscanning.MRIcanbeused\\ntocapturedifferentpartsofthebodyforexampleMRIareusedfordiagnosisofaneurysmsandtumors[54]aswellfor'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 50}, page_content='differentiating between white matter and grey, in brain. MRI can further be used for spinal cord [55] and nerves[56],\\nmuscles [57], and ligaments [58].\\nThere is a specialized MRI called functional Magnetic Resonance Imaging (fMRI), which is used for observing brain\\nstructure and locating the areas of the brain which are activated during cognitive tasks.[59]\\n3.5. Ultrasound\\nRadiologyinfo.org for patients [60] described ultrasound as an imaging modality that invasively create image of'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 50}, page_content='organs, tissues, and other structures inside the body invasively by using sound waves without using any radiation.\\nUltrasound can be used to internal organs within the body, noninvasively. For example, capturing the heart, eyes,\\nbrain, thyroid, blood vessels, breast, abdominal organs, skin, and muscles. Ultrasound images are captured in 2D, 3D,\\nbut it can also capture 4D images which is 3D in motion such as a heart beating [61] or blood flowing through blood\\nvessels [62].'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 50}, page_content='vessels [62].\\n3.6. Histopathology or Whole-Slide Imaging (WSI)\\nTheWhole-SlideImaging(WSI)referstocapturingthemicroscopictissuespecimensfromaglassslideofbiopsy\\norsurgicalspecimenwhichresultsinhigh-resolutiondigitizedimages.Theseimagesarecapturedthrough,firsttaking\\nsmallhigh-resolutionimagetilesorstripsandthenmontagingthemtocreateafullimageofahistologicalsection[63].\\nSpecimens on glass slides transformed into high-resolution digital files can be efficiently stored, accessed, analyzed,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 50}, page_content='and shared with scientists from across the web using slide management technologies. Moreover, WSI is changing the\\nworkflows of many laboratories. It is used in various disease diagnostics, prognostic and treatments such as survival\\nprediction [64],detection of tissue phenotypes [65], Automated grade classification [66, 67, 68], segmentation of\\nmicrovessels and nerves [69, 70], Multi-Organ Nuclei Segmentation [71].\\n3.7. Positron Emission Tomography – Computed Tomography (PET-CT) Scans'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 50}, page_content='According to Radiologyinfo.org [60], Positron Emission Tomography, also called PET imaging or a PET scan,\\nsmall amounts of radioactive material called radiotracers for capturing images. PET-CT scans can be used for cancer\\ndetection and diagnosis [72], determining spread of the cancer, determining the recurrence of cancer, metastasis [73],\\nevaluating brain abnormalities like tumor [36] and memory disorder [74], mapping normal human brain and heart\\nfunction.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 50}, page_content='function.\\n4. Deep Neural Networks - Enhancing Representation Learning from CNNs to Vision\\nTransformers\\nThe goal of this section is to bridge the gap between AI and healthcare analysts. It introduces the deep learning\\nconcepts, techniques, and architectures that is found in the papers surveyed for this review article.\\nThe progression of deep neural networks in computer vision has contributed to various fields of study, and'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 50}, page_content='it primarily revolves around convolutional neural networks (CNN). For instance, while assessing medical images,\\npractitioners can recognize if there is an anomaly. Similarly, this mechanism can be taught to a computer via CNNs\\nto diagnose a disease or an anomaly while taking images as input, hence, giving vision to a computer.The model of a\\n. : Page 11 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 51}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nbasic CNN is illustrated in Figure 10. It takes the image input as a matrix of pixel values, assigns weights to learn the\\nvariousdifferentiablefeatures[15].Itthenpassestheimagethroughmultiplelayersandusesmultiplefilterstocapture\\nthediscriminatedfeaturesfromtheimage.CNNsgenerallyconsistofthreekindsoflayers:convolutionlayers,pooling'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 51}, page_content='layers,andfull-connectedlayers[75].ConvolutionlayersareresponsibleforlearningfeaturesandcapturingtheSpatial\\nandTemporaldependenciesbetweenthefeaturesbyapplicationofrelevantfilters.Thepoolinglayerisresponsiblefor\\nreducingthesizeoffeaturemapstocapturemoresemanticinformationthanspatialinformation.Inconvolutionallayer\\nfiltersofsizeNxNwhereNisequalto1,3,5,7,oranyotheroddnumber.Thepoolinglayerusesawindowofsize2x2,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 51}, page_content='3x3, or any other desired size to take average or maximum value in that window. Before the fully connected layer, the\\noutput of the convolutional and pooling layer which is called feature map is flattened to make a fully connected layer\\nat a function such as softmax is applied to make a prediction and a loss function is used to calculate the error and the\\nis back propagated to update the values of learnable parameters.\\nFigure 10:A general framework of Convolutional Neural Networks (CNNs)'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 51}, page_content='Depending on the application for example image classification, fully connected layers are added at the end of\\nthe network. Stacking these layers on top of each other with a specific arrangement with the help of a differentiable\\nfunction is known as CNN architecture. In recent years several CNN architectures are developed with various such\\narrangements: AlexNet [76], VGGNet [77], GoogleNet [78], ResNet [79], ResNeXt [80], Squeeze and Excitation Net\\n[81], DenseNet [82], and EfficientNet [83].'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 51}, page_content='[81], DenseNet [82], and EfficientNet [83].\\nConvolutionalneuralnetworksareusedinvariousapplicationsinthecategoriesofimageclassification,detection,\\nand segmentation, etc. For example face detection [84], identification of emotions [85], Speech recognition, and\\nMachinetranslationusingCNNs[86],etc.ConsideringtheapplicationsofCNN,itcanbeinferredthattheycanenable\\ncommendableresults[87].Theyareknowntobeablackbox,asthetrainingisaccordingtothetaskanddomain.One'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 51}, page_content='majorlimitationistheunclarityofresultsi.e.thereasonforaparticularoutcome.Especiallyinthemedicaldomain,it\\nis imperative to know the cause of a specific outcome, otherwise, a wrong diagnosis can be a threat to human lives.\\nOnewaytotacklethisproblemhead-onistohavesuchamodelthatfocusesonrelevantpartsoftheimageandcan\\nbe visualized by the doctors. To elucidate this issue,Attention modelswere proposed [16, 69]. The attention model'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 51}, page_content='focuses on the parts that are relevant to the input sequence. Moreover, a model was proposed known as Transformers,\\nitusedtheconceptofAttentiontoenhancethetrainingspeed[19].Transformersconsistofmultipleblocksofidentical\\nencoders and decoders, which were composed of self-attention block and feed-forward networks. In addition, the\\ndecoder consists of an extra attention block, which focuses on the relevant part of the sequence. The embedded words'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 51}, page_content='oftheinputwerepassedtotheencodersequentiallyandwerepropagatedtoalltheencoders.Theoutcomesofthelast\\nencoderwerethenfedtothedecoders.Theperformanceofthetransfermodelswasstate-of-the-artinthetasksrelated\\nto natural language processing.\\nInspiredbythetransformermodel,Dosovitskiyetal[19],appliedittotheimagesanditcanbeusedtoreplaceCNNs\\n. This model was called Vision Transformers (ViT) and its structure is illustrated in Figure 11. ViT model introduced'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 51}, page_content='global attention, but not on the entire image, rather they divided the entire image into small image patches of 16x16.\\nThey introduced simple numbers 1, 2, up to n as positional embeddings for specifying the positions of the patches.\\n. : Page 12 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 52}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nA lookup table was introduced which contained a learnable vector against each number representing the position of\\nthe patch. These embeddings were passed to the network along with the patch. Each patch was unrolled to a linear\\nvector and projected linearly on embedding matrix. The final embedding along with positional embedding was then'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 52}, page_content='fed into the transformer encoder. Along with embedding for patches, an extra embedding with number 0 is also fed\\nintothenetworkanditsoutputwasobtained.Thus,Visiontransformershavethecapabilityofmodelingglobalcontext\\nwhich assists in more accurate results. Lastly, in this review, medical images are considered as the input for vision\\ntransformers.\\nFigure 11:Structural representation of Vision Transformers\\n4.1. Open Access Medical Imaging Datasets employed in ViT Applications'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 52}, page_content='In table 3, we have summarized and structured open-accessed datasets in tabular form. The table includes\\ninformation regarding the tasks i.e. classification, segmentation, detection, report generations, and miscellaneous.\\nFurthermore, the respective image modalities and their applications are also included. Next, we have also compiled\\nthedescriptionandlinkstothecorrespondingdatasets.Hence,itwillassistresearcherstoidentifytheseresourcesthat\\ncan be utilized against different applications.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 52}, page_content='can be utilized against different applications.\\n. : Page 13 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 53}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nTable 3: Publicly available datasets of medical imaging modalities used by researchers for assisted diagnosis\\nTasks Modality Applications Datasets Description Download\\nLinks\\nClassification CT Scans\\nPulmonary Nod-\\nule Characteriza-\\ntion\\nLUNA16 [88]\\nA commonly used dataset\\nfor lung nodule identifica-\\ntion and false positive re-\\nduction.\\nDownload\\nLIDC-IDRI [89]\\nOne of the largest publicly\\navailable lung cancer'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 53}, page_content='One of the largest publicly\\navailable lung cancer\\nscreening datasets,\\nmade up of diagnostic\\nand screening thoracic\\ncomputed tomography\\n(CT) images.\\nDownload\\nEmphysema\\nClassification\\nComputed\\nTomography\\nEmphysema\\nDatabase [90]\\nA freely available dataset\\nthat includes 115 high-\\nresolution CT (HRCT)\\nscans and 168 square\\npatchesthatweremanually\\nannotatedinasubsetofthe\\nslices.\\nDownload\\nCOVID-CT [91]\\nA freely accessible collec-\\ntion of CT-scan images ex-\\ntracted from a number of\\nscholarly articles.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 53}, page_content='tracted from a number of\\nscholarly articles.\\nDownload\\nCOVID-19\\nDetection Sars-CoV-2 [92]\\nA multi-class CT scan\\ndataset for identification of\\nSARS-CoV-2 infection.\\nDownload\\nCOVD19-CT-DB\\n[93]\\nCOVID19-CT-Database\\nconsists of chest CT scans\\nthat are annotated for the\\nexistence of COVID-19.\\nDownload\\nCOVID-CTset\\n[94]\\nOne of the largest open ac-\\ncess COVID-19 lung CT\\ndataset that is available on-\\nline.\\nDownload\\nX-rays Pneumonia Clas-\\nsification\\nChest X-ray Im-\\nages [95]\\nA Dataset of validated'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 53}, page_content='Chest X-ray Im-\\nages [95]\\nA Dataset of validated\\nOCT and Chest X-Ray\\nimages.\\nDownload\\n. : Page 14 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 54}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nTuberculosis\\nPrognosis and\\nDetection\\nMontgomery\\nCounty (MC)\\nCXR\\nSmall tuberculosis dataset\\nfrom USA. Download\\nTuberculosis\\nPrognosis and\\nDetection\\nShenzhen dataset Small tuberculosis dataset\\nfrom Shenzhen (China). Download\\nCOVID Chest X-\\nray dataset\\nAn open access dataset of\\nchest X-ray and CT im-\\nages of patients which are\\npositive or suspected of\\nCOVID-19 or other viral\\nand bacterial Pneumonias.\\nDownload'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 54}, page_content='and bacterial Pneumonias.\\nDownload\\nClassification\\nBIMCV COVID-\\n19+ [96]\\nBIMCV-COVID19+\\ndataset is a large dataset\\nwith chest X-ray images\\nCXR (CR, DX) and\\ncomputed tomography\\n(CT) imaging of COVID-\\n19patientsalongwiththeir\\nradiographic findings.\\nDownload\\nX-rays\\nInterpretable\\nCOVID-19\\nDetection\\n& Severity\\nQuantification\\nCOVID-19\\nPosterior-\\nAnterior Chest\\nRadiography\\nImages [97]\\nThis is a curated COVID-\\n19 Chest X-ray image\\ndataset that was created\\nby combining 15 publicly\\naccessible datasets.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 54}, page_content='by combining 15 publicly\\naccessible datasets.\\nDownload\\nExtensive\\nCOVID-19\\nX-Ray and CT\\nChest Images\\n[98]\\nIn this COVID-19 dataset,\\nboth Non-COVID and\\nCOVID cases are included\\nof both X-ray and CT\\nimages.\\nDownload\\nCOVIDx Dataset\\n[99]\\nA database of chest X-ray\\nimagesforCOVID-19pos-\\nitive cases along with Nor-\\nmal and Viral Pneumonia\\nimages.\\nDownload\\nMRI Scans\\nMulti-Modal\\nMedical Image\\nClassification\\nMRNet Dataset\\n[92]\\nThe MRNet dataset con-\\nsistsof1,370kneeMRIex-\\nams performed at Stanford'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 54}, page_content='sistsof1,370kneeMRIex-\\nams performed at Stanford\\nUniversity Medical Center.\\nDownload\\n. : Page 15 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 55}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nFundus Im-\\nages\\nRetinal Image\\nSynthesis\\nand Disease\\nPrediction\\nColorFundusIm-\\nages [100]\\nA database that has both\\nFFA image and color fun-\\ndus image in this DR grad-\\ning system.\\nDownload\\nHistopatho-\\nlogy Images\\nColorectal\\nHistopathology\\nImage\\nClassification\\nColorectal\\nCancer Histology\\nDataset [101]\\nThis data set represents a\\ncollection of textures in\\nhistological images of hu-\\nman colorectal cancer.\\nDownload\\nDetection'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 55}, page_content='man colorectal cancer.\\nDownload\\nDetection\\nX-rays\\nCOVID-19 Diag-\\nnosis COVIDx [99]\\nA database of chest X-ray\\nimagesforCOVID-19pos-\\nitive cases along with Nor-\\nmal and Viral Pneumonia\\nimages.\\nDownload\\nCOVIDGR-E\\n[102]\\nIt is built by adding 426\\npneumonia images from\\nthe ChestX-ray8 database\\nto the COVIDGR-1.0\\ndataset\\nDownload\\nFundus Im-\\nages\\nMicroaneurysms\\nDetection IDRiD [103] This dataset consists of 81\\nimages. Download\\nCT Scans\\nCOV19-CT-DB\\n[93]\\nCOVID19-CT-Database\\nconsists of chest CT scans'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 55}, page_content='COVID19-CT-Database\\nconsists of chest CT scans\\nthat are annotated for the\\nexistence of COVID-19.\\nDownload\\nCOVID-19 Diag-\\nnosis\\nCOVIDx-CT-2A\\n[104]\\nA benchmark dataset:\\nthe largest comprising a\\nmultinational cohort of\\n4,501 patients from at least\\n15 countries and contains\\nthree classes – COVID-19\\nPneumonia, non-COVID-\\n19 Pneumonia, and\\nnormal.\\nDownload\\nHistopathol-\\nogy Images Cancer Detection\\nThe Cancer\\nGenome Atlas\\n[105]\\nThe compendium includes\\nheat maps for 33 differ-\\nent tumor types and three'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 55}, page_content='ent tumor types and three\\nplatforms:geneexpression,\\nreverse-phase protein ar-\\nrays (RPPA), and miRNA\\nexpression.\\nDownload\\nSegmentation iSeg-2017 [106] this datasets contains brain\\nMRI’s of 39 subjects Download\\n. : Page 16 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 56}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nMRI Scans BrainTissueSeg-\\nmentation\\nBraTS-2020\\n[107]\\nIt contains 1756 MRI of\\n439 subjects Download\\nMRBrainS [108]\\nIt contains 20 Fully an-\\nnotated multi sequence 3T\\nMRI Brains\\nDownload\\nUKBB [109] Download\\nCardiac Segmen-\\ntation M&MS-2\\nIt contains 360 subjects\\nhaving 200 training and\\n160 testing images\\nDownload\\nMRI Scans ERI [110]\\nIt contains LGE Data of\\n28 patients. the number of\\nsegmented images are 358\\nDownload'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 56}, page_content='segmented images are 358\\nDownload\\nAbdominal\\nSegmentation CHAOS [111]\\nThis Data set contains im-\\nages for T2 Segmentation\\nof Liver and Kidneys\\nDownload\\nSegmentation\\nX-rays\\nTooth Root Seg-\\nmentation DRIVE [112]\\nIt include 40 color fundus\\nretinal images that are ran-\\ndomly selected\\nDownload\\nKnee Segmenta-\\ntion OAI\\n212Kneeimageswereseg-\\nmented randomly and the\\ntraining and testing split\\nwas 100 and 112\\nDownload\\nLung Segmenta-\\ntion JSRT [113]\\nThe database includes 154\\nconventional chest radio-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 56}, page_content='conventional chest radio-\\ngraphs with a lung nodule\\n(100 malignant and 54 be-\\nnign nodules)\\nDownload\\nCT Scans Pediatric\\nSegmentation KiTS19 [114]\\nThisDatasetcontainsrenal\\nTumor Segmentation Im-\\nages of 210 Adults\\nDownload\\nDrusen Segmen-\\ntation from Reti-\\nnal OCT Images\\nUSCD [115] This dataset contains 8616\\nretinal OCT B-scans Download\\n. : Page 17 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 57}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nIU Chest X-ray\\nCollection [116]\\nA public and open\\nsource OpenI or Indiana\\nUniversity Chest X-rays\\ndatabase which contains\\n3955 medical reports with\\n7470 frontal and lateral\\nchest x-ray images.\\nDownload\\nClinical\\nReport\\nGeneration\\nX-rays MIMIC-CXR\\n[117]\\nIt is the recently released\\nlargest dataset, consists of\\n377,110 chest X-rays im-\\nages and 227,835 reports\\nfrom 64,588 patients.\\nDownload\\nPEIR GROSS\\n[118]'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 57}, page_content='from 64,588 patients.\\nDownload\\nPEIR GROSS\\n[118]\\nIt consists of publicly ac-\\ncessible 7442 teaching im-\\nages, spread across 21 pre-\\ndefined subcategories. The\\nvocabulary size of the to-\\ntalimagecaptionsis4,452.\\nEach image on average\\ncontainsa12wordcaption.\\nDownload\\nMiscellaneous\\nNIH-AAPM-\\nMayo Clinic\\nLDCT Grand\\nChallenge [119]\\nA public and open source\\n30 contrast-enhanced ab-\\ndominal CT patient scans.\\nDownload\\nPET-CT\\nScans\\nMedical Image\\nEnhancement\\nKirby21 Dataset\\n(KKI01 to\\nKKI05) [120]'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 57}, page_content='Kirby21 Dataset\\n(KKI01 to\\nKKI05) [120]\\nA public and open source\\ndataset containing correla-\\ntion data for 20 subjects\\nfrom Kennedy Krieger\\nDownload\\nMRI Images Medical Image\\nReconstruction\\nDIVerse 2K reso-\\nlution high qual-\\nity (DIV2K) im-\\nagesdataset[121]\\nIt contains a total of 1000\\n2K resolution RGB im-\\nages.\\nDownload\\nfastMRI Scans\\n[122]\\nT1- and T2-weighted\\nimages from 150 subjects\\nwere analyzed (100 for\\ntraining, 10 for validation,\\n40 for testing).\\nDownload\\n. : Page 18 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 58}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nFluorescence\\nMicroscopy\\nDenoising\\nof Celullar\\nMicroscopy\\nImages for\\nAssisted\\nAugmented\\nMicroscopy\\nFlywing, Planaria\\nand Tribolium\\ndatasets [123]\\nThe training data 17,005\\nand 14,725 small cropped\\npatches of size 64×64×16\\nfor Planaria and Tribolium\\ndatasets, while the testing\\ndata are 20 testing images\\nofsize1024 ×1024×95and\\n6 testing images of average\\nsize around 700×700×45\\nforthetwodatasets,respec-\\ntively\\nDownload'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 58}, page_content='forthetwodatasets,respec-\\ntively\\nDownload\\n5. Visual Recognition Tasks in Medical Images\\nArtificial intelligence and deep learning have played a vital role in assisting clinicians for better diagnosis. In\\nthis context, the application of CNNs and ViTs on medical images assists the healthcare professional in disease\\nclassification, lesion detection, segmenting the anatomical structures, automated report generation, denoising the'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 58}, page_content='images,medicalimageregistration,andvariousothertasks.Thissectiongivesabriefoverviewoftheabove-mentioned\\nvisual recognition tasks performed by the application of CNNs and ViTs on medical images.\\n5.1. Medical Image Classification\\nThe practitioners give their diagnosis by analyzing the medical images, hence, determining the presence and type\\nof the disease. This conventional diagnosing way can be assisted with deep learning techniques. Figure 12 shows a'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 58}, page_content='generalized classification network. Through these techniques, the ambiguity in diagnosis among different doctors can\\nbemitigated andtheoutcomes willbemore accurate.Thus, resultsachievedthrough CNNsarenot onlytimeefficient\\nbut can also assist healthcare professionals. Using CNN models, an input image is fed into the network, which is then\\nassessed, analyzed, and interpreted to determine the target and object of different modes [124]. This process is known'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 58}, page_content='asimageclassification.Forexample,consideringthefigure5,thechestX-rayimagesaregivenasinput,andthemodel\\nclassifies them into normal and pneumonia images. At present, image classification has various applications in the\\nmedical domain such as; skin cancer [125], diabetic retinopathy [126], tuberculosis [127], etc.\\nThesignificanceofimageclassificationusingCNNscanbedeterminedbytheaforementionedapplications.These'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 58}, page_content='applications were achieved through various CNN architectures such as AlexNet [76], VGGNet [77], GoogleNet [78],\\nResNet[79].Later,moreresource-efficientarchitectureswereproposedi.e.MobileNet[128],SqueezeandExcitation\\nNet[81],andEfficientNet[83],etc.ThroughtheseConvolutionalneuralnetworks,commendableresultswereachieved\\nin medical applications, however, in terms of resources, improvements are still required.\\n. : Page 19 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 59}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nFigure 12:Medical image classification pipeline in which Chest X-rays (CXRs) are fed into a Deep CNN architecture, which\\nassigns each image a class, such as pneumonia patient or healthy patient.\\n5.2. Lesions Detection\\nClassifyingimageswasindeedastepforwardtowardsautomateddiagnosis.Nevertheless,locatingtheobjectplays\\nanimportantrolewhiledevelopingamorefunctioningapplication.Inthefieldofcomputervision,oneoftheunderlying'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 59}, page_content='goals is to classify the object into a category and determine the location of the object in a given image, this technique\\nisknownasobjectdetection.Figure13figurativelyexplainstheageneraldetectionnetwork.Forinstance,iftheinput\\nimage is an X-ray of a hand, the object detection model will not only classify the category i.e. fractured bone but also\\nlocalize it by using bounding boxes. Considering a situation, where bones were fractured on multiple locations, here,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 59}, page_content='object detection will be a better technique to opt for as it will be more helpful to the diagnostician. The applications\\nof object detection involve; face detection [84], plant disease identification [129],weapon detection [130] , emotion\\ndetection [85], etc.\\nFigure 13:A CNN architecture detecting a colony of tumorous cells given a histopathology image.\\nThe task of object detection is composed of two types; two-stage networks and one-stage networks [124]. The'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 59}, page_content='two-stage networks are based on region proposal algorithms such as R-CNN [131], Fast R-CNN [132] , and Faster-\\nRCNN [133]. The other technique is designed in a way that it works directly on images, examples include; YOLO\\n[134], SSD [135], etc. The two design types have a trade-off between accuracy and time efficiency. The two-stage\\nnetworks are capable of more accuracy, whilst, one-stage networks have more speed. Thus, it depends on the task at'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 59}, page_content='hand and dataset, while choosing these networks for object detection. The limited datasets in the medical domain are\\n. : Page 20 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 60}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nthe biggest constraint while training the models, therefore, researchers are working on models which can work with\\nlimited datasets.\\n5.3. Anatomical Structure Segmentation\\nIn the medical domain, there are numerous cases where it is difficult to distinguish between two different lesions\\nas there are minor differences. Since lesions can have different treatment strategies, determining them as separates is'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 60}, page_content='vital. It is indeed a challenge to recognize such subtle differences, but it is not impossible. If the images are classified\\npixel-wise, they can be localized as well, and result in a fine outline of the object. Such a technique is known as\\nImage segmentation. For example, if an MRI image is fed into the model, the outcome will not just classify the tumor\\ntype, its anatomical structure will also be highlighted as seen in Figure 14. There are various applications of image'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 60}, page_content='segmentation, which includes ; Cardiovascular structure [136] , prostate cancer [137], blood vessel [138] etc.\\nFigure 14: A segmentation framework in which brain MRI scans are fed into a deep CNN architecture which is not just\\nclassifying and locating the tumorous region but also highlighting the anatomical structure.\\nFine-grainedsegmentationisadecisivestepinimage-guidedtreatmentandcomputer-aideddiagnosis.Thewidely'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 60}, page_content='used architectures for image segmentation are; U-Net [139], DeepLab [140], Masked R-CNN [141] etc. The usage of\\nthesesegmentationmodelsdependsontheproblemthatisbeingsolved.Forinstance,multi-scaleobjectsintheimage,\\ndeeplab,anditsvariousstructureswillbeawisechoice.Lastly,theconcernregardingimagesegmentationisthelack\\nof labeled data due to which researchers are considering more unsupervised approaches, however, it is still a work in\\nprogress [142].\\n5.4. Clinical Report Generation'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 60}, page_content='progress [142].\\n5.4. Clinical Report Generation\\nIn the healthcare domain, while examining radiology images i.e. Chest X-rays, CT Scans, MRI, etc, the doctors\\nhave to write detailed reports of the assessment. This conventional method of report writing is tedious, monotonous,\\ntime-consuming,anderror-proneforradiologists[143].Immenseprogresshasbeenmadeindeeplearningtogenerate\\nmedical reports automatically. Automatic report generation can assist clinical practitioners in quick and accurate'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 60}, page_content='decision-making [144]. For example; if the CT scan of a brain is given as input, the output would be a complete\\nreport such as, if the tumour exists, the location of the tumour, its size and other details. Figure 15 shows a general\\nworkflow of clinical report generation. Medical report generation is an application of image caption in which these\\nmodels are applied to medical data. Image captioning refers to computers generating captions by giving images as'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 60}, page_content='input. It consists of mainly an encoder-decoder where CNN is used to extract features, and LSTM or RNN are used to\\ngenerate the captions [145]. The initial work on its architectures include Show and Tell [146], Show Attend and Tell\\n[147],NeuralTalk[148],etc.SomeoftheStateoftheArtincludeDenseCap[149],SemStyle[150],Dense-CaptionNet\\n[151], etc. Lastly, as in supervised learning, a large amount of annotated data is required for these models to perform'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 60}, page_content='well, in the future unsupervised learning could be a more powerful way to proceed with them [152].\\n. : Page 21 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 61}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nFigure 15: A visual representation of clinical report generation mechanism in which different types of image modalities\\nare independently diagnosed by Radiologist and AI model, and then compared to each other to provide a precise medical\\nreport.\\n6. A Recapitulation on Vision Transformer Application to Medical Image Analytics for\\nAssisted Diagnosis\\n6.1. ViT in Image-based Disease Classification'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 61}, page_content='6.1. ViT in Image-based Disease Classification\\nDeep learning has recently come to the power in a variety of research domains. Convolutional Neural Networks\\n(CNNs)havebeenthemostdominantdeepneuralnetworksforautonomousmedicalimageanalysisapplicationssuch\\nas image classification during the last decade. These models, however, have shown poor performance in learning the\\nlong-range information, due to their localized receptive field, which limits their capabilities for vision related tasks.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 61}, page_content='Transformer architecture, proposed by Vaswani et al. [16], is currently the most popular model in the field of natural\\nlanguage processing (NLP). Getting inspiration from the success of self-attention based deep neural architectures,\\nDosovitskiy et al. [19] introduced Vision Transformer (ViT) model for image classification based applications. In\\nthese models, the overall training process is predicated on dividing the input image into patches and considering each'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 61}, page_content='embedded patch as a word in NLP. Self-attention modules are used in these models to learn the relationship between\\nthe embedded patches.\\nIn the following section, we will take a step forward in exploiting the potential applications of self-attention based\\narchitectures like Vision Transformers (ViT) for the task of medical image classification such as COVID-19 detection\\nand severity quantification, Emphysema classification, tuberculosis prognosis etc. The section is further divided into'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 61}, page_content='medical imaging modalities, with a focus on contributions made by vision transformers to the respective medical\\napplications. The section ends with a tabular summary of the proposed approaches and their performance matrices.\\n6.1.1. Computed Tomography (CT) Scans:\\nPulmonaryNoduleCharacterization: Lungcancerisoneofthemostfrequentlyreportedcausesofcancer-related\\nmorbiditiesandmortalities[153].Earlydetectionandtreatmentofmultiplepulmonarynoduleshasbecomeachallenge'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 61}, page_content='in clinical practice as it is one of the most efficacious ways to reduce the number of fatalities associated with the\\ncondition. Prior research [154, 155, 156] on detection and characterization of lung nodules focused on learning the\\n. : Page 22 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 62}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nassociations between various nodules. In other words, they particularly use solitary nodule approaches on several\\nnodular patients, ignoring the relational/contextual information. To overcome this issue, Yang et al. [157] proposed a\\nMultiple Instance Learning (MIL) strategy, which empirically proved the utility of learning the relationships between'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 62}, page_content='nodules. It’s the first time researchers have looked into the relationships between several lung nodules and extracted\\ncritical relational information between solitary-nodule voxels. Instead of using typical pooling-based aggregation in\\nmultiple instance learning, they created Set Attention Transformers (SATs) based on self-attention to understand the\\nrelationships between nodules. A 3D DenseNet is employed as the backbone to learn representations of voxels at the'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 62}, page_content='solitary-nodule level. The SATs are then used to determine how several nodules from the same patient are related.\\nThis data-driven methodology might aid in understanding of etiologic and biologic processes as well as metastasis\\ndiagnosis of multiple pulmonary nodules and motivate clinical and biological research on this important topic.\\nEmphysema Classification: Chronic obstructive pulmonary disease (COPD) is a heterogeneous disorder with a'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 62}, page_content='varietyofcharacteristics,includingsmallandlargerespiratoryinflammation,aswellasEmphysema,whichisthemost\\ncommon cause of progressive lung tissue loss. Emphysema, as characterized by the destruction and persistent growth\\nof the alveoli, can be classified automatically which can aid in determining and quantifying lung destruction patterns.\\nIn this regard, Convolutional Neural Networks (CNNs) serve an essential role, particularly in pulmonary CT image'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 62}, page_content='classification, but transformers have yet to be explored. As a result, Wu et al. [158] conducted a thorough assessment\\nand extensive evaluation of the ViT model for Emphysema classification. First, large image patches (16 x 16) are\\ncropped from CT scans. After resizing, the patches are flattened and linearly embedded to create a sequence of patch\\nembeddings. The positional information is kept by concatenating the class embeddings with the patch embeddings.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 62}, page_content='To acquire the representation, the final embedding sequence is passed into the transformer encoder module. Finally,\\nthe learnable class embedding is fed to a softmax layer for Emphysema classification. Despite the fact that this study\\nemployed a vision transformer model to classify emphysema, unlike other techniques that use CNN models, it still\\nhasseverallimitations.Forexample,patch-basedclassificationmaynotbeasconvenientaspixel-basedsegmentation.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 62}, page_content='Furthermore, the architecture just uses the transformer encoder block, and the CNN’s benefit is not utilized. In short,\\nEmphysema quantification is difficult, and classification is merely the first step. Proposing more efficient networks\\ncapableoflearningsemanticinformationofEmphysemabypartialoraccurateannotationsmaybeapressingneed.In\\nthe near future, more research on the segmentation and quantification of Emphysema will be conducted.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 62}, page_content='COVID-19 Detection: The infectious Coronavirus (COVID-19) and lung disorders have been at the vanguard of\\nthe research community as the pandemic has caused significant public health concerns throughout the world. Using\\ncomputer vision methods, several attempts [159, 160, 161] are being undertaken to create automated systems for\\nfaster and more effective diagnosis of COVID-19. As per several research studies [162, 163], some radiographic'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 62}, page_content='manifestations, such as broncho vascular thickening, Ground Glass Opacities (GGO), crazy-paving pattern, and\\nconsolidation, have been found in chest CT images. However, with the rapidly growing number of patients in the\\ncurrent situation, radiologists have a significant challenge in manually interpreting CT scans.\\nAmbita et al. [164] were the first to use a vision transformer to the task of COVID-19 detection from computed'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 62}, page_content='tomography (CT) scans. They implement a variety of vision transformers (e.g., ViT-B 16, ViT-B 32, ViT-L 16, ViT-\\nL 32, and ViT-H 14) for image classification. They employed ResNet-based Self-Attention Generative Adversarial\\nNetwork (SAGAN-ResNet) as a data augmentation approach for synthetic image generation to alleviate the problem\\nof insufficient data. Furthermore, they demonstrated how ViT offers visualizations for the images by exhibiting which'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 62}, page_content='sectionsoftheinputimagethemodelfocusesitsattentiononinthevariouslayers.Thismightbeusefulforradiologists\\nwhenanalyzingCTscans.Improvementsmayalsobemadebyevaluatingtheproposedapproachondifferentdatasets\\nand customizing the architecture of transformers or GAN.\\nZhang et al. [165] attempted to broaden the scope of vision transformers such that they may be used as a robust\\nfeature learner for COVID-19 diagnosis in 3D CTs. Inspired by the success of Swin vision transformer and CT'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 62}, page_content='classification work in [166, 167], their framework is made of two key stages: lung segmentation followed by image\\nclassificationusingSwintransformerasabackbone.Inthefirststage,apre-trainedUnetisusedforlungsegmentation\\nin CT scans and produced a lung mask that restricted learning to certain lung regions. The features from each 2D\\nCT slice are then extracted using a Swin vision transformer, which are subsequently aggregated into 3D volume level'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 62}, page_content='features using a max-pooling layer. However, it’s worth noting that the framework equipped with the backbone of\\nEfficientNetV2-M achieves a good speed-accuracy tradeoff according to the results on the validation dataset. This\\nimplies that in future study, merely increasing the model size might result in an improvement in classification.\\n. : Page 23 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 63}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nAbovereviewedstudiesareevidentthatvisiontransformers(ViT)isanovelandquicklyevolvingapproachthathas\\ndemonstratedexcellentresultsusingCOVID-19datasets.Thanetal.[168]conductedapreliminarystudytoinvestigate\\nthe efficacy of using ViT with different sized patches on CT scans of diseased lungs, COVID-19 infected lungs, and'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 63}, page_content='normal lungs. A default positional embedding is used and the combination is then passed to a transformer encoder\\nwhich is composed of alternating layers of multi-headed self-attention and multi-layer perceptron (MLP) units. The\\ntransformer encoder’s output is sent into an MLP head, which outputs the predicted class. The proposed methodology\\nissimplerandlessdemandingoncomputationalresourcesascomparedtoCNNs,however,pretrainingthetransformer'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 63}, page_content='andaddingaconvolutionallayerinfrontofit(i.e.Convolutionalvisiontransformers)mayincreaseperformance.Aside\\nfromthat,otherhyper-parameterscanbemodifiedtoincreaseperformance,andexplainableartificialintelligence(XAI)\\nwill be used in the future to explain how deep learning networks like ViT make decisions.\\nCOVID-19 CT scans contain not only the local features, such as local crazy-paving and local hemangiectasis,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 63}, page_content='but also have global characteristics. Since it is characterized by the combination of both local and global features,\\nextracting image features with relatively complex features is a challenging classification problem of such medical\\nimagingmodalities.Fanetal.[169]proposedaparallelbi-branchnetwork(TransCNNNet)thatisessentiallybasedon\\ntheTransformerandConvolutionalNeuralNetwork.Unliketheconventionalapproaches,thesizeoftheconvolutional'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 63}, page_content='filter kernel is not changed to extract features at different scales; instead, they use the global receptive field of the\\ntransformer network. A bi-directional feature fusion structure is then designed, which fuses the extracted features\\nfromtwobranchesbi-directionally,forminganetworkthathasthepotentialtoextractmorecomprehensiveandample\\nfeatures.\\n6.1.2. X-ray or Radiographic Images:'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 63}, page_content='features.\\n6.1.2. X-ray or Radiographic Images:\\nPneumonia Classification: Pneumonia is an infectious disease in which the alveoli in the lungs is to be filled with\\nfluid or pus, making it painful to breath as well as decreasing oxygen intake. A detailed inspection of chest X-ray\\nimages by the radiographer or radiotherapist is required to diagnose Pneumonia. As a result, pneumonia detection is a'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 63}, page_content='time-consuming process, and even a slight error can result in an excruciatingly painful outcome. Several researchers\\nhaveexploredvariouscomputervisionapproachestodiagnosePneumonia,usingX-Rayimagesofhumanchests.Tuagi\\netal[170],haveproposedavisiontransformer(ViT)modelfortheclassificationofpneumonia.Further,tovalidatethe\\nperformanceoftheproposedmodel,itwasalsocomparedwiththeCNNmodelandVGG16.ItwasobservedthatViT'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 63}, page_content='outperformedothertechniquesreachingthehighestaccuracyof0.96.Inaddition,itoutperformedothermodelsinterms\\nof computational cost as well. It is observed that the model still requires further experimentations to investigate the\\nrobustnessofthemodelwithmoreheterogeneousdata.Therehasn’tbeenalotofworkdoneusingvisiontransformers\\nin the field of chest x-ray diagnosis. It may be useful in the diagnosis of other disorders, such as Covid-19 detection,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 63}, page_content='Cystic Fibrosis or Emphysema, Edema, Pleural thickening, Effusion, and even Cancer.\\nTuberculosis Prognosis and Detection:Tuberculosis refers to an infection that affects the lungs and can travel\\nto other parts of the body. It can be diagnosed and assessed by referring to chest x-rays. If the infection is cured at\\nan earlier stage, it can save a person from further misery of painful treatment. For the classification of infected and'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 63}, page_content='non-infected chest x-rays, a methodology was introduced by Duong et al. [171]. The authors have used EfficientNet\\nwithvisiontransformersforthedetectionoftuberculosisusingchestX-raysimages.Theperformancemetricsinclude;\\naccuracy, precision and recall, f1-score, and area under the curve. The highest accuracy achieved was 97 percent with\\nthe backbone of efficient net B1. Hence, the paper validates the robustness of vision transformer models used on a'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 63}, page_content='heterogeneous dataset. However, it should be further evaluated on different baselines models.\\nInterpretable COVID-19 Detection & Severity Quantification:The novel coronavirus disease 2019 (COVID-\\n19), caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), has become one of the deadliest\\nvirusesofthecenturyasofApril2021,infectingover137millionpeoplewithover2.9milliondeathworldwide.Inthe'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 63}, page_content='contextofunprecedentedCOVID-19pandemic,publichealthsystemshavebeenhitbyaslewofchallenges,including\\nscarcity of medical resources, that are pushing healthcare workers to face the threat of infection. Deep learning and\\nComputer Vision are commonly employed in numerous fields of medical imaging for the diagnosis of COVID-19\\nfrom radio-graphic images, X-rays or CT scans. Even though these techniques have yielded commendable outcomes,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 63}, page_content='thecostisalwaysconsideredasanimportantfactorforthesystemtobeapplicable.Theuseofacomputedtomography\\n(CT) scan for COVID-19 diagnosis offers great sensitivity and specificity [172], but it is a severe burden due to its\\nhigh cost and risk for cross-contamination in the radiology suite. In comparison of CT Imaging, X-rays have been\\n. : Page 24 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 64}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nwidelyutilizedforCOVID-19screening,astheyrequirelessimagingtime,arelessexpensive,andX-rayscannersare\\ngenerally available even in remote regions [173].\\nHence, many researchers have worked on diagnosing COVID-19 using Chest X-ray (CXR) images and through\\ntheseautomatedmethods,counter-checkthePCRtestresults.Forinstance,inthestudypresentedbyParketal.[174],'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 64}, page_content='vision transformers were utilized for both classification and quantification of the severity of COVID-19. Firstly, the\\nlow-level features were extracted using state-of-the-art CNN architectures. Secondly, these extracted features were\\ngiven to the transformer model for classification and severity measurement. Thirdly, the severity was demonstrated\\nthrough heat maps which gives interpretability to the generated results. The robustness of the model was illustrated'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 64}, page_content='throughvariousexperimentationsondifferentbaselinemodelsandtestedonexternaldatasets.Themodelwasevaluated\\nonmetricsincluding;AUC,sensitivity,specificity,andaccuracy.Thehighestaccuracyachievedontheexternaldataset\\nwas above 84.8 percent.\\nSimilarly, the study presented by Shome et al. [175] have proposed a pipeline based on Vision Transformers for\\nthe classification of COVID-19. The model was also compared with other baseline models to validate the robustness'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 64}, page_content='of vision transformers. As compared to [174], it was trained on a larger and more heterogeneous dataset achieving the\\naccuracy of 98 percent and AUC of 99 percent in the binary classification. Furthermore, for the multi-classification\\nwhich includes pneumonia x-rays as well, the model achieves the accuracy and AUC of 92 percent and 98 percent,\\nrespectively. In addition, the model uses Grad-CAM for the visualization through heatmaps, for the explainability of'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 64}, page_content='theoutcome.Althoughthemodelperformedbetterthan[174],themodeldoesnotquantifytheseverityoftheinfection.\\nFurther, this research could be expanded to predict the rate at which the infection can spread.\\nSince the medical data in most domains is inadequate, it can be difficult to build robust models. Considering this\\nissue, Rahhal et al. [176] have put forth a methodology that performs well with small training data. To diagnose'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 64}, page_content='covid-19 in both CT scans and X-ray images, the proposed method uses vision transformers as a backbone with the\\nemployment of a Siamese encoder. The input image with a corresponding augmented image was fed into the siamese\\nencoder, it was then connected to two classifiers. Further, the output of these classifiers was fed into a fusion layer for\\ntheoutcome,followedbyheatmapsondifferentlayerstointerprettheresults.Moreover,thesiameseencodercatersto'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 64}, page_content='theissueregardingdeficientdata.Theproposedmodelhasachievedanaccuracyof94.2percentwhichiscommendable\\nin limited training data.\\nUntilnow,onlyafewCAD-basedapproachesfordetectingCOVID-19havebeendeveloped,buttheireffectiveness\\nhas been hampered due to a number of factors. Taking an inspiration from Convolutional Block Attention Module\\n(CBAM), Nawshad et al. [177] proposed an attention-based Convolutional Module using ResNet32 as the backbone'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 64}, page_content='architecture with a 97.69% accuracy. They also conducted a comparative assessment of a variety of deep learning\\nmodels, including VGG, ResNet, and Xception, for the successful detection of COVID-19 and viral pneumonia.\\nUtilizing the attention module with various CNN-based architectures produced much better results than using the\\nbase CNN architectures.\\nSince it is believed that the newly developing pathogen would have similar low-level CXR features with existing'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 64}, page_content='diseases,theuniqueconceptofproducinghigher-leveldiagnosesbyaggregatinglow-levelfeaturecorpusmaybeused\\nto swiftly construct a robust algorithm against it.\\n6.1.3. Magnetic Resonance Imaging (MRI):\\nMulti-Modal Medical Image Classification:The inadequacy of medical data is discussed in the aforementioned\\npapers, and it can be inferred that researchers are trying to overcome the issue by modeling various architectures.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 64}, page_content='For instance, Dai et al. [178], put forth a hybrid transformer model for the classification of multi-modal images. The\\npipeline consists of a CNN to extract low-level features, and then transformers are utilized for the global context. The\\nmodel was applied on two different datasets: for classification of parotid gland tumors and classification of a knee\\ninjury.ThehighestaccuracyachievedbytheParotidglandtumordatasetandkneeinjurydatasetwas88.9percentand'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 64}, page_content='94.9 percent, respectively. Nevertheless, the study has not presented any means for interpretability of the model, as it\\nis essential, especially in the medical field.\\n6.1.4. OCT or Fundus Images:\\nRetinalImageSynthesisandDiseasePrediction: Forthediagnosisofretinalabnormality,FluoresceinAngiogra-\\nphy(FA)canbeused.Tocapturethevascularstructureoftheretina,afluidisinjectedintothebloodstream.However,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 64}, page_content='several reactions have been reported with the usage of FA. Another approach to diagnosing abnormality is by using\\nFundus images. The vascular structure of the eye is not captured in these images. In the paper [179], the authors\\n. : Page 25 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 65}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nused fundus images and synthesize them to produce FA’s using a generative adversarial network(GAN). Next, these\\nimagesarethengiventoatransformermodeltoclassifyanormalandabnormalretina.Themodelachievedthehighest\\naccuracy, sensitivity, and specificity of 85.7, 83.3, and, 90.0, respectively. Moreover, this model needs to be validated'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 65}, page_content='with a more heterogeneous dataset, along with applying this model for predicting other retinal diseases as well.\\n6.1.5. Histopathology Images:\\nColorectal Histopathology Image Classification:Colorectal Cancer (CRC) is a type of cancer that begins in the\\nrectumorcolonandisdefinedbytheuncontrolledgrowthofaberrantcellswiththelatentabilitytoinvadeothertissues.\\nDespitethefactthatmanualinspectionofhistopathologyslidesisstillcrucialinexperimentalpractise,automaticimage'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 65}, page_content='processing enables the quantitative and rapid analysis of malignant tissues. Early detection is crucial for identifying\\nthe appropriate treatment approach and increasing the patient’s chances of survival. As a result, automated techniques\\nare needed to save time and eliminate the risk of human error. Artificial intelligence has recently been put to use\\nin the diagnosis and prediction of several forms of cancer. Zeid et al. [180] used Vision Transformers to perform'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 65}, page_content='a multiclass tissue classification of colorectal cancer, highlighting the potential of employing Transformers in the\\nhistopathological image domain. First of all, a standard vision transformer model was proposed and it achieved an\\naccuracy of 93.3 percent. Since vision transformers demand more data, a hybrid approach combining CNN and\\ntransformer was developed. Low-level features are extracted using the CNN model, and the embeddings are sent to'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 65}, page_content='the transformer. This model is known as a Compact Convolutional transformer, and it achieved an accuracy of 94\\npercent.However,experimentationwithvariousdatasetsanddifferentformsofcancermayalsobedonetoimprovethe\\nmodel’s overall performance. Deep learning algorithms are now becoming increasingly essential for the identification\\nandclassificationofcolorectalhistopathologyimages.Existingtechniques,ontheotherhand,aremoreconcernedwith'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 65}, page_content='end-to-end automatic classification using computers than with human-computer interaction. Hence, Chen et al. [181]\\npresented an IL-MCAM framework. It is based on interactive learning and attention techniques. Automatic learning\\n(AL) and interactive learning (IL) are two steps in the proposed IL-MCAM system (IL). To extract multichannel\\nfeatures for classification in the AL stage, a multi-channel attention mechanism model with three separate attention'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 65}, page_content='mechanism channels and convolutional neural networks is implemented. The proposed IL-MCAM system constantly\\nadds misclassified images to the training set in an interactive method during the IL stage, improving the MCAM\\nmodel’s classification ability. To handle different colorectal histopathological image classification tasks in the future,\\npermutation and combination can be used to identify the best model for the current task from attention mechanisms'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 65}, page_content='anddeeplearningmodels.Moreover,attentionmechanismscanalsobeincludedinvariouslocationsofadeeplearning\\nmodel, in order to investigate the influence of convolutional layers on classification performance.\\nThe table 4 given below summarized the performance gain by the reviewed articles of the classification category.\\nTable 4: List of datasets and performance measures employed by researchers for medical image classification.\\nModality Publication Dataset Performance Measures\\nCT Scans\\nYang et al. [157]'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 65}, page_content='CT Scans\\nYang et al. [157]\\nLUNA16 [88] CPM Score = 0.916\\nLIDC-IDRI [89] Accuracy (%) = 93.17\\nWu et al. [158]\\nAccuracy (%) = 95.95\\nComputedTomographyEm-\\nphysema Database [90] Precision (%) = 98.0\\nRecall (%) = 97.1\\nSpecificity (%) = 98.6\\nCT Scans\\nAmbita et al. [164]\\nCOVID-CT [91]\\nAccuracy (%) = 87.19\\n. : Page 26 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 66}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nPositive Precision (%) =\\n89.11\\nSensitivity (%) = 85.71\\nF1 Score = 0.8738\\nCT Scans\\nSars-CoV-2 [92]\\nAccuracy (%) = 95.41\\nPositive Precision (%) =\\n94.30\\nSensitivity (%) = 98.03\\nF1 Score = 0.9613\\nZhang et al. [165] COV19-CT-DB [93]\\nAccuracy (%) = 94.3\\nPrecision (%) = 93.7\\nRecall (%) = 93.8\\nF1 Score = 0.935\\nMacro F1 Score = 0.94\\nThan et al. [165] COVID-CTset [94]\\nAccuracy (%) = 95.36\\nSenstivity (%) = 83.00'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 66}, page_content='Accuracy (%) = 95.36\\nSenstivity (%) = 83.00\\nLi et al.[182] Private dataset from eight\\ndifferent Hospital [182] Macro F1 (Score) = 0.97\\nMicro F1 (Score) = 0.98\\nX-ray Images\\nTuagi et al. [170] Chest X-ray Images [95] Accuracy (%) = 96.45\\nDuong et al. [171]\\nMontgomery County (MC)\\nCXR Images [183]\\nAccuracy (%) = 97.92\\nShenzhen CXR Dataset\\n[183]\\nCOVID-19 Dataset [184]\\nPark et al. [174] BIMCV COVID-19+ [96]\\nAUC = 0.949\\nAccuracy (%) = 86.8\\nSensitivity (%) = 90.2\\n. : Page 27 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 67}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nSpecificity (%) = 86.2\\nShome et al. [175]\\nAccuracy (%) = 92\\nCOVID-19 Posterior-\\nAnterior Chest Radiography\\nImages [97]\\nPrecision (%) = 93\\nExtensiveCOVID-19X-Ray\\nand CT Chest Images [98] Recall (%) = 89\\nF1 Score = 0.91\\nAUC = 0.98\\nRahha et al. [176]\\nAccuracy (%) = 94.62\\nPrecision (%) = 96.77\\nSars-CoV-2 [92] Recall (%) = 96.77\\nSpecificity (%) = 99.65\\nF1 Score = 0.967\\nMRI Scans Dai et al. [178] MRNet Dataset [92]'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 67}, page_content='MRI Scans Dai et al. [178] MRNet Dataset [92]\\nAUC-ROC = 0.976\\nAccuracy (%) = 91.8\\nSensitivity (%) = 96.8\\nSpecificity (%) = 72.8\\nOCT/Fundus Images Kamran et al. [179] Color Fundus Images [100] Accuracy (%) = 85.7\\nSensitivity (%) = 83.3\\nSpecificity (%) = 90.0\\nHistopathology Images Zeid et al. [180]\\nAccuracy (%) = 93.3\\nColorectalCancerHistology\\nDataset [101] Precision (%) = 93.33\\nRecall (%) = 93.44\\nF1 Score = 0.933\\n. : Page 28 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 68}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\n6.2. ViT in Region-based Lesion Detection\\nThe proceeding section discusses the detection of anomalies in medical images using Vision Transformers, in\\nrelation to the aforementioned modalities.\\n6.2.1. Computed Tomography (CT) Scans:\\nCOVID-19Detection AframeworkforthedetectionofCovid-19wasproposedbyLiangetal.,withchestCTimages'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 68}, page_content='asinput[185].TheframeworkwascomposedofaCNNmodelforfeatureextraction,andthentheSEattentionmodule\\nwasintegratedforthegenerationofattentionvectors.Next,thetransformermodelwasusedtodistinguishthefeatures\\nin the input. The study also proposed a method to resample the inputs, which also contributed to the efficiency of the\\nmodel. The highest f1-score was 88.21 percent which was a 10 percent improvement from the baseline model. The'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 68}, page_content='dataset used, however, was small and imbalanced which doesnot validate the generalizability of the proposed model.\\nAnomalyDetection Inthemedicaldomain,variousmethodologiesareproposedforthedetectionofanomalies.The\\nauthorsofthepaper[186],haveproposedatransformer-basedmodel,whichwasappliedonvariousimagesi.e.retinal\\nOCT, Head- CT Scans, and Brain- MRIs. The representation of the features was learned using autoencoders which'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 68}, page_content='were based on transformers. In addition, to detect the anomalies in multiscale, a transformer model was proposed\\nwith skip-connections, thus it reduced the usage of memory and cost of computation. The models were evaluated on\\nAUROC, achieving 93 %, 95.81 %, and 98.38% for the datasets related to Head-CT, Brain- MRI, and retinal OCT,\\nrespectively.However,theproposedmodelstillrequiresafurtherreductionincomputationalcostsothatitcanbeused\\nin real-time.\\n6.2.2. X-ray or Radiographic Images:'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 68}, page_content='6.2.2. X-ray or Radiographic Images:\\nCOVID-19Diagnosis SinceX-raysarecomparativelycost-effectiveandafasterwayofdiagnosingthevirus,several\\nresearchershaveproposedmethodsfordetectionusingchestx-rays.Inthepaper[187],anotherapproachisproposedto\\ndetect covid-19 using chest X-rays. An adaptive attention network is used which consists of ResNet and an attention-\\nbasedencoder.ResNetisusedtolearnthefeaturerepresentationsandtheAttentionmoduleisthenutilisedfordetection'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 68}, page_content='of the infectious areas. The proposed model was compared with different CNN models on three different datasets.\\nThe evaluation indicated that the proposed model performed significantly better. Moreover, the performance metrics\\nincluded Accuracy, sensitivity, precision, and F1 score. The highest accuracy achieved by the model was 98.5\\nSimilarly, To capture the global context, the authors Kumar et al. have used vision transformers on both X-ray'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 68}, page_content='images and CT images of the chest for the diagnosis of Covid-19 [188]. The data used was labelled as normal,\\npneumonia and covid-19. Furthermore, to address the issue of scarce data, transfer learning is used followed by\\nexplainability through visualisation of the infected areas. The proposed method was compared with other models\\ni.e InceptionV3, CoroNet, CovidNet, etc. The results were evaluated using the metrics; precision, recall, f1-score,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 68}, page_content='accuracy, and specificity. The proposed model outperformed the CNN models reaching the accuracy of 0.96 and 0.98\\nfor CT scans and X-ray images, respectively. However, work on severity information requires attention in both [187]\\n[188].\\nPulmonary Lesions DetectionIn the initial assessment of lung cancer, one of the most used techniques is chest\\nradiography. Since it is essential to diagnose cancer at an earlier stage, many methodologies have been proposed to'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 68}, page_content='detect pulmonary lesions. The study is presented by [189], it has proposed two architectures; convolution networks\\nwith attention feedback, and recurrent attention model with annotation feedback. The first method uses CNN to\\nlearn the features, and generate saliency maps as the soft attention mechanism was incorporated. Next, a recurrent\\nattention model with attention feedback was proposed. The proposed architecture uses reinforcement learning for'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 68}, page_content='better performance of the model. The architectures were evaluated through precision, recall, f1-score, and accuracy.\\nThehighestaccuracyachievedwas85percentforclassification,and74percentforlocalization.Thus,thearchitectures\\nrequire improvement regarding the reduction of computation time and accuracy.\\n6.2.3. OCT or Fundus Images:\\nMicroaneurysms Detection The early diagnosis of lesions in diabetic retinopathy can be done by the detection'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 68}, page_content='of microaneurysms (MA). Since it is difficult to locate them because of their size, several methodologies have been\\nproposed. In this study [190], the proposed methodology for the detection of MAs comprises three stages. First, the\\nimages are preprocessed to improve the quality. Second, a deep network is used with an attention mechanism for\\ndetection. Third, the correspondence between Microaneaurysms and blood vessels is exploited for the final results.\\n. : Page 29 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 69}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nThe performance metrics used for evaluation were precision, recall, sensitivity, and f1-score. The proposed method\\noutperformed prior proposed models with a sensitivity of 0.86. Nevertheless, the model was trained on the images\\nfrom one type of camera, which does not validate the generalizability of the proposed methodology.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 69}, page_content='Glaucoma Detection The authors of the paper [191], have proposed a methodology for the detection of a disease\\nknown as Glaucoma. It causes the loss of vision and is irreversible. The paper has presented a CNN model which was\\nattention-based for the detection of the disease. Furthermore, due to the attention module, the localized features were\\nalso visualized, giving results more explainability. The proposed architecture first locate the area and then classify'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 69}, page_content='the disease. The detection was evaluated using the performance metrics; accuracy, sensitivity, specificity, AUC, and,\\nF2-score,withthehighestaccuracyachievedof96.2percent.However,inthenetwork,themodelsmayidentifyregions\\nwith useless information which may hinder the performance of the model.\\nFurther,Xuetal.havepresentedamodelwhichconsistsofanattentionmodulealongwithtransferlearningforthe'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 69}, page_content='detection of glaucoma [192]. This work has contributed towards the discrimination of general and specific features.\\nSince the models are not able to identify the regions that may give no information, the proposed methodology can\\nextract the regions with more information. In addition, with the attention module, the regions can also be visualized.\\nThe model was then evaluated on two different datasets, achieving the highest accuracy, sensitivity, specificity, and'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 69}, page_content='AUCof85.77percent,84.9percent,86.9percent,and0.929,respectively.Lastly,thismethodcanbefurthervalidated,\\nby applying it to various other eye diseases.\\n6.2.4. Histopathology Images:\\nCancer Detection Barret’s esophagus (BE) refers to the damaging of the swallowing tube that connects the mouth\\nto the stomach because of acid reflux [193]. Ultimately, it increases the risk of esophagus cancer i.e. adenocarcinoma'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 69}, page_content='[194]. Moreover, patients that suffer from BE are at a higher risk of cancer. The detection of lesions at an early stage\\ncan prevent the suffering of patients from cancer, with a better survival rate.In the paper[195], attention-based deep\\nneural networks were proposed for the detection of cancerous and precancerous esophagus tissues. The model uses\\nattention-based mechanisms to detect the cancerous tissues belonging to the classes; normal, BE-no-dysplasia, BE-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 69}, page_content='with-dysplasia, and adenocarcinoma. The mechanism does not require annotations for regions of interest, thus, it\\ndynamically identifies the ROIs. Hence, it is independent of the annotated bounding box and does not require a fixed\\nsizeofinputimages.Theproposedmethodwascomparedwiththeslidingwindowapproachbasedontheperformance\\nmetrics; accuracy, recall, precision, and accuracy. The model outperformed the sliding window method in all classes'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 69}, page_content='withanaverageaccuracyof0.83.However,themodelwastrainedonasmalldataset,hencetherobustnessofthemodel\\nstill needs to be verified using more data.\\nThe issues regarding whole-slide images in terms of detection, include poor adaptability of the model, explain-\\nability, and resource-efficient model. The authors of the paper[196], have proposed a model known as clustering-\\nconstrained-attention multiple-instance learning(CLAM). It was applied to detect three types of cancers; renal cell'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 69}, page_content='carcinoma, non-small cell lung cancer, and breast cancer lymph node metastasis. The proposed method CLAM is a\\nweakly supervised algorithm, it uses an attention module to determine the regions, and classify the cancer type. In\\naddition, it also localized the affected regions with interpretability. The models were evaluated using AUC, hence, it\\nwas greater than 0.95. On contrary to this data-efficient model, it considers various locations as independent, thus,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 69}, page_content='leading to a less context-aware model.\\nNext, another model was used for the detection of cancer leading to the prediction of survival prediction [197].\\nThe framework is a multimodal co-attention transformer( MCAT), that learns the correspondence between WSI’s and\\ngenomic features. The attention module ensures interpretability along with the reduction of memory usage of image\\nbags. The model was applied to five different cancer datasets, and the results were compared with the state-of-the-art'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 69}, page_content='models.\\nThe table 5 given below summarized the performance gain by the reviewed articles of the detection category.\\n. : Page 30 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 70}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nTable 5: List of datasets and performance measures employed by researchers for Region-based Lesion Detection.\\nModality Publication Dataset Performance Measures\\nCT Scans Mondal et al. [188] COVIDx-CT-2A [104]\\nAccuracy (%) = 98.1\\nRecall(%)=96\\nPrecision(%)=96\\nSpecificity(%)=98.8\\nF1(Score)=0.96\\nLiang et al. [185] COV19-CT-DB [93] Macro F1 (Score) = 88.21\\nMicro F1 (Score) = 0.98\\nX-rays\\nLin et al. [187]\\nCOVIDx [99]'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 70}, page_content='X-rays\\nLin et al. [187]\\nCOVIDx [99]\\nAccuracy (%) = 95\\nsensitivity(%)=97\\nPrecision(%)=98.98\\nSpecificity(%)=99.47\\nF1(Score)=0.97\\nCOVIDGR-E [102]\\nAccuracy (%) = 89.53\\nsensitivity(%)=86.05\\nPrecision(%)=83.15\\nSpecificity(%)=91.28\\nF1(Score)=0.84\\nDLAI3 Accuracy (%) = 98.55\\nsensitivity(%)=98.63\\nLin et al. [187] Precision(%)=98.63\\nSpecificity(%)=99.90\\nF1(Score)=0.98\\nPrecision (%) = 15\\n. : Page 31 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 71}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nPesce et al. [189]\\nA dataset consisting of\\n745,479 chest x-ray exams\\ncollected from the historical\\narchives of Guy’s and St.\\nThomas’ NHS Foundation\\nTrust in London from\\nJanuary2005toMarch2016\\nSensitivity (%) = 65\\nAverage Overlap (%) = 43\\nFundus Images\\nZhang et al. [190] IDRiD [103]\\nAccuracy (%) = 94.3\\nPrecision (%) = 87.2\\nRecall (%) = 81.0\\nF1 Score = 0.840\\nSensitivity (%) = 86.8\\nAccuracy (%) = 96.2'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 71}, page_content='Sensitivity (%) = 86.8\\nAccuracy (%) = 96.2\\nSensitivity (%) = 95.4\\nLi et al. [191]\\nLAG Database (obtained\\nfrom Chinese Glaucoma\\nStudy Alliance (CGSA) and\\nBeijing Tongren Hospital.)\\n[191]\\nSpecificity (%) = 96.7\\nAUC = 0.983\\nF2 Score = 0.954\\nXu et al. [192]\\nLAG Database (obtained\\nfrom Chinese Glaucoma\\nStudy Alliance (CGSA) and\\nBeijing Tongren Hospital.)\\n[191]\\nAccuracy (%) = 85.7\\nSensitivity (%) = 84.9\\nSpecificity (%) = 86.9\\nAUC = 0.929\\nTomita et al. [195] Accuracy (%) = 83.0\\nRecall (%) = 60.0'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 71}, page_content='Recall (%) = 60.0\\n. : Page 32 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 72}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nHistopathology Images\\nHistologicalimagesbetween\\nJanuary 1, 2016, and\\nDecember 31, 2018, at\\nDartmouth-Hitchcock\\nMedical Center (Lebanon,\\nNew Hampshire) were\\ncollected.\\nPrecision (%) = 62.0\\nF1 Score = 0.59\\nChen [197] The Cancer Genome Atlas\\n[105]\\nConcordance Index (c-\\nIndex) = 0.653\\n6.3. ViT in Anatomical Structure Segmentation'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 72}, page_content='6.3. ViT in Anatomical Structure Segmentation\\nClear cut and detailed segmentation is a decisive step in image guided treatment and computer-aided diagnosis.\\nA great deal of image segmentation models have been proposed In the last 40 years from traditional models to deep\\nneural networks. But since the emergence of transformers, they have outperformed all the state of art segmentation'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 72}, page_content='models. Transformers functions prominently in error free segmentation of medical images because of their capability\\nto model the global context. As the organs lay out over a wide receptive field, hence, transformers can easily encode\\nthese organs by modeling the association of pixels that are distant spatially. Moreover, the background is dispersed in\\nmedical scans, for that reason gaining the understanding of the global context between those pixels that relate to the'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 72}, page_content='backgroundwillbebeneficialforthemodeltodotheunerringclassification.Belowwereviewedexperimentsthattried\\nto exploit ViT based models for a faultless segmentation. We divided these experiments in accordance with different\\nmodalities used for medical imaging. In the end we give all the results obtained during these experiments on specific\\ndatasets in tabular form.\\n6.3.1. Computed Tomography (CT) Scans:'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 72}, page_content='6.3.1. Computed Tomography (CT) Scans:\\nCoronary Artery Segmentation A precise and correct segmentation of CAC is advantageous for early CVD\\ndiagnosis. But as CAC has blurry and distorted boundaries, the task of segmentation is not very much satisfactory.\\nTo address this issue Ning et al. [198] introduced an efficient multiscale vision Transformer for the segmentation of\\ncoronaryarterycalciumandnameditasCAC-EMVT.Thisarchitectureutilizedthelocalaswellasglobalfeaturesand'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 72}, page_content='thenusedthemcollectivelytomodelthelongandshort-termdependencies.Theirmodelwascomprisedofthreemain\\nmodules,(a)KFS,KeyFactorSamplingmodulethattheyutilizedforextractingthekeyfactorsfromtheimage.These\\nkey factors were made use for low-level reconstruction of highly structured features, (b) NSCF, a non local sparse\\nnet fusion module, that was used to model the information of high level features of texture, (c) NMCA, a non local'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 72}, page_content='multiscale context aggregation, that was used to get the dependencies of long range at different scales. Experimental\\nresults showed that their model outperformed the state of the art methods at that that by giving a Dice similarity\\nco-efficient of 75.39%± 3.17.\\nLeeetal.[199]introducedanewconceptoftemplatetransformernetworksforsegmentationthroughshapepriors\\n(TETRIS), and performed coronary artery segmentation through their model. In this concept they used an end to'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 72}, page_content='end trainable Spatial Transformer (STN)[200] to deform a shape template to complement the under laying region of\\ninterest. They also used this concept of incorporating the priors to the state of the art CNN and U-Net used for binary\\nclassification.TheexperimentalresultsofTETRISandU-Net[139]incorporatingtheprior,wereabletoproducesingly\\nconnected components because they were given the prior information and gave to dice scores of 0.787 and 0.854'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 72}, page_content='respectively. They also compared the U-Net[139] with FCN by giving the prior shape but FCN[201] didn’t perform\\nthat well giving the Dice Score of 0.79 only.\\nLung Tumor SegmentationPET-CT segmentation requires information from both PET and CT modality. Most of\\nthe models get the segmentation information of these modalities separately. In a study, Fu et al. [202] established a\\nmoduleMSAM,MultimodelSpatialAttentionModule,adeeplearningbasedframeworkforlungtumorsegmentation\\n. : Page 33 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 73}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\ninPET-CT.MSAMwasanimpulsivemoduleanditwasabletohighlightthespatialareasorregionsthatarelinkedto\\nthetumorandcensoredthenormalregionsofinputspontaneously.ThismodulewasfollowedbyaCNNthatwasacting\\nasabackboneandthisCNNwasperformingthesegmentationtaskonthemapthatwasprovidedtoitasaninputfrom\\nthe multi model spatial attention module. U-Net[139] was used as backbone for that purpose. They performed their'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 73}, page_content='experimentsontwoclinicalPET-CTdatasetsofNSCLCandSTS.Theresultsoftheexperimentsgaveadicesimilarity\\ncoefficient of 71.44% and 62.26% for NSCLC and STS datasets respectively. in order to refine this architecture, more\\nbetter procedures can be used to further improve the segmentation.\\nRenal Tumor SegmentationDue to the diversification that is present in size and pose, the task of segmentation has'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 73}, page_content='become a strenuous task. Hence La et al. [203] proposed a network that was both size and pose invariant and they\\ntested their network for renal tumor segmentation on 2D CT Scan images. Their architecture was comprised of three\\nsequentialmodulesthatworkedtogetherinthetrainingprocess.Firstwastheregressionmodulethattheyusedtofind\\nthe similarity matrix of input image to the ground truth. Second module was used to find the region of interest and'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 73}, page_content='theynamed itasdifferentiable module.Thethird andlastmodule wasusedto performthesegmentation taskandthey\\nusedU-Net[139]forthispurpose.TheyusedtheSpatialTransformer(STN)[200]intheirarchitecturetoautomatically\\ndetect the bounding box which saved time. Results indicated that the training time was reduced by 8 hours and the\\nDice Score for kidney almost remained same which was 88.01%, but in case of renal tumor, the score got better from'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 73}, page_content='85.52% to 87.12%. one of the shortcomings of their model was that it was valid for only small set of data.\\nAortic Valve SegmentationBasic CNN models for segmentation were performing good on 2D images and they\\nwere struggling against 3D medical imaging. Hence Pak et al. [204] proposed a deep learning based architecture for\\nthesegmentationof3DCTScanimages.ThisnetworkwascomprisedofabaselineU-NetArchitecturethatperformed'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 73}, page_content='the basic segmentation task and a Spatial Transformer[200] that was used to perform some affine transformation. The\\nuse of only U-Net[139] was not sufficient for the segmentation tasks as it requires a lot of memory and also result in\\ndecrease of accuracy. Hence they used a spatial Transformer (STN)[200] which reduced the size of input image by\\nperformingsometransformationandhenceitresultedinbettercomputation.theyutilizetheirmodeltoperformaortic'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 73}, page_content='valvesegmentation.Upontestingtheirmodelondifferentpatientsdata,theDiceScorecoefficienttheygetwas0.717.\\nBone Segmentation In order to perform the segmentation of bone as well as the localization of the anatomical\\nlandmarks of cone beamed computed tomography data simultaneously Lian et al. [205] proposed a network called\\ndynamictransformernetwork(DTNet).Theirmodelcontributedinthreeparts.Inthefirstpart,asynergicarchitecture'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 73}, page_content='was made to accurately catch the global context and fine grained details of image for volume to volume prediction.\\nSecondly, by using the anatomic reliance between landmarks RDLs are made to collectively degenerate the large 3D\\nheatmapsofeverylandmarks.Thirdly,ATMsaremadeforthecompliantlearningofcontextspecificfeatureembedding\\nfrommutualfeaturebases.WhiledoingtheexperimentsonCTscansofmandible,thesegmentationDSCcameoutto'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 73}, page_content='be 93.95% with a std dev of 1.30 whereas for localization the RMSE was 1.95± 0.43. these results were better than\\nU-Net and other models that were used for comparison.\\nLesion Segmentation The early diagnosis of AIS provides valuable knowledge about the disease. But for a human\\neyeitisburdensometodiscriminatedelicatechangesinpathology.Hence,Luoetal.Luoetal.[206]proposedanetwork\\nforthesegmentationofAcuteIschemicStroke(AIS)thatwasbasedonself-attention.Thismechanismhadanencoder'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 73}, page_content='and a decoder. The encoder was comprised of a CNN as a backbone and a transformer. This encoder part picked the\\nglobalcontextfeatures.ThedecoderpartconsistedofMultiHeadCrossAttention(MHCA)modulewhichupsampled\\nthe feature maps that were coming from the encoder. These feature maps were connected via skip connections. The\\nbackboneCNNusedwasRESNET-50.TheirexperimentalresultswerecomparedtoattentionU-Net[207],U-Net[139]'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 73}, page_content='and TransUNet[208] but their model outperformed them by giving the Dice Similarity Coefficient of segmentation of\\nlesion up to 73.58% which was better than all other compared models.\\nSegmentation of OrgansAlthough transformers help in capturing the long term dependencies but when it comes\\nto the segmentation of 3D images, the dependencies face extreme computation. Hence to reduce some computations,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 73}, page_content='Xie et al. [209] presented CoTr, which is a combination of convolutional network and transformer. Instead of using\\nsimpletransformer,theyintroduceddeformabletransfersforcatchingthelongrangedependencies(DeTrans).DeTrans\\nfocusses on only a few key points, which greatly reduces the computational complexity, which also allows to process\\nmultiscaleimages,whicharequiteimportanttoattainanaccuratesegmentation.theytestedtheirmodelonBCVdataset\\n. : Page 34 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 74}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nthatincludestheimagesof11differenthumanorgans.CoTrachievedaDiceScoreof85andHausdorffDistanceof4.01\\nonavgandthesemeasureswerebetterthanothermethodsthattheyusedtocomparetheirmodel.Furtherenhancement\\nin their model could be that it can be enhanced by extended it to operate on different modalities.\\n6.3.2. Magnetic Resonance Imaging (MRI) Scans:'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 74}, page_content='6.3.2. Magnetic Resonance Imaging (MRI) Scans:\\nBrain Tumor SegmentationGlioma segmentation and prediction of IDH genotyping is an important and difficult\\ntaskduetosimilaritiespresentinintraandinter-tumor.ToaddressthisproblemChengetal.Chengetal.[210]proposed\\nan MRI based fully automated multi model that could predict IDH genotyping and Glioma segmentation at the same\\ntime. The pre existing methods were not able to perform the both tasks at the same time, also these methods faced the'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 74}, page_content='problems of inter and intra-tumour heterogeneity. So they address these issues by using a joint CNN and Transformer\\nencoder. The transformer was used to extract the global features that were used for the glioma segmentation. It also\\ncontained a multi-scale classifier, which was used for IDH genotyping. A multi-task loss was then used to balance\\nthe segmentation and IDH genotyping and this loss collectively joined the classification loss and segmentation loss.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 74}, page_content='In the end they proposed an unpredictability aware pseudo-label-selection to make pseudo-labels for IDH on a large\\nunlabeledDataset.TheynamedtheirmodelasMTTU-Net.OnexperimentstheirmodelimprovedtheHD95andDice\\nscore 1.69mm and 1.23% for glioma segmentation and 2.13% and 4.28% in case of AUC and accuracy respectively.\\nSagar et al. [211] proposed ViTBIS, vision transformer for bio medical image segmentation, that was based on'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 74}, page_content='encoder and decoder architecture. Both encoder and decoder had transformer inside them. The feauture map of input\\nimage was split into three different convolutions before it was fed to the transformer. These convolutions were, 1X1,\\n3X3and5X5.Thesethreedifferentfeaturemapswereconcatenatedwiththehelpofconcatoperator,thenitwasfedto\\nthetransformerintheencoder.Thesetransformerhadtheattentionmechanisminsidethemthetransformersofencoder'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 74}, page_content='and decoder were joined together via skip connections. The same architecture of multiscale was used in the decoder\\nas well. Before producing a segmentation mask after linear projection, different sizes were concatenated via concat\\noperator. Upon testing their architecture on a public dataset for brain tumor segmentation the DSC achieved was 0.86\\nwhich was better than other state of the art CNN and transformer networks.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 74}, page_content='Brain Tissue SegmentationIn order to solve the problem of multi model medical image segmentation, Sun et al.\\n[212] presented a novel multi model architecture based on transformer and Convolutional Neural Network for multi\\nmodel image segmentation and named it as HybridCTrm, and used this model to segment different brain tissues. This\\nnetworkusedtwopathsfortheimageencoding,onepathwasfromtheCNNandtheotherpathwasfromTransformer.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 74}, page_content='Thentherepresentationofimagefrombothpathswerejoinedtogetherfordecodingandthesegmentationpurpose.The\\nCNN controlled the rapid convergence of gradient descent while extracting the local features, whereas the non local\\nfeatures were extracted by the transformer. They used two strategies for the fusion, one was the single path strategy\\nand the other was the multiple path strategy and used both of these strategies in their experiments. Experiments were'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 74}, page_content='carried out on two different datasets and by following both strategies. On MRBrainS dataset the DSC came out to be\\n82.98and83.47forsigleandmultiplepathstrategiesrespectivelywhereasontheiSeg-2017datasetthesescoreswere\\n86.75 and 87.16 which were better than the models they used to do the comparison like HyperDenseNet[213],\\nBrainStructureSegmentation Agooddealofdeeplearningarchitecturesusedtoperformthetaskofsegmentation'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 74}, page_content='on medical images confront the problem of noise at the inference time and result in inaccurate result. To address this\\nproblem Sinclair et al. [214] proposed a network, Atlas-ISTN, atlas image and spatial transformer network, that was\\nable to perform both registration as well as segmentation on 2D and 3D data of Brain structure. This network could\\nperform segmentation on numerous interest regions/ structures and to register the atlas label map to an in-between'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 74}, page_content='segmentation(pixel-wise). This model was also able to do the fine tuning of the parameters at the inference time in\\nordertoachievebetterpixelwisesegmentation,duetowhichtheeffectofnoiseintheimagealsoreduced.Thismodel\\nwas then tested on three different datasets, two 3D and one 2D. the results were compared with U-Net and this model\\nwas performing better than U-Net[139] giving a DSC of 0.888.\\nCardiacSegmentation Combiningthesharedinformationofanyorganfromdifferentmodalitiesisveryhelpfulfor'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 74}, page_content='learningandmultimodalityprocessing.Inordertoschievethis,Chartsiasetal.[215]proposeddisentangled,alignand\\nfuse network, DAFNet, that was able to learn the information present in different modalities input, hence producing\\na more precise segmentation mask. Anatomical factors from different inputs are combined and processed at the same\\ntime.DAFNetcollectedtheinformationpresentindifferentmodalitiesdespiteofthefactthatfewlabels(supervised)are\\n. : Page 35 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 75}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nthere or even no labels (unsupervised). Spatial Transformer was used to align the anatomical factors in case of image\\nmisregistration. They evaluated their model by performing L2, T1 and Cardiac segmentation on different datasets.\\nTheirmodelwasabletoperformonbothsinglemodelandmultimodelinputsanditoutperformedothermodelswhen'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 75}, page_content='it was trained on single modality input whether with few labels(semi supervised) or no labels (unsupervised).\\nDefiningtherightventricle(RV)structureincardiacsegmentationisastretchingworktodobecauseofitscomplex\\nandmultiplexstructure.Henceitrequiresshortaxisaswellaslongaxisimages.InordertoaddressthisissueGaoetal.\\n[216] established a consistency based co training mechanism that used the geometric relationships between different'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 75}, page_content='view CMR images for the segmentation. Along with this mechanism, they also used the U-Net[139] architecture in\\nordertocapturesomelongrangedependencies.EvaluationofthemodelwasdoneontheM&MS-2challengedataset\\nand the Dice score came out to be 0.83 and 0.86 for short axis and long axis respectively.\\nColerectalCancerSegmentation InastudydonetosegmentthecolorectalcancerregionSuietal.[217]established'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 75}, page_content='a novel approach, based on transformer, that performed the segmentation as well as detection of colorectal cancer\\nregion collectively. Their model was based on two pipelines, one for the detection and the other for the segmentation.\\nIn the detection part, region proposals were generated. They utilize image level decision approach that was based\\non auto encoders. Whereas in the segmentation part they used patches of the image as input and to make the final'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 75}, page_content='mask prediction, class embeddings were used. They compared their model with the Faster CNN and Yolo-v3 for the\\ndetection task and their model performed exceptionally well on the used dataset, giving an accuracy of 88.6% where\\nas the segmentation score came out to be 91.1% which was way better than U-Net[139] and FCN[201].\\n6.3.3. X-ray or Radiographic Images:\\nBreast Tumor SegmentationCorrect and accurate segmentation of tumor in ABVS is a difficult task because the'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 75}, page_content='sizeofimageishugeanditsqualityislow.InordertosegmenttumorfromtheABVSimages,Liuetal.[218]adopted\\nthe use of both transformers and CNNs and named their model as 3D-UNet. They joined the attention module and\\nthe U-Net[139] model. For further improvements in the performance they also made use of Atrous Spatial Pyramid\\nPooling(ASPP)intheirmodel.ASPPcanhelpcatchtheinformationatmultiscales.Theycomparedtheirmodelwith'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 75}, page_content='different3Dsegmentationmodelslike3DFCN[219],3DPSPNet[220]andrecordedtheDiceScoreCoefficient.Their\\nscore recorded as 76.36±6.11, which was better than the networks that were used for the comparison.\\nAnatomicalStructureSegmentation Mostofthesegmentationnetworksworkonsupervisedlearningwhereexpert\\nlabeled image is required as a label and this is an obstacle if there aren’t much experts available. Hence in order to'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 75}, page_content='make an annotation efficient Lu et al. [221] introduced Contour Transformer Network, CTN, which is an annotation\\nefficient segmentation method for anatomical structures. They copied the human ability doing the segmentation of\\nanatomical structures with very less exemplars available. To achieve this, they proposed a semi supervised learning\\nmechanismthatutilizetheresemblanceofstructureandappearanceofthedesiredobjectbetweenunlabeledandlabeled'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 75}, page_content='images. They made the segmentations of anatomy in the form of contour evolution process and model the behavior\\nby GCNs. They named their model as one shot anatomy segmentation model. On performing the segmentation on\\nfour different anatomies, their model comprehensively performed better than u supervised learning mechanisms and\\nperformed competitively against the supervised state of the art methods. Upon experiments the accuracy of one shot'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 75}, page_content='model came out to be 96.58% which was almost 15% better than Braintorm, which is another one shot based model.\\nWhereas in comparison with non-learning based model, the accuracy was 16% improved. one shortcoming of their\\nnetwork is that their network only performed on 2D data. hence extending this architecture to work on 3D data would\\nbe an important step in the field of 3D segmentation.\\nGuide Wire SegmentationA study done by researchers tried to resolve the task of segmentation of guide wire'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 75}, page_content='in X-ray fluoroscopy sequence. Zhang et al. [222] proposed a network that takes in account current frame as well\\nas the previous frame while taking input for the guide-wire segmentation. By considering both frames helped them\\nin obtaining the temporary information. Their network contained two parts, one was a CNN and the other was a\\ntransformer. The CNN wasn’t able to capture the global features hence transformer came into play that can learn the'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 75}, page_content='global features by using its attention mechanism. CNN and transformer lied in the encoder part whereas decoder\\ncontained up sampling, concatenating operations and convolutions. They evaluated their model on datasets from\\nthree different hospitals and measured the F1-score and compared their score with other state of the art models like\\nFrrnet[223], Parnet[224] and U-Net[139] and their model was outperforming all other models.\\n. : Page 36 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 76}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nTooth Root SegmentationAn accurate and precise segmentation of the boundaries present in the roots of the tooth\\nisnecessarytoattainaperfectrootcanaltherapyassessment.Lietal.[225]introducedaGroupTransformerNetwork\\n(GT U-Net) in order to achieve segmentation of root boundaries. Their model’s structure was similar to the U-Net but'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 76}, page_content='they used group of transformers in place of encoders and decoders. Also in order to incorporate the prior knowledge\\ntheyusedFourierDescriptorLoss.Theirmodelachievedanaccuracyof96.31%andf1scoreof84.58%outperforming\\nother state of the art models.\\n6.3.4. OCT or Fundus Images:\\nDrusen Segmentation It is very crucial to diagnose the AMD at an early stage in retinal OCT images via Drusen\\nSegmentation.InordertoachieveanaccuratesegmentationWangetal.[226]proposedamultiscaletransformerglobal'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 76}, page_content='attention network MsTGANet for the segmentation of drusen in retinal OCT images. Their model was composed of\\na U-shaped architecture containing an encoder and decoder. To collect the non-local features at different scales with\\nling term dependencies from multiple encoder layers, a novel multi scale transformer non local module is proposed\\nand used at encoder’s top. Another module, MsGCS was introduced to assist the model to join different semantic'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 76}, page_content='knowledgebetweenencoderanddecoder.TheyalsointroducedasemisupervisedversionofMsTGANet.Thisversion\\nwas comprised of pseudo-labeled data augmentation strategy. This model can used huge amount of un labeled data in\\norder to increase the performance on segmentation, upon experiments the DSC came out to be 0.8692 with a std of\\n0.0052 outperforming the other state of the art models. this model was trained on a smaller dataset, however it will be'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 76}, page_content='better to collect a larger set of data in order to see its efficiency. Also, different semi-supervised learning approaches\\ncan also be used to further improve its performance.\\nThe table 6 given below summarized the performance gain by the reviewed articles of the segmentation category.\\nTable 6: A list of datasets and performance measures adopted by researchers for Segmentation.\\nModality Publication Dataset Performance Measures\\nMRI\\nCheng et al.[210] BRaTS2020[107]\\nDice Score = 0.90'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 76}, page_content='Dice Score = 0.90\\nHausdorff Distance = 4.4\\nAUC(%) = 91.04\\nAccuracy(%) = 90\\nSensitivity(%) = 87.50\\nSpecificity(%) = 92.11\\nChartasis et al.[215]\\nERI[110] Dice Score = 0.82\\nCHAOS[111] Dice Score = 0.85\\nSun et al.[212]\\nMRBrainS[108] Dice Score = 0.83\\niSeg2017[106] Dice Score = 0.87\\nSinclair et al.[214] UKBB[109] Dice Score = 0.86\\nHausdorff Distance = 7.2\\nSagar et al.[211] BRaTS2019 Dice Score = 0.86\\n. : Page 37 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 77}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nHausdorff Distance = 7.1\\nGao et al.[216] M&MS2\\nDice Score = 0.86\\nHausdorff Distance = 9.6\\nX-ray Images\\nLu et al.[221]\\nOAI\\nIOU(%) = 97.32\\nHausdorff Distance = 6.0\\nJSRT[113]\\nIOU(%) = 94.75\\nHausdorff Distance = 12.1\\nLi et al. [225] DRIVE[112] Dice Score = 0.92\\nCT Scans La et al.[203] KiTS2019[114] Dice Score = 0.88\\nOCT/Fundus Images Wang et al.[226] USCD[115] Dice Score = 0.86\\n6.4. Clinical Report Generation'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 77}, page_content='6.4. Clinical Report Generation\\nIn this section, we briefly describe various transformer models to generate the medical reports and address the\\npreceding challenges associated with automatic clinical report generation.\\n6.4.1. Supervised Learning Based Approaches\\nSupervised learning refers to a type of learning algorithms that learn under the presence of a supervisor. An input\\nfromthetrainingsetispassedthroughthenetworkthentheoutputofthenetworkiscomparedtothedesiredoutputand'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 77}, page_content='learningweightsareupdatedaccordingly.Followingstudieshaveemployedsupervisedlearningintheirmethodologies.\\nIncorporatingGlobalLevelFeatures Globallevelfeaturesareextractedfromtheentiremedicalimagei.e.encoded\\nfeatures of both normal and disease regions in the image. Following studies have incorporated this notion into their\\nmethodologies. You et al. [144] proposed a transformer-based architecture, AlignTransformer. They resolved the data'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 77}, page_content='bias and long sequence modeling problems to generate a coherent medical report by delineating the normal and\\nabnormalregions.TheyusedResNet-50pre-trainedonImageNetandfine-tunedonCheXpertdatasettoextractvisual\\nfeatures. Furthermore, they fed the extracted visual features into the pre-trained multi-label classification netwrok to\\npredict the disease tags. Align hierarchical attention as an encoder aligned the disease tags and visual regions by'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 77}, page_content='learning the correlation and relationship between them. Moreover, they acquired the multi-grained disease-grounded\\nvisual features from the aligned disease tags and visual regions to alleviate the data bias problem. Multi-grained\\ntransformer as a decoder exploited the multi-grained disease grounded visual features to generate a proper medical\\nreport. In automatic evaluation, they compared their experimental results with the previous state-of-the-art models'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 77}, page_content='i.e. R2Gen, PPKED,and SentSAT +KG etc. and achieved competitive results on IU X-ray and MIMIC-CXR datasets.\\nIn human evaluation, the results of their model were far better than that of R2Gen model. Similarly, Amjoud et al.\\n[227] also proposed a transformer-based deep learning model for generating long and detailed reports of chest x-ray\\nimages. They used a pre-trained DenseNet-121 [82] instead of ResNet-50 [144] to avoid gradient vanishing problem'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 77}, page_content='and redundant feature maps. They suppressed the last classification layer of the pre-trained model to extract global\\nandregionalfeaturesfrommedicalimages.Afterthat,theextractedfeatureswerefedasinputintotheencodertomap\\nthem into a sequence of continuous representations. They modified the decoder of the vanilla transformer by adding a\\nrelational memory module to the normalization layer. Experiments demonstrated that their model generated detailed'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 77}, page_content='findings reports for IU chest x-rays test images and outperformed the state-of-the-art models for BLUE-1, BLUE-2,\\nand ROUGE metrics with 0.479, 0.359, and 0.380 scores respectively. However, the model could not perform well\\n. : Page 38 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 78}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nfor BLUE-3, BLUE-4, and METEOR metrics. Also, they used a small corpus for training, as a result, some sentences\\nwere unseen during inference which lead to the scattering problem.\\nIn another work, Pahwa et al. [143] leveraged the skip connections by proposing a transformer-based architecture\\nnamedMEDSKIPbymodifyingahigh-resolutionnetwork(HRNet).Theymodified(HRNet)[228]forvisualfeatures'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 78}, page_content='extraction by incorporating skip connections along with convolutional block attention modules (CBAM). First, they\\nextracted features representation from each down-sampled layer and after extracting crucial features using attention,\\nCBAMconcatenatedthem.CBAMconstitutedspatiallyandchannelattentionsub-modulesforinferringa1Dchannel\\nattention map and a 2D spatial attention map respectively. The proposed architecture also contained a memory-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 78}, page_content='driven transformer which constituted a standard encoder but the decoder contained a memory-driven conditional\\nnormalization layer to incorporate relational memory. The decoder facilitated the learning from patterns in reports\\nand recorded key information of the generated process. Extensive experiments on two publicly available datasets\\nPEIR GROSS and IU chest x-rays showed that their proposed model had given the state-of-the-art results for BLEU,\\nMETEOR, and ROGUE metrics.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 78}, page_content='METEOR, and ROGUE metrics.\\nIncorporating Global and Local Level FeaturesA medical image contains both normal and disease regions. To\\nencode the disease regions of the image, previous studies encoded the complete image which lead to the encoding of\\nirrelevant visual content which is adverse for radiology report generation. Some diseases have strong correlation and\\nfindingthosecorrelationisbeneficialforgeneratingreportforrarediseases.Variousstudiestriedtotakeadvantageof'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 78}, page_content='thisfeaturebyconstructingcorrelationmatrixintheencodingstagebydata-drivenmethodologiesorexpertknowledge\\nbut these studies failed to decode these correlations effectively while decoding.\\nTo address these problems, Jia et al. [229] leveraged the transformer-based architecture and proposed a few-shot\\nradiology report generation model, namely TransGen. In the encoding stage, they introduced a semantic-aware visual'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 78}, page_content='Learning(SVL)moduleinwhichtheyusedResNet101toidentifyandcapturethediseaseregionsofrarediseases.They\\ncaptured the disease regions from the image itself and the feature map generated at time step (t-1) by learning the two\\nmasksrespectivelytorefinethevisualrepresentationofrarediseases.Theyadoptedaweightedsumofthesetwomasks\\nattimestepttolearnthevisualrepresentationsefficientlybyincorporatingbothglobalandlocallevelinformation.For'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 78}, page_content='efficient decoding of encoded correlation among the diseases, the memory augmented semantic enhancement module\\nwasintroducedatthedecodingstage.Experimentsdemonstratedthattheirmodeloutperformedthestate-of-artmodels\\non the MIMIC-CXR dataset but could not perform well for the IU X-ray dataset.\\nSimilarly, Lee et al.[230] also incorporated both local and global level features by proposing Cross Encoder-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 78}, page_content='Decoder Transformer (CEDT) contained a Global-Local Visual Extractor. They used a convolution neural network\\n(e.g.ResNet101) as a global visual extractor to encode the complete radiology image into a sequence of patch features\\ntoaccuratelycapturethefeaturesatthegloballeveli.e.bonestructureorsizeoftheorgan.However,whileincorporating\\nglobal-levelfeatures,itwasdifficulttoencodetheexactlocationandthesizeofthelesionarea.Toaddressthisproblem,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 78}, page_content='theycroppedthediseaseregionsoftheimagewiththelastlayeroftheCNNusingtheattention-guidedmaskinference\\nprocess and after resizing to the same size as the image, used them as input to the local feature extractor to extract\\nthe local level features. Then, they concatenated the local visual features and global visual features and used them as\\ninput to the CEDT. The standard transformer uses only the last layer information but they [230] also used low-level'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 78}, page_content='features in addition to the high-level features by using the concept of [231]. They used multiple encoders to get the\\nall-level information from them and utilize the outputs of all encoders on each decoder using parallel multi-headed\\nattention.Theyaddedtheextractedfeaturesofeachencoderlayerwhichresultedinbettercaptioningthanthebaseline\\nmodelR2Gen.Furthermore,theyalsoemployedMCLNandRMforrecordingandutilizingtheimportantinformation.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 78}, page_content='ExtensiveexperimentsdemonstratedthattheirmodeloutperformedthebaselinemodelforBLEU-1,BLEU-2,BLEU-\\n3,METEOR,andROUGE-LonIUX-raydataset.Theyalsoperformedexperimentwithpre-trainedGLVEbutitcould\\nnot perform well for the BLEU-4 metric.\\n6.4.2. Reinforcement Learning Based Approach\\nPrevious studies [143, 144, 227, 229, 230] have used supervised learning approaches to generate medical reports.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 78}, page_content='Supervised learning approaches are prone to exposure bias problems in language modeling methods. To address this\\nproblem, Xiong et al. [232] proposed a novel hierarchical neural network architecture using reinforcement learning to\\ngenerate a long coherent medical report. They incorporated the self-critical reinforcement learning method into the\\ndetector, encoder, and captioning decoder. Previous studies used only top-down visual encoders, however, this was'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 78}, page_content='the first study that incorporated a bottom-up visual detector as well to extract semantic rich features from the medical\\n. : Page 39 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 79}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nimages.Forthispurpose,firstlytheyusedDenseNet-121,pre-trainedonchestX-ray14dataset,todetecttheregionof\\ninterest(ROI)proposalsusingabottom-upattentionmechanism.TheregiondetectoroutputtedasetofROIproposals\\nalongwithclassifiedclassesandsomeassociatedattributes.Secondly,theyusedtop-downtransformervisualencoder'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 79}, page_content='to extract further pixel-wise visual information from proposed ROI using pooling operations. Lastly, the transformer\\ncaptioningmoduleusedimprovedROIproposalsasinputfromthetransformervisualencoderandgenerateddescriptive\\nsentences for each proposed ROI by calculating reward directly using the CIDEr metric. Their proposed architecture\\noutperformedthestate-of-the-artmethodsfortheCIDErevaluationmetricontheIUX-raydatasetbutfortheBELU-1'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 79}, page_content='metric, their model could not perform state-of-the-art. Their model over-fitted as they used only the findings portion\\nof the generated medical report. This problem can be resolved using a larger labelled dataset.\\nThetable7givenbelowsummarizedtheperformancegainbythereviewedarticlesoftheclinicalreportgeneration.\\nTable 7: A list of datasets and performance measures adopted by researchers for Clinical Report Generation.\\nModality Publication Dataset Performance Measures'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 79}, page_content='Modality Publication Dataset Performance Measures\\nX-ray Images You et al. [144]\\nIU X-ray [116]\\nBLEU-1 = 0.484\\nBLEU-2 = 0.313\\nBLEU-3 = 0.225\\nBLEU-4=0.173\\nMETERO=0.204\\nROUGE-L=0.379\\nMIMIC-CXR [117]\\nBLEU-1=0.378\\nBLEU-2=0.235\\nBLEU-3=0.156\\nBLEU-4=0.112\\nMETERO=0.158\\nROUGE-L = 0.283\\nBLEU-1=0.479\\nX-ray Images\\nAmjoud et al.[227] IU X-ray [116] BLEU-2=0.359\\nROUGE-L = 0.380\\n. : Page 40 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 80}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nX-rays\\nPahwa et al. [143]\\nIU X-ray [116]\\nBLEU-1= 0.467\\nBLEU-2=0.297\\nBLEU-3=0.214\\nBLEU-4=0.162\\nMETERO=0.187\\nROUGE-L=0.355\\nPEIR GROSS [118]\\nBLEU-1= 0.399\\nBLEU-2=0.278\\nBLEU-3=0.209\\nBLEU-4=0.148\\nMETERO=0.176\\nROUGE-L=0.414\\nJia et al.[229] IU X-ray [116]\\nBLEU-1=0.461\\nBLEU-2=0.285\\nBLEU-3=0.196\\nBLEU-4=0.145\\nROUGE-L= 0.367\\nKA (%) = 0.367\\nMIMIC-CXR [117]\\nBLEU-1=0.368\\nBLEU-2=0.243\\nBLEU-3=0.178\\n. : Page 41 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 81}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nBLEU-4=0.138\\nROUGE-L= 0.338\\nBLEU-1=0.5064\\nIU X-ray [116]\\nBLEU-2=0.3195\\nBLEU-3=0.2201\\nLee et al.[230] BLEU-4=0.1924\\nROUGE-L= 0.3802\\nX-ray Images\\nXiong et al. [232] X-ray [116]\\nBLEU-1=0.350\\nBLEU-2=0.234\\nBLEU-3=0.143\\nBLEU-4=0.096\\nCIDEr = 0.323\\n6.5. Miscellaneous ViT Applications in Medical Imaging\\nTranformer-based architecture has also played a vital role in other applications of medical field i.e. in image'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 81}, page_content='synthesis, denoising the low dose computed tomography, and positron emission tomography images, enhancing the\\nresolution of medical images etc.\\n6.5.1. functional Magnetic Resonance Imaging (fMRI) Scans:\\nVisualizing Regenerated Neural Visual ContentIn the past decades, few studies have been conducted to decode\\nthe human brain neural activities into natural language sentences. The main purpose of decoding brain neural activity'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 81}, page_content='is basically to know the human brain’s perception of textual or visual content. In the past, most deep learning studies\\nfocused on different task specific decodings, i.e. detection, classification, recognition etc., using functional magnetic\\nresonance imaging (fMRI) data. With the advancement in technology, several research has been done in language\\ndecoding to decode the human brain semantics evoked by linguistic stimuli into natural language words or sentences.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 81}, page_content='Inspiringfromthesetasks,Zhangetal.[59]proposedahybridlanguagedecodingmodel:CNN-Transformertodecode\\nthe visual stimuli evoked at multi-times by natural images into descriptive sentences. They exploited the concept of\\nneuralmachinetranslation(NMT)[17]butthedifferencewasinsourcesequencei.e.naturalimagesinNMTbutvisual\\nneuralactivitiesin[59].Toachievethistask,firstlytheyextractedmeaningfulsemanticlow-dimensionalfeaturesfrom'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 81}, page_content='high-dimensional visual neural activities (low-level raw fMRI data) using two-layer one dimensional CNN. Secondly,\\nthe encoder part of the transformer encoded the semantic features into multi-level abstract representation. Lastly, the\\n. : Page 42 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 82}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\ndecoder of the transformer decoded the multi-level representation into descriptive natural language sentences. They\\ncomparedtheirmodelwithotherdecodingmodelsandachievedstate-of-the-artresultsforBLEU,CIDEr,andROUGE\\nmetrics with 0.17, 0.66, and 0.18 scores respectively. In future, this transformer-based brain decoding technology will'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 82}, page_content='beusefulforthepeoplewhoareunabletotransmittheirvisualperceptionintospeechandwillalsobeabreakthrough\\nfor neuro-scientists in understanding and decoding the neural activities of human brain.\\n6.5.2. PET-CT Scans:\\nMedical Image EnhancementComputed tomography is a non-invasive imaging technique for medical diagnosis.\\nSince high exposure to X-rays radiation is deadly for humans and has become the main concern for medical'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 82}, page_content='practitioners. To lessen this effect of X-rays radiation, it is used in less quantity in CT scans but it poses some serious\\nproblems, i.e. less contrast, sharp features, corners, edges, and stronger noise, which affects the quality of CT scans.\\nAlthough low-dose computed tomography (LDCT) is mainstream in clinical applications, but the posed problems\\ncausehindranceinaneffectiveclinicaldiagnosis.Manytraditionalmethods(iterativemethods)andconvolution-based'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 82}, page_content='deep learning approaches were employed to acquire high quality LDCT images by deblurring and suppressing the\\nartifacts.Thehigh-frequencysub-bandofimagesarenoisyareaswhilethelow-frequencysub-bandarenoise-freeareas\\ncontainingmainimagecontent.Sinceconvolution-basedmethodsarelimitedtoextractingfeaturesfromthelocalareas\\nof images due to limited receptive fields. Therefore, transformers came into the scientific field and revolutionized the'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 82}, page_content='world with their facts of capturing long-range dependencies between image regions.\\nKeepinginaccountalltheseobservations,Zhangetal.[233]proposedatransformer-basedarchitecturetodenoise\\nthe LDCT images by decomposing them into high frequency (HF) and low-frequency parts. Hence, the noise was\\nonly retained in the HF part and it also contained a plethora of image textures. To ensure the relationship between'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 82}, page_content='HF and LF parts of the image, they denoised the noisy HF part with the assistance of the latent texture of the LF\\npart. For this purpose, they employed CNNs to extract corresponding texture and content features from the LF image\\npart.Furthermore,theyacquiredthehigh-levelfeaturesfromthetransformerusingthetexturefeaturesfromthenoisy\\nLF and embeddings from the HF part. They used a modified transformer with three encoders and three decoders.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 82}, page_content='Finally, they reconstructed the high-quality LDCT image piecewise by combining these high-level features with the\\ncontent features from the LF part of the image. ent features from the LF part of the image. Extensive experiments\\ndemonstrated that their model outperformed all the baseline methods achieving 93.7% for VIT metrics, improved\\nstructuresimilarityby12.3%,androotmeansquareerrorloweredby40.5%onMayolow-dosecomputedtomography'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 82}, page_content='images dataset. Since convolution-based methods cannot capture global contextual information, Wang et al. [6] first\\ntime proposed a convolution-free token-to-token vision transformer-based dilation network to denoise the LDCT\\nimages.Theycapturedthenoisefrominputmedicalimagesbylearningdeepfeatures,afterthat,theyremovethenoisy\\nestimated residual images in order to clean them. Firstly, they used tokenization block to tokenize the feature map'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 82}, page_content='patches into tokens. Secondly, they fed those tokens into transformer block, further, for enhancing the tokenization,\\nthey applied tokenization in cascaded form in token-to-token block. They further enlarged the receptive field and\\nrefined the contextual information using dilation in tokenization procedure. They performed dilation using reshaping,\\nsoft split and cyclic shift to enhance the context. They compared their model with other state-of-the-art models and'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 82}, page_content='their model outperformed for SSIM and RMSE metrics with 0.9144 and 8.7681 scores respectively, in denoising the\\nimages. Without down-scaling tokenization of the image can be enhanced.\\nPET/MRIcanconcurrentlyprovideanatomicalandmorphologicalimaginginformationthataidsinclinicaldisease\\ndiagnosis. PET acquires metabolic imaging information with the help of radio-tracers while MRI uses magnetic field'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 82}, page_content='gradientsandradiowavestoacquireimagesofsoftbodytissues.Although,theseimagingmodalitieshaveapplications\\nindiseasediagnosis,i.e.cancer,tumor,andbraindiseases,butalsoposesomeseriousconcerns.Sincetimerequirement\\nfor PET imaging acquisition is high and as a result, patient discomfort can affect the image quality i.e. low contrast-\\nto-noise ratio. The Information from MRI can assist in denoising the PET images using registration approach. Many'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 82}, page_content='traditional deep learning and computer vision methods proposed to enhance the PET image quality using MRI but\\nduetodiscrepancyinmodalitiestheycouldnotextractcontextualandspatialinformationefficiently.Toaddressthese\\nproblems. Zhang et al. [234] proposed a spatial adaptive and transformer fusion network (STFNet) for denoising low\\ncount PET with MRI. They adapted dual path using the spatial-adaptive block to extract features. For the fusion of'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 82}, page_content='high-levelfeatures,theymodifiedthetraditionaltransformerencoderandincorporatedglobalattentiontoformapixel-\\nto-pixelrelationshipbetweenMRIandPET.ThefusedfeaturemapwasusedasinputtothedecoderforPETdenoising.\\nTheir model obtained promising results for on RMSE,PSNR,SSIM, and PCC metrics.\\n. : Page 43 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 83}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nSimilarly, Luo et al. [235] proposed 3D Transformer-GAN to build a standard dose PET image from the low-\\ndose PET image. They leveraged the CNN-Transformer architecture to incorporate both global and local information.\\nCNN-based-Encoder extracted enriched spatial information from the input medical image. Moreover, the transformer'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 83}, page_content='captured the long-range dependency from the extracted features from the CNN-based-Encoder. The learned repre-\\nsentation from the transformer are incorporated into the CNN-based-Decoder to restore them and reconstructed the\\nstandard PET image. Extensive experiments demonstrated that their model outperformed the state-of-the-art on real\\nhuman brain dataset.\\n6.5.3. Magnetic Resonance Imaging (MRI) Scans:'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 83}, page_content='6.5.3. Magnetic Resonance Imaging (MRI) Scans:\\nMedical Image ReconstructionMRI is a prevalent non-invasive imaging technique but its acquisition process is\\nslow.Consequently,thereisaneedtodevelopacceleratedMRImethods.Simultaneously,severalstudiesondeepneural\\nnetworks have been conducted to develop state-of-the-art methods for accelerated MRI. Therefore, Korkmaz et al. [3]\\naccelerated MRI by reconstructing full-sampled MRI images using unsupervised learning incorporating deep image'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 83}, page_content='prior framework to alleviate the problem of under-sampled acquisition. They proposed generative vision transformer\\nbased unsupervised MRI reconstruction architecture to increase the receptive field. Firstly, they performed generative\\nnon-linear mapping over latent and noisy space to improve the invertibility of the model. Secondly, they used cross-\\nattention to improve the context, i.e. both global and local context, of image and latent features. They did not use'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 83}, page_content='anypre-trainedmodel.Lastly,theyperformedinferenceoneachindividualsubjecttoincreasethegeneralization.They\\nperformedextensiveexperimentsonacceleratedmulti-contrastbrainMRIdatasetandoutperformedtheconvolutional-\\nbased generative models for PSNR and SSIM metrics.\\nWang et al. [236] proposed a super-resolution approach to reconstruct the high resolution MRI scans from low\\nresolution scans. They proposed adjacent slices feature transformer (ASFT) network. Firstly, they incorporated extra'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 83}, page_content='slices in the consecutive in-plane slices of an-isotropic 3D MRI images. Secondly, to harness the similarity between\\ntheconsecutiveslicestheyintroducedamulti-branchfeaturestransformationandextraction(MFTE)block.Thirdly,to\\nenrichthetargethighresolutionsliceswiththeinformationfromthelowresolutionreferenceslicestheyfilteredoutthe\\nuselessinformationusingMFTEblock.Moreover,theyusedspatialadaptiveblocktorefinethefeaturesspatially.They'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 83}, page_content='used channel attention to incorporate the multi-scale features and consequently, they enhanced the super resolution.\\nTheir model achieved the state-of-the-art performance for super-resolution task.\\nMedical Image Synthesis Tissue morphology information acquired from multimodal medical images play an\\nimportant role in the clinical practice. However, it is not commonly used because of the expensive capturing of these'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 83}, page_content='information. Generative models such as generative adversarial network (GAN) are in practice to artificially synthesis\\ntheseimages.GANisaCNNbasedarchitecturethatshowslocalitybiasandspatialinvarianceacrossallthepositions.\\nHu etal. [237] introduced a transformerbased double-scale deep learningarchitecture for cross-modal medicalimage\\nsynthesis to incorporate log-range dependencies Double-scale GAN showed efficient performance on benchmark IXI\\nMRI dataset.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 83}, page_content='MRI dataset.\\nMedicalImageRegistration Tissuedeformationinahighlydeformableanatomyisestimatedusingimageregistra-\\ntion.Diffeomorphicregistrationisoneoftheimageregistrationtechniquesthatpreservestheinvertibleandonetoone\\nmappingbetweenimages.Currentdeeplearningtechniqueslacktheabilitytohandlelongrangerelevancethuslimiting\\nthe meaningful contextual correspondence between images. In the paper [238] a dual transformer network (DTN)'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 83}, page_content='modelisproposedforthediffeomorphicregistrationofMRimages.DTN,usingself-attentionmechanisms,modelthe\\nrelevance from both the separate and concatenated images embeddings, which facilitate contextual correspondence\\nbetween anatomies. DTN consists of learnable embedding module, relevance learning module and registration field\\ninferencemodule.Diffeomorphicregistrationfieldisestimatedusingmovingfixedandmovingimagesforone-to-one'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 83}, page_content='mapping. DTN has two branches to learn the relevance based on the embeddings of separate one-channel images\\nand concatenated two-channel images. First the Low-level image feature for the concatenated and separate images are\\nextracted using CNN. Second, the image features, converted into sequences, are fed to DTN for feature enhancement,\\nbasedonglobalcorrespondence.Concatenatedfeaturesformbothbranchesarethenusedtoinferthevelocityfieldand'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 83}, page_content='registration filed. The deformation field is represented as the exponential of the velocity which ensure the invertible\\nmapping. Metric space is used to optimize the proposed DTN in an unsupervised manner.\\n. : Page 44 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 84}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\n6.5.4. Endoscopy Images\\nMedicalImage Reconstruction Endoscopeis a minimallyinvasive medicalimaging modality.It assistsin medical\\ndiagnoses by acquiring accurate images of the internal organs of a patient’s body. However, the small imaging sensor\\nin the endoscope causes problems in acquiring the high magnified blood vessels images. Many traditional methods'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 84}, page_content='incorporatinginterpolatedmethodsanddeeplearninghaveapplicationsinreconstructionofsuper-resolutionofsingle\\nimages. Mainly convolution-based deep learning methods are there in reconstructing high-resolution images which\\nhave problems in capturing the global context. Consequently, Gu et al. [239] leveraged transformers and proposed\\nhybrid architecture with the convolutional neural network to enhance the texture of blood vessels. Firstly, CNN'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 84}, page_content='extracted the low-level features from low-resolution (LR) images and the transformer sampled the LR image into\\nthreedifferentsamplestoextracttexturefromtheimage.Secondly,similaritywasexaminedbetweendifferentfeatures\\nextracted from transformer-based extractor and CNN-based extractor. Thirdly, they employed a texture migration\\nmethod to interchange the information between multi-scale features extracted from the transformer and CNN to'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 84}, page_content='synthesizetheimage.Lastly,sub-pixelconvolutionoperationwasperformedonmigratedbasicimagestosynthesizea\\nhigh-resolution.TheirmodelacquiredpromisingqualitativeandquantitativeresultsthantheCNNbasedsingleimage\\nsuper-resolution methods.\\n6.5.5. Fluorescence Microscopy\\nQuantitative Characterization of Anatomical Structure using Combination of Markers in Bone Marrow\\nVasculatureandFetalLiverTissues Fluorescencemicroscopyisavariantoftraditionalmicroscopy,whichusesa'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 84}, page_content='higherintensitylightsourcethatexcitesafluorescentspeciesinasampleofinterest.Itisusedfordifferentpurposessuch\\nasdetailedexaminationofanatomicallandmarks,cells,andcellularnetworks.AlvaroGomarizetal.[240]proposeda\\nMarkerSamplingandExcitenetworktoexploitthepotentialofattention-basednetworkonthefluorescencemicroscopy\\ndatasets which are underexplored by the deep learning. The capability of the network is tested by the quantitative'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 84}, page_content='characterization in various datasets of microvessels in fetal liver tissues and bone marrow vasculature in 3D confocal\\nmicroscopydatasets.ProposedmodelgivesaconvincingperformancewithF1Scoreof91.2%forsinusoidsand71.2%\\nfor arteries in the liver vasculature dataset.\\nDenoising of Celullar Microscopy Images for Assisted Augmented MicroscopyDeep learning has greatly\\nassisted augmented microscopy that enable high-quality microscopy images with using costly microscopy apparatus.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 84}, page_content='Zhengyang Wang et al. [241] proposed global voxel transformer networks (GVTNets) that uses attention operators\\nto address the limitations in already existing U-Net based neural. Instead of local operator that lack dynamic non-\\nlocal information aggregation they used attention operators that allow global receptive field during prediction. They\\nmeasured the performance of the model on three existing datasets for three different augmented microscopy tasks.\\n6.5.6. Histopathology Images'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 84}, page_content='6.5.6. Histopathology Images\\nWhole Slide Images contain rich content about the anatomical and morphological characteristics. To better depict\\ntheimagecontent,pathologistsfrequentlyexaminethetagsassociatedwiththeseimages.Sincethistaggingprocessis\\nlabour-intensive, consequently Li et al. [242] first time proposed a Patch Transformer based architecture to automate\\nthe multi-tagging whole slide images process. They incorporated attention mechanism to extract global level features'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 84}, page_content='from the patches of images. They employed multi-tag attention module to build tag on the basis of weight. Extensive\\nexperiments demonstrated that their model outperformed on 4920 WSI for macro and micro F1 metric.\\n6.5.7. OCT or Fundus Images\\nDiabetes damages the retina and causes diabetic retinopathy. This disease can lead to vision loss, therefore, early\\ndetectioniscrucial.Variousdeeplearningapproacheshaveautomatedtherecognitionofdiabeticretinopathygrading.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 84}, page_content='However, Wu et al. [158] proposed vision transformer based architecture to assist the ophthalmologist in recognising\\nthe diabetic retinopathy grade. They divided the fundus images into patches, flatten them to generate sequence, and\\nthenconvertedthemintolinearandpositionalembeddings.Togeneratethefinalrepresentations,theyfedthepositional\\nembeddingsintomulti-headattentionlayers.TheirmodeloutperformedtheCNN-basedarchitecturewithanaccuracy\\nof 91.4%.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 84}, page_content='of 91.4%.\\nThetable8givenbelowsummarizedtheperformancegainbythereviewedarticlesofthemiscellaneouscategory.\\n. : Page 45 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 85}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nTable 8: A list of datasets and performance measures adopted by researchers for Miscellaneous Application in ViT\\nModality Publication Dataset Performance Measures\\nCT Scans\\nWang et al. [6]\\nNIH-AAPM-Mayo Clinic\\nLDCT Grand Challenge\\n[119]\\nRMSE = 8.7681\\nSSIM = 0.9144\\nRMSE = 21.199+\\n− 2.054\\nZhang et al. [233]\\nNIH-AAPM-Mayo\\nClinic LDCT Grand\\nChallenge[119]\\nSSIM = 0.933+\\n− 0.012\\nVIF = 0.144+\\n− 0.025\\nPET Scans'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 85}, page_content='− 0.012\\nVIF = 0.144+\\n− 0.025\\nPET Scans\\nZhang et al.[234] uPMR790\\nRMSE = 0.0447\\nPSNR (db) = 27.5321\\nSSIM = 0.9291\\nPCC = 0.9899\\nLuo et al. [235]\\nPSNR = 24.818\\nClinical dataset which in-\\ncludes eight normal control\\n(NC) subjects.\\nSSIM = 0.986\\nNMSE = 0.0212\\nPSNR = 25.249\\nClinical dataset which in-\\ncludes eight mild cognitive\\nimpairment (MCI) subjects.\\nSSIM = 0.987\\nNMSE = 0.0231\\nEndoscopy Images\\nGu et al. [239]\\nDIVerse 2K resolution high\\nquality (DIV2K) images\\ndataset [121]\\nSet5 PSNR (db) = 31.94'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 85}, page_content='dataset [121]\\nSet5 PSNR (db) = 31.94\\n. : Page 46 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 86}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nSSIM = 0.8935\\nSet14 PSNR (db) = 28.11\\nSSIM = 0.7842\\nB100 PSNR (db) = 27.54\\nSSIM = 0.7464\\nUrban100 PSNR (db) = 25.87\\nSSIM = 0.7844\\nManga109 PSNR (db) = 30.09\\nSSIM = 0.9077\\nMRI Scans\\nWang et al. [236]\\nKirby21 dataset (KKI01 to\\nKKI05) [120] PSNR = 40.19+\\n− 2.04\\nSSIM= 0.9882+\\n− 0.0034\\nDice = 0.91\\nTang et al. [238]\\nT1-weighted images (of size\\n182×218×182) of 102 drug-\\naddicts and 10 healthy vol-\\nunteers.\\nHD = 2.68'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 86}, page_content='addicts and 10 healthy vol-\\nunteers.\\nHD = 2.68\\nASD = 0.59\\nIXI dataset\\nT1, R=4 PSNR = 32.55+\\n− 1.77\\nSSIM (%) = 94.58+\\n− 0.82\\nKokmaz et al. [3] T1, R=8 PSNR = 30.28+\\n− 1.68\\nSSIM (%) = 91.64+\\n− 1.42\\nT2, R=4 PSNR = 32.71+\\n− 0.73\\nSSIM (%) = 87.66+\\n− 1.67\\nT2, R=8 PSNR = 29.90+\\n− 0.70\\nSSIM (%) = 84.03+\\n− 1.89\\n. : Page 47 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 87}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nHu et al. [237] IXI dataset\\nT1 PSNR = 34.91+\\n− 1.00\\nSSIM = 0.895+\\n− 0.07\\nT2 PSNR = 35.34+\\n− 0.95\\nSSIM (%) = 0.895+\\n− 0.07\\nfMRI Scans\\nCIDEr = 0.741\\nZhang et al. [59]\\nfMRI experiments(the vi-\\nsual stimulus consisted of\\n2750 natural images from\\nImageNet data)\\nROUGE-L= 0.2009\\nBLEU= 0.186\\nFluorescence\\nMicroscopy Gomariz et al. [240] 3D confocal microscopy\\ndatasets F1 Score = 0.912+\\n− 3.9'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 87}, page_content='datasets F1 Score = 0.912+\\n− 3.9\\nFundus Images Wu et al. [158] Diabetic Retinopathy Detec-\\ntion dataset Accuracy (%) = 91.4\\nSpecificity = 0.977\\nPrecision = 0.928\\nSensitivity = 0.926\\nQuadratic weighted kappa\\nscore = 0.935\\nAUC = 0.986\\nWSI Li et al. [242]\\n4,920 WSIs provided by a\\nhistopathology service com-\\npany\\nMcro F1 = 0.910\\nMicro F1 = 0.944\\n7. Discussion and Conclusion\\nVision Transformers (ViT) are now one of the hottest topics in the discipline of computer vision because of its'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 87}, page_content='exemplary performance and tremendous potential compared with CNNs. Although CNNs are matured enough for the\\ndevelopment of applications that can ensure an efficient and accurate diagnosis. However, in the medical field, where\\nan inaccurate output might endanger lives, the concept of attention in vision transformers has paved its way for more\\nprecise outcomes. Since ViT models assess the global context of the image along with the interpretability through\\n. : Page 48 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 88}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nthe attention module, their performance is more precise than CNNs. A variety of approaches have been proposed in\\nrecent years, as outlined and summarized in this review, to explore and utilize the competency of vision transformers.\\nThese approaches showed excellent performance on a wide range of visual recognition tasks, including classification,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 88}, page_content='lesionsdetection,anatomicalstructuresegmentation,andclinicalreportgeneration.Nevertheless,therealpotentialof\\ntransformersforcomputervisionhasyettobefullyexplored,whichmeansthatsignificantchallengesarestillthereto\\nbe resolved. The following section envisages these challenges as well as provides insights on future prospects.\\n7.1. Current Trends and Open Challenges\\nAlthoughseveraltransformer-basedmodelshavebeenproposedbyresearcherstoaddressvisualrecognitiontasks,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 88}, page_content='thesemethodsaresimplytheinitialstepsinthisfieldandthereisstillconsiderableroomforimprovement.Forexample,\\nthe transformer architecture in ViT [19], is based on the standard transformer for NLP [16], although an enhanced\\nversionparticularlytailoredforCVneedstobeexplored.Furthermore,itisnecessarytoemploytransformerstoother\\nmedical domains such as orthodotics, medical report generation, and phenotype extraction etc., to discover the power\\nof vision transformers.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 88}, page_content='of vision transformers.\\nIn this multidisciplinary study, we provide a comprehensive view of how vision transformers employed medical\\nimaging modalities in visual recognition tasks for assisted diagnosis. We have reviewed the research articles selected\\nfrom top tier journals and conferences in which researchers have proposed excellent frameworks that employed\\nvision transformers to accelerate the efficiency and performance of already proposed CNN based models. The'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 88}, page_content='Figure 5 is a clear evidence that vision transformers are widely employed in classification and segmentation related\\nvisual recognition tasks whereas their significance in registration related tasks have not been greatly explored. The\\nmiscellaneous studies in this review include the work on enhancing the resolutions for better outcomes, denoising of\\nCT scans with a low dosage of X-ray radiation, image registration etc. Nevertheless, the undertaken studies are based'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 88}, page_content='ondiscreteapplications,therefore,theyarenotcomparable.Thefieldofresearchisquiteopenatthisstageasbaselines\\nareavailable.Then,reportgenerationcoveredonly7%whichindicatesanareatoresearchuponasitsapplicationscan\\nassist both doctors and patients.\\nSimilarly, in the imaging modalities, it can be observed in Figure 6 that 61% of the papers found, are related to\\nX-rays,andCTscans.Thevisiontransformermodelwasfirstproposedintheyear2020,andmostofthecorresponding'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 88}, page_content='literature available was related to the diagnosis of coronavirus as it was prevailing in the same year. The transformer\\nmodels achieved admirable results in regards to COVID-19, however, other domains especially digital pathology,\\nretinal, breast, etc., require attention as not much literature is available. Also, the literature related to COVID-19 is\\neither classifying or detecting the virus, thus, no studies were found that were performing segmentation. Even though'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 88}, page_content='MRI produces images with better resolution than CT scans, it covered only 13% of the total modalities. The papers\\naremostlyusedforimagesegmentationandhaveproducedrespectableoutcomes.Theseoutcomesarebasedonafew\\nstudies targeting distinct diseases, hence, there is much more to explore.\\nInaddition,itwasobservedinclinicalreportgeneration’sreviewthatamongvariousmodalities,thestudiesrelated'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 88}, page_content='to x-ray images were only available. Thus, it means that the data required for report generation is only present in\\ncorrespondence to x-ray images. As X-rays are a cost-efficient method for examining various parts of the body, it\\nis comprehensible that due to more reports and X-ray images available, it was possible to collect enough data for\\nautomaticreportgeneration.Sinceitassistsdoctorsinwritingmedicalreports,thecollectionofcorrespondingimages'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 88}, page_content='and text should be collected to implement automated report generations in other domains.\\nNext, one of the major limitations regarding medical datasets is that not enough data is available to train a\\ntransformer model. These models require data in huge amounts to perform well and adapt to generalizability. Another\\nobservation is the non-availability of a benchmark as observed in the Figure 16. Since almost all the studies are based'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 88}, page_content='on different datasets, the performances of the models are not comparable. Even if the datasets are the same in some\\nstudies, they are either used for different purposes or are combined with other datasets to form one large repository.\\nNow, as long as there is no standard dataset for experimenting with different proposed models, research can not take\\nits step forwards towards development using vision transformers.\\n. : Page 49 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 89}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nFigure 16:Long-tail graphical representation of datasets employed in literature.\\n7.2. Future Prospects:\\nIn order to propel the development of vision transformers in medical domain, we propose various significant\\nfuture directions. One direction is that, several papers in this study have done a comparative analysis of CNNs and'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 89}, page_content='ViT, thus, it can be inferred that transformer models have given better results in terms of efficiency, computational\\ncost, and accuracy. Since these models have achieve such outcomes where researchers are talking about CNN’s being\\nreplaceable,visiontransformersrequiremoreresearchandimplementationastheyareunravelingapathtowardsmore\\nresource-efficient models.\\nThe paper demonstrates the significance of using vision transformers on medical images. The future directions of'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 89}, page_content='research involve working with more heterogeneous data sets, and more extensive comparisons with other models to\\ngive validity to the proposed transformer models. Next, the studies, may they be related to any category or modality,\\nare using different datasets, hence there cannot be a comparative analysis in terms of the proposed vision transformer\\nmodels. Therefore, we should work on creating a benchmark dataset.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 89}, page_content='Similarly, new knowledge emerging in cancer biology and deep learning enabled us to step into this rapidly evolving\\ndomain. Genomic profiling is prevalent; however, it is crucial to correlate cancer genomic and phenotypic data to\\nfully understand cancer behavior. Cancer phenotype information includes tumor morphology (e.g., histopathologic\\ndiagnosis),laboratoryresults(e.g.,geneamplificationstatus),specifictumorbehaviors(e.g.,metastasis),andresponse'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 89}, page_content='totreatment(e.g.,theeffectofachemotherapeuticagentontumorvolume).Visiontransformerscanalsobeappliedto\\nmedical images in order extract phenotypic information in order to better diagnose cancer related disorders. Another\\ndirectionisrelatedtothelimitationregardingcoronaviruscasesbeingdetectedandclassified,butnotbeingsegmented.\\nThe usage of segmenting the virus can help determine the rate of spread of the virus, that is why it should be worked'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 89}, page_content='uponinthefuture.Furthermore,theliteratureforclinicalreportgenerationisrelatedtoX-raysimages,andasdiscussed\\nabovemostofthevisiontransformersusedforX-rayimagesarebeingusedtodetectcoronavirus.Asperourknowledge,\\nnone of the literature is related to medical report generation for coronavirus, and considering the work that has been\\ndone in this regard, research in medical image captioning can propel towards the application side and assist the'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 89}, page_content='practitioners. Overall, since it covers only 7 % of the total tasks in this study, there is more to explore in terms of\\nX-raysandotherimagingmodalities.Althoughvisiontransformershaveachievedanothermilestoneforimprovedand\\naccurate diagnosis in the medical domain, there is still room for improvement in terms of resource efficiency. In other\\nwords, we still have to discover the undiscovered.\\n. : Page 50 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 90}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\n7.3. Conclusion\\nThepaperenvisagesthesummaryanalysisofvisiontransformermodelsusedinthemedicaldomain.Thisstudywill\\nserve researchers from multidisciplinary backgrounds. The visual recognition tasks considered in this paper include;\\nclassification,detection,segmentation,andclinicalreportgenerations.Otherthanthat,somemiscellaneoustaskswere'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 90}, page_content='also included such as image registration, image reconstruction, etc. Furthermore, medical imaging modalities such as\\nX-rays,CTScans,MRIs,etc.wereusedasinputtorecognizemedicalimagingthroughvariousmodels.Ourgoalisto\\npresentthestudyinawaythatapprehendsthestatusandfuturedirectionsofvisiontransformers.Hence,westructured\\nthe information of datasets, categories, modalities, and their results in a tabular form which will assist researchers to\\nmove forward in the medical field conveniently.\\nReferences'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 90}, page_content='References\\n[1] Isabella Castiglioni, Leonardo Rundo, Marina Codari, Giovanni Di Leo, Christian Salvatore, Matteo Interlenghi, Francesca Gallivanone,\\nAndrea Cozzi, Natascha Claudia D’Amico, and Francesco Sardanelli. Ai applications to medical images: From machine learning to deep\\nlearning. Physica Medica, 83:9–24, 2021.\\n[2] S Kevin Zhou, Hayit Greenspan, Christos Davatzikos, James S Duncan, Bram Van Ginneken, Anant Madabhushi, Jerry L Prince, Daniel'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 90}, page_content='Rueckert, and Ronald M Summers. A review of deep learning in medical imaging: Imaging traits, technology trends, case studies with\\nprogress highlights, and future promises.Proceedings of the IEEE, 2021.\\n[3] YilmazKorkmaz,MahmutYurt,SalmanUlHassanDar,MuzafferÖzbey,andTolgaCukur. Deepmrireconstructionwithgenerativevision\\ntransformers. InInternational Workshop on Machine Learning for Medical Image Reconstruction, pages 54–64. Springer, 2021.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 90}, page_content='[4] James A Diao, Jason K Wang, Wan Fung Chui, Victoria Mountain, Sai Chowdary Gullapally, Ramprakash Srinivasan, Richard N Mitchell,\\nBenjamin Glass, Sara Hoffman, Sudha K Rao, et al. Human-interpretable image features derived from densely mapped cancer pathology\\nslides predict diverse molecular phenotypes.Nature communications, 12(1):1–15, 2021.\\n[5] GeorgiosAKaissis,MarcusRMakowski,DanielRückert,andRickmerFBraren. Secure,privacy-preservingandfederatedmachinelearning'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 90}, page_content='in medical imaging.Nature Machine Intelligence, 2(6):305–311, 2020.\\n[6] Dayang Wang, Zhan Wu, and Hengyong Yu. Ted-net: Convolution-free t2t vision transformer-based encoder-decoder dilation network for\\nlow-dose ct denoising. InInternational Workshop on Machine Learning in Medical Imaging, pages 416–425. Springer, 2021.\\n[7] Justin M Johnson and Taghi M Khoshgoftaar. Survey on deep learning with class imbalance.Journal of Big Data, 6(1):1–54, 2019.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 90}, page_content='[8] Nima Tajbakhsh, Laura Jeyaseelan, Qian Li, Jeffrey N Chiang, Zhihao Wu, and Xiaowei Ding. Embracing imperfect datasets: A review of\\ndeep learning solutions for medical image segmentation.Medical Image Analysis, 63:101693, 2020.\\n[9] Dandi Yang, Cristhian Martinez, Lara Visuña, Hardev Khandhar, Chintan Bhatt, and Jesus Carretero. Detection and analysis of covid-19 in\\nmedical images using deep learning techniques.Scientific Reports, 11(1):1–13, 2021.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 90}, page_content='[10] Chen Li, Hao Chen, Xiaoyan Li, Ning Xu, Zhijie Hu, Dan Xue, Shouliang Qi, He Ma, Le Zhang, and Hongzan Sun. A review for cervical\\nhistopathology image analysis using machine vision approaches.Artificial Intelligence Review, 53(7):4821–4862, 2020.\\n[11] Chaoran Yu and Ernest Johann Helwig. The role of ai technology in prediction, diagnosis and treatment of colorectal cancer.Artificial\\nIntelligence Review, 55(1):323–343, 2022.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 90}, page_content='Intelligence Review, 55(1):323–343, 2022.\\n[12] Ghulam Murtaza, Liyana Shuib, Ainuddin Wahid Abdul Wahab, Ghulam Mujtaba, Henry Friday Nweke, Mohammed Ali Al-garadi, Fariha\\nZulfiqar,GhulamRaza,andNorAnizaAzmi. Deeplearning-basedbreastcancerclassificationthroughmedicalimagingmodalities:stateof\\nthe art and research challenges.Artificial Intelligence Review, 53(3):1655–1720, 2020.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 90}, page_content='[13] Inês Domingues, Gisèle Pereira, Pedro Martins, Hugo Duarte, João Santos, and Pedro Henriques Abreu. Using deep learning techniques in\\nmedical imaging: a systematic review of applications on ct and pet.Artificial Intelligence Review, 53(6):4093–4160, 2020.\\n[14] GeertLitjens,ThijsKooi,BabakEhteshamiBejnordi,ArnaudArindraAdiyosoSetio,FrancescoCiompi,MohsenGhafoorian,JeroenA.W.M.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 90}, page_content='van der Laak, Bram van Ginneken, and Clara I. Sánchez. A survey on deep learning in medical image analysis.Medical Image Analysis,\\n42:60–88, 2017.\\n[15] Asifullah Khan, Anabia Sohail, Umme Zahoora, and Aqsa Saeed Qureshi. A survey of the recent architectures of deep convolutional neural\\nnetworks. Artificial intelligence review, 53(8):5455–5516, 2020.\\n[16] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ŁukaszKaiser,andIlliaPolosukhin. Attention'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 90}, page_content='is all you need.Advances in neural information processing systems, 30, 2017.\\n[17] DzmitryBahdanau,KyunghyunCho,andYoshuaBengio. Neuralmachinetranslationbyjointlylearningtoalignandtranslate. arXivpreprint\\narXiv:1409.0473, 2014.\\n[18] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation.arXiv\\npreprint arXiv:1508.04025, 2015.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 90}, page_content='preprint arXiv:1508.04025, 2015.\\n[19] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,ThomasUnterthiner,MostafaDehghani,Matthias\\nMinderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale.arXiv preprint\\narXiv:2010.11929, 2020.\\n[20] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. Transformers in vision:\\nA survey.ACM Computing Surveys (CSUR), 2021.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 90}, page_content='A survey.ACM Computing Surveys (CSUR), 2021.\\n[21] Kai Han,Yunhe Wang, Hanting Chen, XinghaoChen, Jianyuan Guo, ZhenhuaLiu, Yehui Tang, AnXiao, Chunjing Xu, Yixing Xu,et al. A\\nsurvey on vision transformer.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.\\n[22] NIH. National institutes of health, us.https://www.nibib.nih.gov/science-education/science-topics.\\n. : Page 51 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 91}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\n[23] Bin Guan, Guoshan Zhang, Jinkun Yao, Xinbo Wang, and Mengxuan Wang. Arm fracture detection in x-rays based on improved deep\\nconvolutional neural network.Computers & Electrical Engineering, 81:106530, 2020.\\n[24] Amit Kumar Jaiswal, Prayag Tiwari, Sachin Kumar, Deepak Gupta, Ashish Khanna, and Joel J.P.C. Rodrigues. Identifying pneumonia in\\nchest x-rays: A deep learning approach.Measurement, 145:511–518, 2019.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 91}, page_content='[25] Naveed Chouhan, Asifullah Khan, Jehan Zeb Shah, Mazhar Hussnain, and Muhammad Waleed Khan. Deep convolutional neural network\\nand emotional learning based breast cancer detection using digital mammography.Computers in Biology and Medicine, 132:104318, 2021.\\n[26] Ademola Enitan Ilesanmi, Utairat Chaumrattanakul, and Stanislav S Makhanov. A method for segmentation of tumors in breast ultrasound'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 91}, page_content='images using the variant enhanced deep learning.Biocybernetics and Biomedical Engineering, 41(2):802–818, 2021.\\n[27] Lifang Chen, Tengfei Mao, and Qian Zhang. Identifying cardiomegaly in chest x-rays using dual attention network.Applied Intelligence,\\npages 1–10, 2022.\\n[28] PearlMarySamuelandThanikaiselvanVeeramalai. Vsscnet:vesselspecificskipchainconvolutionalnetworkforbloodvesselsegmentation.\\nComputer methods and programs in biomedicine, 198:105769, 2021.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 91}, page_content='[29] AbdelbakiSouid,NizarSakli,andHediSakli. Classificationandpredictionsoflungdiseasesfromchestx-raysusingmobilenetv2. Applied\\nSciences, 11(6):2751, 2021.\\n[30] Abdul Qayyum, Imran Razzak, M Tanveer, and Ajay Kumar. Depth-wise dense neural network for automatic covid19 infection detection\\nand diagnosis.Annals of operations research, pages 1–21, 2021.\\n[31] UM Prakash, Kottilingam Kottursamy, Korhan Cengiz, Utku Kose, and Bui Thanh Hung. 4x-expert systems for early prediction of'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 91}, page_content='osteoporosis using multi-model algorithms.Measurement, 180:109543, 2021.\\n[32] NeslihanBayramoglu,MiikaTNieminen,andSimoSaarakkala. Machinelearningbasedtextureanalysisofpatellafromx-raysfordetecting\\npatellofemoral osteoarthritis.International journal of medical informatics, 157:104627, 2022.\\n[33] Atıf Emre Yüksel, Sadullah Gültekin, Enis Simsar, Şerife Damla Özdemir, Mustafa Gündoğar, Salih Barkın Tokgöz, and İbrahim Ethem'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 91}, page_content='Hamamcı. Dental enumeration and multiple treatment detection on panoramic x-rays using deep learning.Scientific reports, 11(1):1–10,\\n2021.\\n[34] Rima Arnaout, Lara Curran, Yili Zhao, Jami C Levine, Erin Chinn, and Anita J Moon-Grady. An ensemble of neural networks provides\\nexpert-level prenatal detection of complex congenital heart disease.Nature medicine, 27(5):882–891, 2021.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 91}, page_content='[35] HanemEllethy,ShekharSChandra,andFatimaANasrallah. Thedetectionofmildtraumaticbraininjuryinpaediatricsusingartificialneural\\nnetworks. Computers in Biology and Medicine, 135:104614, 2021.\\n[36] Marcin Woźniak, Jakub Siłka, and Michał Wieczorek. Deep neural network correlation learning mechanism for ct brain tumor detection.\\nNeural Computing and Applications, pages 1–16, 2021.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 91}, page_content='[37] DhimanDas,KathyayiniSivasubramanian,PraveenbalajiRajendran,andManojitPramanik.Label-freehighframerateimagingofcirculating\\nblood clots using a dual modal ultrasound and photoacoustic system.Journal of Biophotonics, 14(3):e202000371, 2021.\\n[38] Jordan Chamberlin, Madison R Kocher, Jeffrey Waltz, Madalyn Snoddy, Natalie FC Stringer, Joseph Stephenson, Pooyan Sahbaee, Puneet'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 91}, page_content='Sharma, Saikiran Rapaka, U Joseph Schoepf, et al. Automated detection of lung nodules and coronary artery calcium using artificial\\nintelligence on low-dose ct scans for lung cancer screening: accuracy and prognostic value.BMC medicine, 19(1):1–14, 2021.\\n[39] J Akilandeswari, G Jothi, A Naveenkumar, RS Sabeenian, P Iyyanar, and ME Paramasivam. Detecting pulmonary embolism using deep\\nneural networks.International Journal of Performability Engineering, 17(3), 2021.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 91}, page_content='[40] Adel Oulefki, Sos Agaian, Thaweesak Trongtirakul, and Azzeddine Kassah Laouar. Automatic covid-19 lung infected region segmentation\\nand measurement using ct-scans images.Pattern recognition, 114:107747, 2021.\\n[41] SumitaMondal,AnupKSadhu,andPranabKumarDutta. Adaptivelocalternarypatternonparameteroptimized-fasterregionconvolutional\\nneural network for pulmonary emphysema diagnosis.IEEE Access, 9:114135–114152, 2021.\\n[42] AAO. American academy of ophthalmology.https://www.aao.org/.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 91}, page_content='[43] Gabriella Moraes, Dun Jack Fu, Marc Wilson, Hagar Khalid, Siegfried K Wagner, Edward Korot, Daniel Ferraz, Livia Faes, Christopher J\\nKelly, TerrySpitz, etal. Quantitativeanalysis ofoct for neovascularage-related macular degenerationusing deeplearning.Ophthalmology,\\n128(5):693–705, 2021.\\n[44] GahyungRyu,KyungminLee,DonggeunPark,SangHyunPark,andMinSagong. Adeeplearningmodelforidentifyingdiabeticretinopathy\\nusing optical coherence tomography angiography.Scientific reports, 11(1):1–9, 2021.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 91}, page_content='[45] Shotaro Asano, Ryo Asaoka, Hiroshi Murata, Yohei Hashimoto, Atsuya Miki, Kazuhiko Mori, Yoko Ikeda, Takashi Kanamoto, Junkichi\\nYamagami, and Kenji Inoue. Predicting the central 10 degrees visual field in glaucoma by applying a deep learning algorithm to optical\\ncoherence tomography images.Scientific Reports, 11(1):1–10, 2021.\\n[46] Esther Parra-Mora, Alex Cazañas-Gordon, Rui Proença, and Luís A da Silva Cruz. Epiretinal membrane detection in optical coherence'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 91}, page_content='tomography retinal images using deep learning.IEEE Access, 9:99201–99219, 2021.\\n[47] Zhenhua Wang, Yuanfu Zhong, Mudi Yao, Yan Ma, Wenping Zhang, Chaopeng Li, Zhifu Tao, Qin Jiang, and Biao Yan. Automated\\nsegmentation of macular edema for the diagnosis of ocular disease using deep learning method.Scientific Reports, 11(1):1–12, 2021.\\n[48] SyedAlEHassan,ShahzadAkbar,SaharGull,AmjadRehman,andHindAlaska. Deeplearning-basedautomaticdetectionofcentralserous'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 91}, page_content='retinopathyusingopticalcoherencetomographicimages. In 20211stInternationalConferenceonArtificialIntelligenceandDataAnalytics\\n(CAIDA), pages 206–211. IEEE, 2021.\\n[49] M Kashif Yaqoob, Syed Farooq Ali, Irfan Kareem, and Muhammad Moazam Fraz. Feature-based optimized deep residual network\\narchitecture for diabetic retinopathy detection. In2020 IEEE 23rd International Multitopic Conference (INMIC), pages 1–6. IEEE, 2020.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 91}, page_content='[50] Shumpei Obata, Yusuke Ichiyama, Masashi Kakinoki, Osamu Sawada, Yoshitsugu Saishin, Taku Ito, Mari Tomioka, and Masahito Ohji.\\nPredictionofpostoperativevisualacuityaftervitrectomyformacularholeusingdeeplearning–basedartificialintelligence. Graefe’sArchive\\nfor Clinical and Experimental Ophthalmology, pages 1–11, 2021.\\n[51] Michael Abràmoff and Christine N. Kay. Chapter 6 - image processing. In Stephen J. Ryan, SriniVas R. Sadda, David R. Hinton, Andrew P.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 91}, page_content='Schachat, SriniVas R. Sadda, C.P. Wilkinson, Peter Wiedemann, and Andrew P. Schachat, editors,Retina (Fifth Edition), pages 151–176.\\n. : Page 52 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 92}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\nW.B. Saunders, London, fifth edition edition, 2013.\\n[52] Quang TM Pham, Sangil Ahn, Jitae Shin, and Su Jeong Song. Generating future fundus images for early age-related macular degeneration\\nbased on generative adversarial networks.Computer Methods and Programs in Biomedicine, page 106648, 2022.\\n[53] Sufian A Badawi, Muhammad Moazam Fraz, Muhammad Shehzad, Imran Mahmood, Sajid Javed, Emad Mosalam, and Ajay Kamath'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 92}, page_content='Nileshwar. Detection and grading of hypertensive retinopathy using vessels tortuosity and arteriovenous ratio.Journal of Digital Imaging,\\npages 1–21, 2022.\\n[54] HariMohanRaiandKalyanChatterjee. 2dmriimageanalysisandbraintumordetectionusingdeeplearningcnnmodelleu-net. Multimedia\\nTools and Applications, 80(28):36111–36141, 2021.\\n[55] ZamirMerali,JustinZWang,JetanHBadhiwala,ChristopherDWitiw,JeffersonRWilson,andMichaelGFehlings. Adeeplearningmodel'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 92}, page_content='for detection of cervical spinal cord compression in mri scans.Scientific reports, 11(1):1–11, 2021.\\n[56] SaeedaNaz,AbidaAshraf,andAhmadZaib. Transferlearningusingfreezefeaturesforalzheimerneurologicaldisorderdetectionusingadni\\ndataset. Multimedia Systems, 28(1):85–94, 2022.\\n[57] Mei Yang, Yiming Zheng, Zhiying Xie, Zhaoxia Wang, Jiangxi Xiao, Jue Zhang, and Yun Yuan. A deep learning model for diagnosing\\ndystrophinopathies on thigh muscle mri images.BMC neurology, 21(1):1–9, 2021.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 92}, page_content='[58] Mazhar Javed Awan, Mohd Shafry Mohd Rahim, Naomie Salim, Mazin Abed Mohammed, Begonya Garcia-Zapirain, and Karrar Hameed\\nAbdulkareem. Efficient detection of knee anterior cruciate ligament from magnetic resonance imaging using deep learning approach.\\nDiagnostics, 11(1):105, 2021.\\n[59] Jiang Zhang, Chen Li, Ganwanming Liu, Min Min, Chong Wang, Jiyi Li, Yuting Wang, Hongmei Yan, Zhentao Zuo, Wei Huang, et al. A'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 92}, page_content='cnn-transformerhybridapproachfordecodingvisualneuralactivityintotext. ComputerMethodsandProgramsinBiomedicine ,214:106586,\\n2022.\\n[60] Radiologyinfo.org. Radiologyinfo.org for patients.https://www.radiologyinfo.org/.\\n[61] ZhenWang,GuangxuLi,JingjieZhou,andPhilipO.Ogunbona. Opticalflownetworksforheartbeatestimationin4dultrasoundimages. In\\n2021 7th International Conference on Computing and Artificial Intelligence, pages 127–131, 2021.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 92}, page_content='[62] Yuyu Guo, Lei Bi, Euijoon Ahn, Dagan Feng, Qian Wang, and Jinman Kim. A spatiotemporal volumetric interpolation network for 4d\\ndynamic medical image. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4726–4735,\\n2020.\\n[63] Anil V Parwani.Whole Slide Imaging: Current Applications and Future Directions. Springer Nature, 2021.\\n[64] Muhammad Shaban, Syed Ali Khurram, Muhammad Moazam Fraz, Najah Alsubaie, Iqra Masood, Sajid Mushtaq, Mariam Hassan, Asif'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 92}, page_content='Loya, and Nasir M Rajpoot. A novel digital score for abundance of tumour infiltrating lymphocytes predicts disease free survival in oral\\nsquamous cell carcinoma.Scientific reports, 9(1):1–13, 2019.\\n[65] SajidJaved,ArifMahmood,MuhammadMoazamFraz,NavidAlemiKoohbanani,KsenijaBenes,Yee-WahTsang,KatherineHewitt,David\\nEpstein,DavidSnead,andNasirRajpoot.Cellularcommunitydetectionfortissuephenotypingincolorectalcancerhistologyimages. Medical\\nimage analysis, 63:101696, 2020.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 92}, page_content='image analysis, 63:101696, 2020.\\n[66] RM Saad Bashir, Hanya Mahmood, Muhammad Shaban, Shan E Ahmed Raza, M Moazam Fraz, Syed Ali Khurram, and Nasir M Rajpoot.\\nAutomated grade classification of oral epithelial dysplasia using morphometric analysis of histology images. InMedical Imaging 2020:\\nDigital Pathology, volume 11320, page 1132011. International Society for Optics and Photonics, 2020.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 92}, page_content='[67] Yanning Zhou, Simon Graham, Navid Alemi Koohbanani, Muhammad Shaban, Pheng-Ann Heng, and Nasir Rajpoot. Cgc-net: Cell graph\\nconvolutional network for grading of colorectal cancer histology images. InProceedings of the IEEE/CVF International Conference on\\nComputer Vision Workshops, pages 0–0, 2019.\\n[68] MuhammadShaban,RuqayyaAwan,MuhammadMoazamFraz,AyeshaAzam,Yee-WahTsang,DavidSnead,andNasirMRajpoot.Context-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 92}, page_content='aware convolutional neural network for grading of colorectal cancer histology images.IEEE Transactions on Medical Imaging, 2020.\\n[69] MMFraz,SAKhurram,SGraham,MShaban,MHassan,ALoya,andNMRajpoot. Fabnet:featureattention-basednetworkforsimultaneous\\nsegmentationofmicrovesselsandnervesinroutinehistologyimagesoforalcancer. NeuralComputingandApplications ,pages1–14,2019.\\n[70] MMFraz,MuhammadShaban,SimonGraham,SyedAliKhurram,andNasirMRajpoot. Uncertaintydrivenpoolingnetworkformicrovessel'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 92}, page_content='segmentation in routine histology images. InComputational pathology and ophthalmic medical image analysis, pages 156–164. Springer,\\n2018.\\n[71] SNRashid,MMFraz,andSJaved. Multiscaledilatedunetforsegmentationofmulti-organnucleiindigitalhistologyimages. In 2020IEEE\\n17thInternationalConferenceonSmartCommunities:ImprovingQualityofLifeUsingICT,IoTandAI(HONET) ,pages68–72.IEEE,2020.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 92}, page_content='[72] Moritz Schwyzer, Daniela A Ferraro, Urs J Muehlematter, Alessandra Curioni-Fontecedro, Martin W Huellner, Gustav K Von Schulthess,\\nPhilipp A Kaufmann, Irene A Burger, and Michael Messerli. Automated detection of lung cancer at ultralow dose pet/ct by deep neural\\nnetworks–initial results.Lung Cancer, 126:170–173, 2018.\\n[73] Benjamin H Kann, Sanjay Aneja, Gokoulakrichenane V Loganadane, Jacqueline R Kelly, Stephen M Smith, Roy H Decker, James B Yu,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 92}, page_content='HenrySPark,WendellGYarbrough,AjayMalhotra,etal.Pretreatmentidentificationofheadandneckcancernodalmetastasisandextranodal\\nextension using deep learning neural networks.Scientific reports, 8(1):1–11, 2018.\\n[74] Kobra Etminani, Amira Soliman, Anette Davidsson, Jose R Chang, Begoña Martínez-Sanchis, Stefan Byttner, Valle Camacho, Matteo\\nBauckneht, Roxana Stegeran, Marcus Ressner, et al. A 3d deep learning model to predict the diagnosis of dementia with lewy bodies,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 92}, page_content='alzheimer’s disease, and mild cognitive impairment using brain 18f-fdg pet.European journal of nuclear medicine and molecular imaging,\\n49(2):563–584, 2022.\\n[75] Rikiya Yamashita, Mizuho Nishio, Richard Kinh Gian Do, and Kaori Togashi. Convolutional neural networks: an overview and application\\nin radiology.Insights into imaging, 9(4):611–629, 2018.\\n[76] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks.Advances in'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 92}, page_content='neural information processing systems, 25, 2012.\\n. : Page 53 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 93}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\n[77] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint\\narXiv:1409.1556, 2014.\\n[78] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and\\nAndrew Rabinovich. Going deeper with convolutions. InProceedings of the IEEE conference on computer vision and pattern recognition,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 93}, page_content='pages 1–9, 2015.\\n[79] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. InProceedings of the IEEE\\nconference on computer vision and pattern recognition, pages 770–778, 2016.\\n[80] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In\\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 1492–1500, 2017.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 93}, page_content='[81] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. InProceedings of the IEEE conference on computer vision and pattern\\nrecognition, pages 7132–7141, 2018.\\n[82] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. InProceedings\\nof the IEEE conference on computer vision and pattern recognition, pages 4700–4708, 2017.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 93}, page_content='[83] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. InInternational conference on\\nmachine learning, pages 6105–6114. PMLR, 2019.\\n[84] Hafiz Syed Ahmed Qasim, Muhammad Shahzad, and Muhammad Moazam Fraz. Deep learning for face detection: Recent advancements.\\nIn 2021 International Conference on Digital Futures and Transformative Technologies (ICoDT2), pages 1–6. IEEE, 2021.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 93}, page_content='[85] Muhammad Arslan Hashmi, Qaiser Riaz, Muhammad Zeeshan, Muhammad Shahzad, and Muhammad Moazam Fraz. Motion reveal\\nemotions: identifying emotions from human walk using chest mounted smartphone.IEEE Sensors Journal, 20(22):13511–13522, 2020.\\n[86] WeiWangandJianxunGang. Applicationofconvolutionalneuralnetworkinnaturallanguageprocessing. In 2018InternationalConference\\non Information Systems and Computer Aided Education (ICISCAE), pages 64–70. IEEE, 2018.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 93}, page_content='[87] Qi Zhan, Wenjin Wang, and Gerard de Haan. Analysis of cnn-based remote-ppg to understand limitations and sensitivities.Biomedical\\noptics express, 11(3):1268–1283, 2020.\\n[88] ArnaudArindraAdiyosoSetio,AlbertoTraverso,ThomasDeBel,MoiraSNBerens,CasVanDenBogaard,PiergiorgioCerello,HaoChen,\\nQi Dou, Maria Evelina Fantacci, Bram Geurts, et al. Validation, comparison, and combination of algorithms for automatic detection of'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 93}, page_content='pulmonary nodules in computed tomography images: the luna16 challenge.Medical image analysis, 42:1–13, 2017.\\n[89] Samuel G Armato III, Geoffrey McLennan, Luc Bidaut, Michael F McNitt-Gray, Charles R Meyer, Anthony P Reeves, Binsheng Zhao,\\nDenise R Aberle, Claudia I Henschke, Eric A Hoffman, et al. The lung image database consortium (lidc) and image database resource\\ninitiative (idri): a completed reference database of lung nodules on ct scans.Medical physics, 38(2):915–931, 2011.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 93}, page_content='[90] LaugeSorensen,SaherBShaker,andMarleenDeBruijne. Quantitativeanalysisofpulmonaryemphysemausinglocalbinarypatterns. IEEE\\ntransactions on medical imaging, 29(2):559–569, 2010.\\n[91] XingyiYang,XuehaiHe,JinyuZhao,YichenZhang,ShanghangZhang,andPengtaoXie. Covid-ct-dataset:actscandatasetaboutcovid-19.\\narXiv preprint arXiv:2003.13865, 2020.\\n[92] PlamenAngelovandEduardoAlmeidaSoares.Sars-cov-2ct-scandataset:Alargedatasetofrealpatientsctscansforsars-cov-2identification.\\nMedRxiv, 2020.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 93}, page_content='MedRxiv, 2020.\\n[93] DimitriosKollias,AnastasiosArsenos,LevonSoukissian,andStefanosKollias. Mia-cov19d:Covid-19detectionthrough3-dchestctimage\\nanalysis. InProceedings of the IEEE/CVF International Conference on Computer Vision, pages 537–544, 2021.\\n[94] Mohammad Rahimzadeh, Abolfazl Attar, and Seyed Mohammad Sakhaei. A fully automated deep learning-based network for detecting\\ncovid-19 from a new and large lung ct scan dataset.Biomedical Signal Processing and Control, 68:102588, 2021.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 93}, page_content='[95] DanielKermany,KangZhang,andMichaelGoldbaum. Labeledopticalcoherencetomography(oct)andchestx-rayimagesforclassification.\\nMendeley Data, 2, 2018.\\n[96] Maria de la Iglesia Vayá, Jose Manuel Saborit, Joaquim Angel Montell, Antonio Pertusa, Aurelia Bustos, Miguel Cazorla, Joaquin Galant,\\nXavier Barber, Domingo Orozco-Beltrán, Francisco García-García, et al. Bimcv covid-19+: a large annotated dataset of rx and ct images\\nfrom covid-19 patients.arXiv preprint arXiv:2006.01174, 2020.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 93}, page_content='[97] Unais Sait, Gokul Lal KV, Sunny Prakash Prajapati, Rahul Bhaumik, Tarun Kumar, Sanjana Shivakumar, and Kriti Bhalla. Curated dataset\\nfor covid-19 posterior-anterior chest radiography images (x-rays).Mendeley Data, 3, 2021.\\n[98] Fathi El-Shafai, Walid; Abd El-Samie. Extensive covid-19 x-ray and ct chest images dataset.Mendeley Data, 3, 2020.\\n[99] LindaWang,ZhongQiuLin,andAlexanderWong. Covid-net:Atailoreddeepconvolutionalneuralnetworkdesignfordetectionofcovid-19'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 93}, page_content='cases from chest x-ray images.Scientific Reports, 10(1):1–12, 2020.\\n[100] Shirin Hajeb Mohammad Alipour, Hossein Rabbani, and Mohammad Reza Akhlaghi. Diabetic retinopathy grading by digital curvelet\\ntransform. Computational and mathematical methods in medicine, 2012, 2012.\\n[101] JakobNikolasKather,FGZöllner,FBianconi,SMMelchers,LRSchad,TGaiser,AMarx,andCAWeis. Collectionoftexturesincolorectal\\ncancer histology.Zenodo https://doi. org/10, 5281, 2016.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 93}, page_content='[102] Siham Tabik, Anabel Gómez-Ríos, José Luis Martín-Rodríguez, Iván Sevillano-García, Manuel Rey-Area, David Charte, Emilio Guirado,\\nJuan-Luis Suárez, Julián Luengo, MA Valero-González, et al. Covidgr dataset and covid-sdnet methodology for predicting covid-19 based\\non chest x-ray images.IEEE journal of biomedical and health informatics, 24(12):3595–3605, 2020.\\n[103] Prasanna Porwal, Samiksha Pachade, Ravi Kamble, Manesh Kokare, Girish Deshmukh, Vivek Sahasrabuddhe, and Fabrice Meriaudeau.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 93}, page_content='Indian diabetic retinopathy image dataset (idrid): a database for diabetic retinopathy screening research.Data, 3(3):25, 2018.\\n[104] HaydenGunraj,AliSabri,DavidKoff,andAlexanderWong. Covid-netct-2:Enhanceddeepneuralnetworksfordetectionofcovid-19from\\nchest ct images through bigger, more diverse learning.arXiv preprint arXiv:2101.07433, 2021.\\n[105] John N Weinstein, Eric A Collisson, Gordon B Mills, Kenna R Shaw, Brad A Ozenberger, Kyle Ellrott, Ilya Shmulevich, Chris Sander, and'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 93}, page_content='Joshua M Stuart. The cancer genome atlas pan-cancer analysis project.Nature genetics, 45(10):1113–1120, 2013.\\n. : Page 54 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 94}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\n[106] Li Wang, Dong Nie, Guannan Li, Élodie Puybareau, Jose Dolz, Qian Zhang, Fan Wang, Jing Xia, Zhengwang Wu, Jia-Wei Chen, et al.\\nBenchmark on automatic six-month-old infant brain segmentation algorithms: the iseg-2017 challenge.IEEE transactions on medical\\nimaging, 38(9):2219–2230, 2019.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 94}, page_content='imaging, 38(9):2219–2230, 2019.\\n[107] Spyridon Bakas, Hamed Akbari, Aristeidis Sotiras, Michel Bilello, Martin Rozycki, Justin S Kirby, John B Freymann, Keyvan Farahani,\\nand Christos Davatzikos. Advancing the cancer genome atlas glioma mri collections with expert segmentation labels and radiomic features.\\nScientific data, 4(1):1–13, 2017.\\n[108] Adriënne M Mendrik, Koen L Vincken, Hugo J Kuijf, Marcel Breeuwer, Willem H Bouvy, Jeroen De Bresser, Amir Alansary, Marleen'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 94}, page_content='De Bruijne, Aaron Carass, Ayman El-Baz, et al. Mrbrains challenge: online evaluation framework for brain image segmentation in 3t mri\\nscans. Computational intelligence and neuroscience, 2015, 2015.\\n[109] Cathie Sudlow, John Gallacher, Naomi Allen, Valerie Beral, Paul Burton, John Danesh, Paul Downey, Paul Elliott, Jane Green, Martin\\nLandray, et al. Uk biobank: an open access resource for identifying the causes of a wide range of complex diseases of middle and old age.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 94}, page_content='PLoS medicine, 12(3):e1001779, 2015.\\n[110] Colin G Stirrat, Shirjel R Alam, Thomas J MacGillivray, Calum D Gray, Marc R Dweck, Jennifer Raftis, William SA Jenkins, William A\\nWallace,RenzoPessotto,KelvinHHLim,etal. Ferumoxytol-enhancedmagneticresonanceimagingassessinginflammationaftermyocardial\\ninfarction. Heart, 103(19):1528–1535, 2017.\\n[111] A Emre Kavur, N Sinem Gezer, Mustafa Barış, Sinem Aslan, Pierre-Henri Conze, Vladimir Groza, Duc Duy Pham, Soumick Chatterjee,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 94}, page_content='Philipp Ernst, Savaş Özkan, et al. Chaos challenge-combined (ct-mr) healthy abdominal organ segmentation.Medical Image Analysis,\\n69:101950, 2021.\\n[112] JoesStaal,MichaelDAbràmoff,MeindertNiemeijer,MaxAViergever,andBramVanGinneken. Ridge-basedvesselsegmentationincolor\\nimages of the retina.IEEE transactions on medical imaging, 23(4):501–509, 2004.\\n[113] Junji Shiraishi, Shigehiko Katsuragawa, Junpei Ikezoe, Tsuneo Matsumoto, Takeshi Kobayashi, Ken-ichi Komatsu, Mitate Matsui, Hiroshi'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 94}, page_content='Fujita,YoshieKodera,andKunioDoi.Developmentofadigitalimagedatabaseforchestradiographswithandwithoutalungnodule:receiver\\noperating characteristic analysis of radiologists’ detection of pulmonary nodules.American Journal of Roentgenology, 174(1):71–74, 2000.\\n[114] Nicholas Heller, Fabian Isensee, Klaus H Maier-Hein, Xiaoshuai Hou, Chunmei Xie, Fengyi Li, Yang Nan, Guangrui Mu, Zhiyong Lin,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 94}, page_content='Miofei Han, et al. The state of the art in kidney and kidney tumor segmentation in contrast-enhanced ct imaging: Results of the kits19\\nchallenge. Medical image analysis, 67:101821, 2021.\\n[115] Daniel S Kermany, Michael Goldbaum, Wenjia Cai, Carolina CS Valentim, Huiying Liang, Sally L Baxter, Alex McKeown, Ge Yang,\\nXiaokang Wu, Fangbing Yan, et al. Identifying medical diagnoses and treatable diseases by image-based deep learning.Cell, 172(5):1122–\\n1131, 2018.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 94}, page_content='1131, 2018.\\n[116] Dina Demner-Fushman, Marc D Kohli, Marc B Rosenman, Sonya E Shooshan, Laritza Rodriguez, Sameer Antani, George R Thoma, and\\nClement J McDonald. Preparing a collection of radiology examinations for distribution and retrieval.Journal of the American Medical\\nInformatics Association, 23(2):304–310, 2016.\\n[117] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Roger G Mark, and'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 94}, page_content='Steven Horng. Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports.Scientific data, 6(1):1–8,\\n2019.\\n[118] Baoyu Jing, Pengtao Xie, and Eric Xing. On the automatic generation of medical imaging reports.arXiv preprint arXiv:1711.08195, 2017.\\n[119] Baiyu Chen, Xinhui Duan, Zhicong Yu, Shuai Leng, Lifeng Yu, and Cynthia McCollough. Development and validation of an open data\\nformat for ct projection data.Medical physics, 42(12):6964–6972, 2015.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 94}, page_content='[120] BennettALandman,AlanJHuang,AliyaGifford,DeeptiSVikram,IsselAnneLLim,JonathanADFarrell,JohnABogovic,JunHua,Min\\nChen, Samson Jarso, et al. Multi-parametric neuroimaging reproducibility: a 3-t resource study.Neuroimage, 54(4):2854–2866, 2011.\\n[121] EirikurAgustssonandRaduTimofte. Ntire2017challengeonsingleimagesuper-resolution:Datasetandstudy. In ProceedingsoftheIEEE\\nconference on computer vision and pattern recognition workshops, pages 126–135, 2017.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 94}, page_content='[122] Jure Zbontar, Florian Knoll, Anuroop Sriram, Tullie Murrell, Zhengnan Huang, Matthew J Muckley, Aaron Defazio, Ruben Stern, Patricia\\nJohnson, Mary Bruno, et al. fastmri: An open dataset and benchmarks for accelerated mri.arXiv preprint arXiv:1811.08839, 2018.\\n[123] Martin Weigert, Uwe Schmidt, Tobias Boothe, Andreas Müller, Alexandr Dibrov, Akanksha Jain, Benjamin Wilhelm, Deborah Schmidt,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 94}, page_content='Coleman Broaddus, Siân Culley, et al. Content-aware image restoration: pushing the limits of fluorescence microscopy.Nature methods,\\n15(12):1090–1097, 2018.\\n[124] Mingyu Kim, Jihye Yun, Yongwon Cho, Keewon Shin, Ryoungwoo Jang, Hyun-jin Bae, and Namkug Kim. Deep learning in medical\\nimaging. Neurospine, 16(4):657, 2019.\\n[125] Md Shahin Ali, Md Sipon Miah, Jahurul Haque, Md Mahbubur Rahman, and Md Khairul Islam. An enhanced technique of skin cancer'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 94}, page_content='classificationusingdeepconvolutionalneuralnetworkwithtransferlearningmodels. MachineLearningwithApplications ,5:100036,2021.\\n[126] Jyostna Devi Bodapati, Nagur Shareef Shaik, and Veeranjaneyulu Naralasetti. Composite deep neural network with gated-attention\\nmechanism for diabetic retinopathy severity classification.Journal of Ambient Intelligence and Humanized Computing, 12(10):9825–9839,\\n2021.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 94}, page_content='2021.\\n[127] Vinayakumar Ravi, Harini Narasimhan, and Tuan D Pham. Efficientnet-based convolutional neural networks for tuberculosis classification.\\nIn Advances in Artificial Intelligence, Computation, and Data Science, pages 227–244. Springer, 2021.\\n[128] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 94}, page_content='Mobilenets: Efficient convolutional neural networks for mobile vision applications.arXiv preprint arXiv:1704.04861, 2017.\\n[129] Muhammad Sufyan Arshad, Usman Abdur Rehman, and Muhammad Moazam Fraz. Plant disease identification using transfer learning. In\\n2021 International Conference on Digital Futures and Transformative Technologies (ICoDT2), pages 1–5. IEEE, 2021.\\n[130] TufailSajjadShahHashmi,NazeefUlHaq,MuhammadMoazamFraz,andMuhammadShahzad. Applicationofdeeplearningforweapons'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 94}, page_content='detection in surveillance videos. In2021 International Conference on Digital Futures and Transformative Technologies (ICoDT2), pages\\n1–6. IEEE, 2021.\\n. : Page 55 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 95}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\n[131] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic\\nsegmentation. InProceedings of the IEEE conference on computer vision and pattern recognition, pages 580–587, 2014.\\n[132] Ross Girshick. Fast r-cnn. InProceedings of the IEEE international conference on computer vision, pages 1440–1448, 2015.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 95}, page_content='[133] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks.\\nAdvances in neural information processing systems, 28, 2015.\\n[134] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. InProceedings\\nof the IEEE conference on computer vision and pattern recognition, pages 779–788, 2016.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 95}, page_content='[135] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot\\nmultibox detector. InEuropean conference on computer vision, pages 21–37. Springer, 2016.\\n[136] Lohendran Baskaran, Subhi J Al’Aref, Gabriel Maliakal, Benjamin C Lee, Zhuoran Xu, Jeong W Choi, Sang-Eun Lee, Ji Min Sung, Fay Y\\nLin, Simon Dunham, et al. Automatic segmentation of multiple cardiovascular structures from cardiac computed tomography angiography'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 95}, page_content='images using deep learning.PloS one, 15(5):e0232573, 2020.\\n[137] Kunal Nagpal, Davis Foote, Yun Liu, Po-Hsuan Cameron Chen, Ellery Wulczyn, Fraser Tan, Niels Olson, Jenny L Smith, Arash\\nMohtashamian, James H Wren, et al. Development and validation of a deep learning algorithm for improving gleason scoring of prostate\\ncancer. NPJ digital medicine, 2(1):1–10, 2019.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 95}, page_content='cancer. NPJ digital medicine, 2(1):1–10, 2019.\\n[138] Sufian A Badawi and Muhammad Moazam Fraz. Optimizing the trainable b-cosfire filter for retinal blood vessel segmentation.PeerJ,\\n6:e5855, 2018.\\n[139] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. InInternational\\nConference on Medical image computing and computer-assisted intervention, pages 234–241. Springer, 2015.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 95}, page_content='[140] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation\\nwith deep convolutional nets, atrous convolution, and fully connected crfs.IEEE transactions on pattern analysis and machine intelligence,\\n40(4):834–848, 2017.\\n[141] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. InProceedings of the IEEE international conference on\\ncomputer vision, pages 2961–2969, 2017.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 95}, page_content='computer vision, pages 2961–2969, 2017.\\n[142] Ruixin Yang and Yingyan Yu. Artificial convolutional neural network in object detection and semantic segmentation for medical imaging\\nanalysis. Frontiers in Oncology, 11:573, 2021.\\n[143] EshaPahwa,DwijMehta,SanjeetKapadia,DevanshJain,andAchleshwarLuthra.Medskip:Medicalreportgenerationusingskipconnections\\nand integrated attention. InProceedings of the IEEE/CVF International Conference on Computer Vision, pages 3409–3415, 2021.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 95}, page_content='[144] Di You, Fenglin Liu, Shen Ge, Xiaoxia Xie, Jing Zhang, and Xian Wu. Aligntransformer: Hierarchical alignment of visual regions and\\ndisease tags for medical report generation. InInternational Conference on Medical Image Computing and Computer-Assisted Intervention,\\npages 72–82. Springer, 2021.\\n[145] RunyiLi,ZizhouWang,andLeiZhang.Imagecaptionandmedicalreportgenerationbasedondeeplearning:areviewandalgorithmanalysis.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 95}, page_content='In 2021 International Conference on Computer Information Science and Artificial Intelligence (CISAI), pages 373–379. IEEE, 2021.\\n[146] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. InProceedings of\\nthe IEEE conference on computer vision and pattern recognition, pages 3156–3164, 2015.\\n[147] KelvinXu,JimmyBa,RyanKiros,KyunghyunCho,AaronCourville,RuslanSalakhudinov,RichZemel,andYoshuaBengio. Show,attend'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 95}, page_content='andtell:Neuralimagecaptiongenerationwithvisualattention. In Internationalconferenceonmachinelearning ,pages2048–2057.PMLR,\\n2015.\\n[148] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Neural baby talk. InProceedings of the IEEE conference on computer vision and\\npattern recognition, pages 7219–7228, 2018.\\n[149] Justin Johnson, Andrej Karpathy, and Li Fei-Fei. Densecap: Fully convolutional localization networks for dense captioning. InProceedings'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 95}, page_content='of the IEEE conference on computer vision and pattern recognition, pages 4565–4574, 2016.\\n[150] Alexander Mathews, Lexing Xie, and Xuming He. Semstyle: Learning to generate stylised image captions using unaligned text. In\\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8591–8600, 2018.\\n[151] ImranKhurram,MMFraz,MuhammadShahzad,andNasirMRajpoot. Dense-captionnet:asentencegenerationarchitectureforfine-grained'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 95}, page_content='description of image semantics.Cognitive Computation, 13(3):595–611, 2021.\\n[152] MD Zakir Hossain, Ferdous Sohel, Mohd Fairuz Shiratuddin, and Hamid Laga. A comprehensive survey of deep learning for image\\ncaptioning. ACM Computing Surveys (CsUR), 51(6):1–36, 2019.\\n[153] TomotakaSobue,NoriyukiMoriyama,MasahiroKaneko,MasahikoKusumoto,ToshiakiKobayashi,RyosukeTsuchiya,RyutaroKakinuma,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 95}, page_content='HironobuOhmatsu,KanjiNagai,HiroyukiNishiyama,etal.Screeningforlungcancerwithlow-dosehelicalcomputedtomography:anti-lung\\ncancer association project.Journal of clinical oncology, 20(4):911–920, 2002.\\n[154] WeiZhao,JianchengYang,YingliSun,ChengLi,WeilanWu,LiangJin,ZhimingYang,BingbingNi,PanGao,PeijunWang,etal. 3ddeep\\nlearning from ct scans predicts tumor invasiveness of subcentimeter pulmonary adenocarcinomas.Cancer research, 78(24):6881–6889,\\n2018.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 95}, page_content='2018.\\n[155] Wei Zhao, Jiancheng Yang, Bingbing Ni, Dexi Bi, Yingli Sun, Mengdi Xu, Xiaoxia Zhu, Cheng Li, Liang Jin, Pan Gao, et al. Toward\\nautomaticpredictionofegfrmutationstatusinpulmonaryadenocarcinomawith3ddeeplearning. Cancermedicine,8(7):3532–3543,2019.\\n[156] Fangzhou Liao, Ming Liang, Zhe Li, Xiaolin Hu, and Sen Song. Evaluate the malignancy of pulmonary nodules using the 3-d deep leaky\\nnoisy-or network.IEEE transactions on neural networks and learning systems, 30(11):3484–3495, 2019.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 95}, page_content='[157] JianchengYang,HaoranDeng,XiaoyangHuang,BingbingNi,andYiXu. Relationallearningbetweenmultiplepulmonarynodulesviadeep\\nset attention transformers. In2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI), pages 1875–1878. IEEE, 2020.\\n[158] Yanan Wu, Shouliang Qi, Yu Sun, Shuyue Xia, Yudong Yao, and Wei Qian. A vision transformer for emphysema classification using ct\\nimages. Physics in Medicine & Biology, 66(24):245016, 2021.\\n. : Page 56 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 96}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\n[159] Cuong Do and Lan Vu. An approach for recognizing COVID-19 cases using convolutional neural networks applied to CT scan images.\\nIn Andrew G. Tescher and Touradj Ebrahimi, editors,Applications of Digital Image Processing XLIII, volume 11510, pages 719 – 727.\\nInternational Society for Optics and Photonics, SPIE, 2020.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 96}, page_content='[160] Halgurd S Maghdid, Aras T Asaad, Kayhan Zrar Ghafoor, Ali Safaa Sadiq, Seyedali Mirjalili, and Muhammad Khurram Khan. Diagnosing\\ncovid-19 pneumonia from x-ray and ct images using deep learning and transfer learning algorithms. InMultimodal image exploitation and\\nlearning 2021, volume 11734, page 117340E. International Society for Optics and Photonics, 2021.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 96}, page_content='[161] Matteo Polsinelli, Luigi Cinque, and Giuseppe Placidi. A light cnn for detecting covid-19 from ct scans of the chest.Pattern recognition\\nletters, 140:95–100, 2020.\\n[162] LeiwenFu,BingyiWang,TanweiYuan,XiaotingChen,YunlongAo,ThomasFitzpatrick,PeiyangLi,YiguoZhou,Yi-fanLin,QibinDuan,\\net al. Clinical characteristics of coronavirus disease 2019 (covid-19) in china: a systematic review and meta-analysis.Journal of Infection,\\n80(6):656–665, 2020.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 96}, page_content='80(6):656–665, 2020.\\n[163] Feng Pan, Tianhe Ye, Peng Sun, Shan Gui, Bo Liang, Lingli Li, Dandan Zheng, Jiazheng Wang, Richard L Hesketh, Lian Yang, et al. Time\\ncourse of lung changes on chest ct during recovery from 2019 novel coronavirus (covid-19) pneumonia.Radiology, 2020.\\n[164] Ara Abigail E. Ambita, Eujene Nikka V. Boquio, and Prospero C. Naval. Covit-gan: Vision transformer forcovid-19 detection in ct scan'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 96}, page_content='imageswith self-attention gan fordataaugmentation. In Igor Farkaš, Paolo Masulli, Sebastian Otte, and Stefan Wermter, editors,Artificial\\nNeural Networks and Machine Learning – ICANN 2021, pages 587–598, Cham, 2021. Springer International Publishing.\\n[165] Lei Zhang and Yan Wen. A transformer-based framework for automatic covid19 diagnosis in chest cts. In2021 IEEE/CVF International\\nConference on Computer Vision Workshops (ICCVW), pages 513–518, 2021.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 96}, page_content='[166] Xinggang Wang, Xianbo Deng, Qing Fu, Qiang Zhou, Jiapei Feng, Hui Ma, Wenyu Liu, and Chuansheng Zheng. A weakly-supervised\\nframeworkforcovid-19classificationandlesionlocalizationfromchestct. IEEEtransactionsonmedicalimaging ,39(8):2615–2625,2020.\\n[167] Lin Li, Lixin Qin, Zeguo Xu, Youbing Yin, Xin Wang, Bin Kong, Junjie Bai, Yi Lu, Zhenghan Fang, Qi Song, et al. Using artificial'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 96}, page_content='intelligencetodetectcovid-19andcommunity-acquiredpneumoniabasedonpulmonaryct:evaluationofthediagnosticaccuracy. Radiology,\\n296(2):E65–E71, 2020.\\n[168] JoelCMThan,PunLiangThon,OmarMohdRijal,RosminahMKassim,AshariYunus,NorlizaMNoor,andPatrickThen. Preliminarystudy\\non patch sizes in vision transformers (vit) for covid-19 and diseased lungs classification. In2021 IEEE National Biomedical Engineering\\nConference (NBEC), pages 146–150. IEEE, 2021.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 96}, page_content='Conference (NBEC), pages 146–150. IEEE, 2021.\\n[169] Xiaole Fan, Xiufang Feng, Yunyun Dong, and Huichao Hou. Covid-19 ct image recognition algorithm based on transformer and cnn.\\nDisplays, page 102150, 2022.\\n[170] Khushal Tyagi, Gaurav Pathak, Rahul Nijhawan, and Ankush Mittal. Detecting pneumonia using vision transformer and comparing with\\nother techniques. In2021 5th International Conference on Electronics, Communication and Aerospace Technology (ICECA), pages 12–16.\\nIEEE, 2021.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 96}, page_content='IEEE, 2021.\\n[171] Linh T Duong, Nhi H Le, Toan B Tran, Vuong M Ngo, and Phuong T Nguyen. Detection of tuberculosis from chest x-ray images: boosting\\nthe performance with vision transformer and transfer learning.Expert Systems with Applications, 184:115519, 2021.\\n[172] AdamBernheim, XueyanMei, MingqianHuang, YangYang, ZahiA Fayad,Ning Zhang,Kaiyue Diao,Bin Lin,Xiqi Zhu,Kunwei Li,etal.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 96}, page_content='Chest ct findings in coronavirus disease-19 (covid-19): relationship to duration of infection.Radiology, page 200463, 2020.\\n[173] Soumya Ranjan Nayak, Deepak Ranjan Nayak, Utkarsh Sinha, Vaibhav Arora, and Ram Bilas Pachori. Application of deep learning\\ntechniques for detection of covid-19 cases using chest x-ray images: A comprehensive study.Biomedical Signal Processing and Control,\\n64:102365, 2021.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 96}, page_content='64:102365, 2021.\\n[174] SangjoonPark,GwanghyunKim,YujinOh,JoonBeomSeo,SangMinLee,JinHwanKim,SungjunMoon,Jae-KwangLim,andJongChul\\nYe. Multi-task vision transformer using low-level chest x-ray feature corpus for covid-19 diagnosis and severity quantification.Medical\\nImage Analysis, 75:102299, 2022.\\n[175] Debaditya Shome, T Kar, Sachi Nandan Mohanty, Prayag Tiwari, Khan Muhammad, Abdullah AlTameem, Yazhou Zhang, and Abdul'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 96}, page_content='Khader Jilani Saudagar. Covid-transformer: Interpretable covid-19 detection using vision transformer for healthcare.International Journal\\nof Environmental Research and Public Health, 18(21):11086, 2021.\\n[176] Mohamad Mahmoud Al Rahhal, Yakoub Bazi, Rami M Jomaa, Ahmad AlShibli, Naif Alajlan, Mohamed Lamine Mekhalfi, and Farid\\nMelgani. Covid-19 detection in ct/x-ray imagery using vision transformers.Journal of Personalized Medicine, 12(2):310, 2022.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 96}, page_content='[177] Muhammad Aasharib Nawshad, Usama Aleem Shami, Sana Sajid, and Muhammad Moazam Fraz. Attention based residual network for\\neffective detection of covid-19 and viral pneumonia. In2021 International Conference on Digital Futures and Transformative Technologies\\n(ICoDT2), pages 1–7. IEEE, 2021.\\n[178] Yin Dai, Yifan Gao, and Fayu Liu. Transmed: Transformers advance multi-modal medical image classification.Diagnostics, 11(8):1384,\\n2021.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 96}, page_content='2021.\\n[179] Sharif Amit Kamran, Khondker Fariha Hossain, Alireza Tavakkoli, Stewart Lee Zuckerbrod, and Salah A Baker. Vtgan: Semi-supervised\\nretinal image synthesis and disease prediction using vision transformers. InProceedings of the IEEE/CVF International Conference on\\nComputer Vision, pages 3235–3245, 2021.\\n[180] Magdy Abd-Elghany Zeid, Khaled El-Bahnasy, and SE Abo-Youssef. Multiclass colorectal cancer histology images classification using'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 96}, page_content='vision transformers. In2021 Tenth International Conference on Intelligent Computing and Information Systems (ICICIS), pages 224–230.\\nIEEE, 2021.\\n[181] Haoyuan Chen, Chen Li, Xiaoyan Li, Md Mamunur Rahaman, Weiming Hu, Yixin Li, Wanli Liu, Changhao Sun, Hongzan Sun, Xinyu\\nHuang, et al. Il-mcam: An interactive learning and multi-channel attention mechanism-based weakly supervised colorectal histopathology\\nimage classification approach.Computers in Biology and Medicine, page 105265, 2022.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 96}, page_content='[182] Jingxing Li, Zhanglei Yang, and Yifan Yu. A medical ai diagnosis platform based on vision transformer for coronavirus. In2021 IEEE\\nInternational Conference on Computer Science, Electronic Information Engineering and Intelligent Control Technology (CEI), pages 246–\\n252. IEEE, 2021.\\n. : Page 57 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 97}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\n[183] Stefan Jaeger, Sema Candemir, Sameer Antani, Yì-Xiáng J Wáng, Pu-Xuan Lu, and George Thoma. Two public chest x-ray datasets for\\ncomputer-aided screening of pulmonary diseases.Quantitative imaging in medicine and surgery, 4(6):475, 2014.\\n[184] Joseph Paul Cohen, Paul Morrison, Lan Dao, Karsten Roth, Tim Q Duong, and Marzyeh Ghassemi. Covid-19 image data collection:'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 97}, page_content='Prospective predictions are the future.arXiv preprint arXiv:2006.11988, 2020.\\n[185] Shuang Liang, Weicun Zhang, and Yu Gu. A hybrid and fast deep learning framework for covid-19 detection via 3d chest ct images. In\\nProceedings of the IEEE/CVF International Conference on Computer Vision, pages 508–512, 2021.\\n[186] Liyang Chen, Zhiyuan You, Nian Zhang, Juntong Xi, and Xinyi Le. Utrad: Anomaly detection and localization with u-transformer.Neural\\nNetworks, 147:53–62, 2022.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 97}, page_content='Networks, 147:53–62, 2022.\\n[187] Zhijie Lin, Zhaoshui He, Shengli Xie, Xu Wang, Ji Tan, Jun Lu, and Beihai Tan. Aanet: Adaptive attention network for covid-19 detection\\nfrom chest x-ray images.IEEE Transactions on Neural Networks and Learning Systems, 32(11):4781–4792, 2021.\\n[188] Arnab Kumar Mondal, Arnab Bhattacharjee, Parag Singla, and AP Prathosh. xvitcos: Explainable vision transformer based covid-19'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 97}, page_content='screening using radiography.IEEE Journal of Translational Engineering in Health and Medicine, 10:1–10, 2021.\\n[189] Emanuele Pesce, Samuel Joseph Withey, Petros-Pavlos Ypsilantis, Robert Bakewell, Vicky Goh, and Giovanni Montana. Learning to detect\\nchest radiographs containing pulmonary lesions using visual attention networks.Medical image analysis, 53:26–38, 2019.\\n[190] LizongZhang,ShuxinFeng,GuiduoDuan,YingLi,andGuisongLiu. Detectionofmicroaneurysmsinfundusimagesbasedonanattention'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 97}, page_content='mechanism. Genes, 10(10):817, 2019.\\n[191] LiuLi,MaiXu,HanruoLiu,YangLi,XiaofeiWang,LaiJiang,ZulinWang,XiangFan,andNingliWang. Alarge-scaledatabaseandacnn\\nmodel for attention-based glaucoma detection.IEEE transactions on medical imaging, 39(2):413–424, 2019.\\n[192] Xi Xu, Yu Guan, Jianqiang Li, Zerui Ma, Li Zhang, and Li Li. Automatic glaucoma detection based on transfer induced attention network.\\nBioMedical Engineering OnLine, 20(1):1–19, 2021.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 97}, page_content='BioMedical Engineering OnLine, 20(1):1–19, 2021.\\n[193] Rodger C Haggitt. Barrett’s esophagus, dysplasia, and adenocarcinoma.Human pathology, 25(10):982–993, 1994.\\n[194] Christopher P Wild and Laura J Hardie. Reflux, barrett’s oesophagus and adenocarcinoma: burning questions.Nature Reviews Cancer,\\n3(9):676–684, 2003.\\n[195] NaofumiTomita,BehnazAbdollahi,JasonWei,BingRen,AriefSuriawinata,andSaeedHassanpour. Attention-baseddeepneuralnetworks'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 97}, page_content='for detection of cancerous and precancerous esophagus tissue on histopathological slides.JAMA network open, 2(11):e1914645–e1914645,\\n2019.\\n[196] Ming Y Lu, Drew FK Williamson, Tiffany Y Chen, Richard J Chen, Matteo Barbieri, and Faisal Mahmood. Data-efficient and weakly\\nsupervised computational pathology on whole-slide images.Nature biomedical engineering, 5(6):555–570, 2021.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 97}, page_content='[197] Richard J Chen, Ming Y Lu, Wei-Hung Weng, Tiffany Y Chen, Drew FK Williamson, Trevor Manz, Maha Shady, and Faisal Mahmood.\\nMultimodal co-attention transformer forsurvival prediction in gigapixel whole slide images. InProceedings of the IEEE/CVFInternational\\nConference on Computer Vision, pages 4015–4025, 2021.\\n[198] YangNing,ShouyiZhang,XiaomingXi,JieGuo,PeideLiu,andCaimingZhang. Cac-emvt:Efficientcoronaryarterycalciumsegmentation'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 97}, page_content='withmulti-scalevisiontransformers. In 2021IEEEInternationalConferenceonBioinformaticsandBiomedicine(BIBM) ,pages1462–1467.\\nIEEE, 2021.\\n[199] Matthew Chung Hai Lee, Kersten Petersen, Nick Pawlowski, Ben Glocker, and Michiel Schaap. Tetris: Template transformer networks for\\nimage segmentation with shape priors.IEEE transactions on medical imaging, 38(11):2596–2606, 2019.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 97}, page_content='[200] MaxJaderberg,KarenSimonyan,AndrewZisserman,etal.Spatialtransformernetworks. Advancesinneuralinformationprocessingsystems ,\\n28, 2015.\\n[201] XiaomengLi,QiDou,HaoChen,Chi-WingFu,XiaojuanQi,DanielLBelav `y,GabrieleArmbrecht,DieterFelsenberg,GuoyanZheng,and\\nPheng-AnnHeng. 3dmulti-scalefcnwithrandommodalityvoxeldropoutlearningforintervertebraldisclocalizationandsegmentationfrom\\nmulti-modality mr images.Medical image analysis, 45:41–54, 2018.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 97}, page_content='[202] XiaohangFu,LeiBi,AshnilKumar,MichaelFulham,andJinmanKim. Multimodalspatialattentionmodulefortargetingmultimodalpet-ct\\nlung tumor segmentation.IEEE Journal of Biomedical and Health Informatics, 25(9):3507–3516, 2021.\\n[203] Giammarco La Barbera, Pietro Gori, Haithem Boussaid, Bruno Belucci, Alessandro Delmonte, Jeanne Goulin, Sabine Sarnacki, Laurence\\nRouet, and Isabelle Bloch. Automatic size and pose homogenization with spatial transformer network to improve and accelerate pediatric'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 97}, page_content='segmentation. In2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), pages 1773–1776. IEEE, 2021.\\n[204] Daniel H Pak, Andrés Caballero, Wei Sun, and James S Duncan. Efficient aortic valve multilabel segmentation using a spatial transformer\\nnetwork. In2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI), pages 1738–1742. IEEE, 2020.\\n[205] Chunfeng Lian, Fan Wang, Hannah H Deng, Li Wang, Deqiang Xiao, Tianshu Kuang, Hung-Ying Lin, Jaime Gateno, Steve GF Shen,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 97}, page_content='Pew-ThianYap,etal. Multi-taskdynamictransformernetworkforconcurrentbonesegmentationandlarge-scalelandmarklocalizationwith\\ndentalcbct. In InternationalConferenceonMedicalImageComputingandComputer-AssistedIntervention ,pages807–816.Springer,2020.\\n[206] Chun Luo, Jing Zhang, Xinglin Chen, Yinhao Tang, Xiechuan Weng, and Fan Xu. Ucatr: Based on cnn and transformer encoding and'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 97}, page_content='cross-attention decoding for lesion segmentation of acute ischemic stroke in non-contrast computed tomography images. In2021 43rd\\nAnnual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), pages 3565–3568. IEEE, 2021.\\n[207] Olivier Petit, Nicolas Thome, Clement Rambour, Loic Themyr, Toby Collins, and Luc Soler. U-net transformer: Self and cross attention for'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 97}, page_content='medical image segmentation. InInternational Workshop on Machine Learning in Medical Imaging, pages 267–276. Springer, 2021.\\n[208] JienengChen,YongyiLu,QihangYu,XiangdeLuo,EhsanAdeli,YanWang,LeLu,AlanLYuille,andYuyinZhou. Transunet:Transformers\\nmake strong encoders for medical image segmentation.arXiv preprint arXiv:2102.04306, 2021.\\n[209] Yutong Xie, Jianpeng Zhang, Chunhua Shen, and Yong Xia. Cotr: Efficiently bridging cnn and transformer for 3d medical image'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 97}, page_content='segmentation. InInternationalconferenceonmedicalimagecomputingandcomputer-assistedintervention ,pages171–180.Springer,2021.\\n[210] Jianhong Cheng, Jin Liu, Hulin Kuang, and Jianxin Wang. A fully automated multimodal mri-based multi-task learning for glioma\\nsegmentation and idh genotyping.IEEE Transactions on Medical Imaging, 2022.\\n. : Page 58 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 98}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\n[211] Abhinav Sagar. Vitbis: Vision transformer for biomedical image segmentation. InClinical Image-Based Procedures, Distributed and\\nCollaborative Learning, Artificial Intelligence for Combating COVID-19 and Secure and Privacy-Preserving Machine Learning, pages\\n34–45. Springer, 2021.\\n[212] Qixuan Sun, Nianhua Fang, Zhuo Liu, Liang Zhao, Youpeng Wen, and Hongxiang Lin. Hybridctrm: Bridging cnn and transformer for'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 98}, page_content='multimodal brain image segmentation.Journal of Healthcare Engineering, 2021, 2021.\\n[213] Jose Dolz, Karthik Gopinath, Jing Yuan, Herve Lombaert, Christian Desrosiers, and Ismail Ben Ayed. Hyperdense-net: a hyper-densely\\nconnected cnn for multi-modal image segmentation.IEEE transactions on medical imaging, 38(5):1116–1126, 2018.\\n[214] MatthewSinclair,AndreasSchuh,KarlHahn,KerstenPetersen, YingBai,JamesBatten,MichielSchaap,andBenGlocker. Atlas-istn:joint'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 98}, page_content='segmentation,registrationandatlasconstructionwithimage-and-spatialtransformernetworks. MedicalImageAnalysis ,page102383,2022.\\n[215] AgisilaosChartsias,GiorgosPapanastasiou,ChengjiaWang,ScottSemple,DavidENewby,RohanDharmakumar,andSotiriosATsaftaris.\\nDisentangle,alignandfuseformultimodalandsemi-supervisedimagesegmentation. IEEEtransactionsonmedicalimaging ,40(3):781–792,\\n2020.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 98}, page_content='2020.\\n[216] Zheyao Gao and Xiahai Zhuang. Consistency based co-segmentation for multi-view cardiac mri using vision transformer. InInternational\\nWorkshop on Statistical Atlases and Computational Models of the Heart, pages 306–314. Springer, 2021.\\n[217] Dong Sui, Kang Zhang, Weifeng Liu, Jing Chen, Xiaoxuan Ma, and Zhaofeng Tian. Cst: A multitask learning framework for colorectal\\ncancer region mining based on transformer.BioMed Research International, 2021, 2021.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 98}, page_content='[218] YiyaoLiu,YiYang,WeiJiang,TianfuWang,andBaiyingLei. 3ddeepattentiveu-netwithtransformerforbreasttumorsegmentationfrom\\nautomated breast volume scanner. In2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society\\n(EMBC), pages 4011–4014. IEEE, 2021.\\n[219] HolgerRRoth,HirohisaOda,XiangrongZhou,NatsukiShimizu,YingYang,YuichiroHayashi,MasahiroOda,MichitakaFujiwara,Kazunari'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 98}, page_content='Misawa, and Kensaku Mori. An application of cascaded 3d fully convolutional networks for medical image segmentation.Computerized\\nMedical Imaging and Graphics, 66:90–99, 2018.\\n[220] Xiliang Zhu, Zhaoyun Cheng, Sheng Wang, Xianjie Chen, and Guoqing Lu. Coronary angiography image segmentation based on pspnet.\\nComputer Methods and Programs in Biomedicine, 200:105897, 2021.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 98}, page_content='[221] Yuhang Lu, Kang Zheng, Weijian Li, Yirui Wang, Adam P Harrison, Chihung Lin, Song Wang, Jing Xiao, Le Lu, Chang-Fu Kuo, et al.\\nContourtransformernetworkforone-shotsegmentationofanatomicalstructures. IEEEtransactionsonmedicalimaging ,40(10):2672–2684,\\n2020.\\n[222] GuifangZhang,Hon-ChengWong,ChengWang,JianjunZhu,LigongLu,andGaojunTeng. Atemporarytransformernetworkforguide-wire'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 98}, page_content='segmentation. In202114thInternationalCongressonImageandSignalProcessing,BioMedicalEngineeringandInformatics(CISP-BMEI) ,\\npages 1–5. IEEE, 2021.\\n[223] Yan-JieZhou,Xiao-LiangXie,Zeng-GuangHou,Gui-BinBian,Shi-QiLiu,andXiao-HuZhou. Frr-net:Fastrecurrentresidualnetworksfor\\nreal-time catheter segmentation and tracking in endovascular aneurysm repair. In2020 IEEE 17th International Symposium on Biomedical\\nImaging (ISBI), pages 961–964. IEEE, 2020.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 98}, page_content='Imaging (ISBI), pages 961–964. IEEE, 2020.\\n[224] Yan-Jie Zhou, Xiao-Liang Xie, Xiao-Hu Zhou, Shi-Qi Liu, Gui-Bin Bian, and Zeng-Guang Hou. Pyramid attention recurrent networks for\\nreal-timeguidewiresegmentationandtrackinginintraoperativex-rayfluoroscopy. ComputerizedMedicalImagingandGraphics ,83:101734,\\n2020.\\n[225] Yunxiang Li, Shuai Wang, Jun Wang, Guodong Zeng, Wenjun Liu, Qianni Zhang, Qun Jin, and Yaqi Wang. Gt u-net: A u-net like group'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 98}, page_content='transformer network for tooth root segmentation. InInternational Workshop on Machine Learning in Medical Imaging, pages 386–395.\\nSpringer, 2021.\\n[226] MengWang,WeifangZhu,FeiShi,JinzhuSu,HaoyuChen,KaiYu,YiZhou,YuanyuanPeng,ZhongyueChen,andXinjianChen. Mstganet:\\nAutomatic drusen segmentation from retinal oct images.IEEE Transactions on Medical Imaging, 2021.\\n[227] AyoubBenaliAmjoudandMustaphaAmrouch. Automaticgenerationofchestx-rayreportsusingatransformer-baseddeeplearningmodel.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 98}, page_content='In 2021 Fifth International Conference On Intelligent Computing in Data Sciences (ICDS), pages 1–5. IEEE, 2021.\\n[228] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for human pose estimation. InProceedings\\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5693–5703, 2019.\\n[229] Xing Jia, Yun Xiong, Jiawei Zhang, Yao Zhang, Blackley Suzanne, Yangyong Zhu, and Chunlei Tang. Radiology report generation for rare'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 98}, page_content='diseases via few-shot transformer. In2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), pages 1347–1352.\\nIEEE, 2021.\\n[230] Hojun Lee, Hyunjun Cho, Jieun Park, Jinyeong Chae, and Jihie Kim. Cross encoder-decoder transformer with global-local visual extractor\\nfor medical image captioning.Sensors, 22(4):1429, 2022.\\n[231] MarcellaCornia,MatteoStefanini,LorenzoBaraldi,andRitaCucchiara. Meshed-memorytransformerforimagecaptioning. In Proceedings'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 98}, page_content='of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10578–10587, 2020.\\n[232] Yuxuan Xiong, Bo Du, and Pingkun Yan. Reinforced transformer for medical image captioning. InInternational Workshop on Machine\\nLearning in Medical Imaging, pages 673–680. Springer, 2019.\\n[233] Zhicheng Zhang, Lequan Yu, Xiaokun Liang, Wei Zhao, and Lei Xing. Transct: dual-path transformer for low dose computed tomography.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 98}, page_content='In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 55–64. Springer, 2021.\\n[234] Lipei Zhang, Zizheng Xiao, Chao Zhou, Jianmin Yuan, Qiang He, Yongfeng Yang, Xin Liu, Dong Liang, Hairong Zheng, Wei Fan, et al.\\nSpatial adaptive and transformer fusion network (stfnet) for low-count pet blind denoising with mri.Medical Physics, 49(1):343–356, 2022.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 98}, page_content='[235] Yanmei Luo, Yan Wang, Chen Zu, Bo Zhan, Xi Wu, Jiliu Zhou, Dinggang Shen, and Luping Zhou. 3d transformer-gan for high-quality pet\\nreconstruction. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 276–285. Springer,\\n2021.\\n[236] LuluWang,HuazhengZhu,ZhongshiHe,YuanyuanJia,andJinglongDu. Adjacentslicesfeaturetransformernetworkforsingleanisotropic\\n3d brain mri image super-resolution.Biomedical Signal Processing and Control, 72:103339, 2022.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 98}, page_content='. : Page 59 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 99}, page_content='Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\\n[237] ZebinHu,HaoLiu,ZhendongLi,andZekuanYu. Data-enabledintelligenceincomplexindustrialsystemscross-modeltransformermethod\\nfor medical image synthesis.Complexity, 2021, 2021.\\n[238] KunTang,ZhiLi,LiliTian,LihuiWang,andYueminZhu. Admir–affineanddeformablemedicalimageregistrationfordrug-addictedbrain\\nimages. IEEE Access, 8:70960–70968, 2020.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 99}, page_content='images. IEEE Access, 8:70960–70968, 2020.\\n[239] XiaogangGu,FeixiangZhou,RongfeiChen,XinzhenRen,andWenjuZhou. Endoscopicsingleimagesuper-resolutionbasedontransformer\\nand convolutional neural network. InIntelligent Life System Modelling, Image Processing and Analysis, pages 24–32. Springer, 2021.\\n[240] Alvaro Gomariz, Tiziano Portenier, Patrick M Helbling, Stephan Isringhausen, Ute Suessbier, César Nombela-Arrieta, and Orcun Goksel.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 99}, page_content='Modalityattentionandsamplingenablesdeeplearningwithheterogeneousmarkercombinationsinfluorescencemicroscopy. Naturemachine\\nintelligence, 3(9):799–811, 2021.\\n[241] Zhengyang Wang, Yaochen Xie, and Shuiwang Ji. Global voxel transformer networks for augmented microscopy. Nature Machine\\nIntelligence, 3(2):161–171, 2021.\\n[242] Weijian Li, Viet-Duy Nguyen, Haofu Liao, Matt Wilder, Ke Cheng, and Jiebo Luo. Patch transformer for multi-tagging whole slide'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 99}, page_content='histopathology images. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 532–540.\\nSpringer, 2019.\\n. : Page 60 of 60'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 100}, page_content='Snapshot of Algebraic Vision\\nJoe Kileel and Kathl´ en Kohn\\nIn honor of Bernd Sturmfels’ 60th birthday\\nAbstract. In this survey article, we present interactions between algebraic\\ngeometry and computer vision, which have recently come under the header of\\nalgebraic vision. The subject has given new insights in multiple view geometry\\nand its application to 3D scene reconstruction and carried a host of novel\\nproblems and ideas back into algebraic geometry.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 100}, page_content='problems and ideas back into algebraic geometry.\\nComputer vision is the research field that studies how computers can gain un-\\nderstanding from 2D images and videos, similar to human cognitive abilities. Typ-\\nical computer vision tasks include the automatic recognition of objects in images,\\nthe detection of events in videos, and the reconstruction of 3D scenes from many\\ngiven 2D images. A general overview of computer vision is presented in textbook'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 100}, page_content='form in [172]. The subject is a pillar in the AI revolution.\\nAlgebraic vision is the symbiosis of computer vision and algebraic geometry.\\nMotivated by Chris Aholt’s Ph.D. thesis titled Polynomials in Multiview Geometry\\n[9] and earlier works, the term “algebraic vision” was coined during a particular\\nlunch held at a Seattle office of Google in early spring 2014, attended by Sameer\\nAgarwal, Chris Aholt, Joe Kileel, Hon-Leung Lee, Max Lieblich, Bernd Sturmfels,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 100}, page_content='and Rekha Thomas. The intent was to encourage interactions between the applied\\nalgebraic geometry community and the 3D reconstruction community in computer\\nvision. A short discussion of algebraic vision can be found in the review [ 34] on\\nnonlinear algebra and its applications.\\nHistorically, computer vision made substantial use of projective geometry and\\ncomputational algebra in parts of its foundations. Specifically multiple view geom-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 100}, page_content='etry, as described in the textbook [ 94] of Hartley and Zisserman, is modeled on\\nprojective three-space and two-space and group-equivariant (multi-)linear transfor-\\nmations between these. Similar algebraic treatments of the subject are the text-\\nbooks [133] and [137]. Previously, this connection was not well-appreciated by the\\n2020 Mathematics Subject Classification. Primary 68T45, 14Q20, 13P25; Secondary 13P15,\\n65H14, 13P10.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 100}, page_content='65H14, 13P10.\\nJ.K. is supported in part by NSF awards DMS-2309782 and IIS-2312746, and start-up grants\\nfrom the Department of Mathematics and Oden Institute at UT Austin.\\nK.K. is supported in part by the Wallenberg AI, Autonomous Systems and Software Program\\n(WASP) funded by the Knut and Alice Wallenberg Foundation.\\n1\\narXiv:2210.11443v2  [math.AG]  17 Oct 2023'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 101}, page_content='2 JOE KILEEL AND KATHL ´EN KOHN\\ncomputational algebra geometry community. However, in the last decade, algebro-\\ngeometric papers and workshops on 3D reconstruction have been appearing, leading\\nto novel results in multiple view geometry while motivating developments in applied\\nalgebraic geometry.\\nThe present article provides a survey of algebraic vision. No previous knowledge\\nof computer vision is assumed, and the prerequisites for computational algebraic'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 101}, page_content='geometry are kept mostly to the level of undergraduate texts [51]. Due to space lim-\\nitations, the article makes no attempt to be comprehensive in any way, but instead\\nit focuses narrowly on the role of projective varieties and systems of polynomial\\nequations in 3D vision. An outline of the sections is as follows:\\n• In Section 1, we introduce the problem of 3D scene reconstruction from\\nunkown cameras and its algebro-geometric nature.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 101}, page_content='unkown cameras and its algebro-geometric nature.\\n• In Section 2, we discuss a variety of usual models for cameras.\\n• In Section 3, we study multiview varieties which characterize feasible im-\\nages of points under fixed cameras. Their defining equations play a key\\nrole in 3D reconstruction algorithms, and their Euclidean distance degrees\\nmeasure the intrinsic complexity of noisy triangulation (i.e., the task of\\nrecovering the 3D coordinates of a point observed by known cameras).'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 101}, page_content='• In Section 4, we consider the space of all cameras. We explain how tuples\\nof cameras – up to changes of world coordinates – can be encoded via\\nmultifocal tensors [94].\\n• In Section 5, we overview the most popular algorithmic pipeline to solve\\n3D scene reconstruction, highlightingminimal problems that are the algebro-\\ngeometric heart of the pipeline.\\n• In Section 6, we describe polynomial solvers for minimal problems, focus-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 101}, page_content='ing on Gr¨ obner basis methods usingelimination templates and homotopy\\ncontinuation. Those method applies to zero-dimensional parameterized\\npolynomial systems in general.\\n• In Section 7, we discuss algebro-geometric approaches to understand de-\\ngenerate world scenes and image data, where uniqueness of reconstruction\\nbreaks down and algorithms can encounter difficulty.\\nAfter reading Sections 1 and 2, the other sections are essentially independent;'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 101}, page_content='only Section 6 builds on Section 5. We provide specific pointers to earlier sections\\nin case of partial dependencies.\\nSome important topics in algebraic vision that are omitted include group syn-\\nchronization (e.g., [161, 127]), uses of polynomial optimization (e.g., [ 104, 50, 6,\\n190, 44]), and approaches based on differential invariants (e.g., [42, 30]). Readers\\nmay consult [147] for a survey that covers numerical and large-scale optimization\\naspects in 3D reconstruction.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 101}, page_content='aspects in 3D reconstruction.\\nAcknowledgements. We thank Sameer Agarwal, Paul Breiding, Luca Car-\\nlone, Tim Duff, Hongyi Fan, Fredrik Kahl, Anton Leykin, Tomas Pajdla, Jean\\nPonce, Kristian Ranestad, Felix Rydell, Elima Shehu, Rekha Thomas, Matthew\\nTrager and Uli Walther for their comments on earlier versions of the manuscript.\\n1. Computer vision through the algebraic lens\\nOne of the main challenges in computer vision is the structure-from-motion'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 101}, page_content='(SfM) problem: given many 2D images, the task is to reconstruct the 3D scene'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 102}, page_content='SNAPSHOT OF ALGEBRAIC VISION 3\\nand also the positions of the cameras that took the pictures. This has many ap-\\nplications such as 3D mapping from images taken by drones [ 158], to localize and\\nnavigate autonomous cars and robots in a 3D world [ 83], or in the movie industry\\nto reconstruct 3D backgrounds [107], for photo tourism [2], and for combining real\\nand virtual worlds [60].\\nThe structure-from-motion problem is typically solved using the 3D reconstruc-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 102}, page_content='tion pipeline. We will now sketch a highly simplified version of that pipeline, il-\\nlustrated in Figure 1. We provide more details in Section 5.1. Given a set of 2D\\nimages, the first step in the pipeline is to take a few of the given images and identify\\ngeometric features, such as points or lines, that they have in common. In Figure 1b,\\na detection algorithm has been used that only identifies points. In the second step'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 102}, page_content='of the pipeline, we forget the original images and only keep the geometric features\\nwe have identified. We reconstruct the 3D coordinates of those features and also the\\ncamera poses, that is, the locations and orientations of the cameras. In Figure 1c,\\nfive common points were identified on two images, so we aim to reconstruct the\\nfive points in 3-space and the two cameras. Finally, we repeat this process several\\ntimes until we have recovered all cameras and also enough geometric features to'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 102}, page_content='approximate the 3D scene.\\n(a) Input images\\n (b) Image matching\\n(c) Reconstruct cam-\\neras and 3D points\\n (d) Output\\nFigure 1. 3D reconstruction pipeline (courtesy of Tomas Pajdla).\\nAs the second step of the pipeline forgets the pictures and only works with\\nalgebro-geometric features, such as points or lines, the reconstruction problem be-\\ncomes purely algebraic. More specifically, we aim to compute a fiber of the joint\\ncamera map:\\nΦ : X × Cm 99K Y,(1.1)'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 102}, page_content='camera map:\\nΦ : X × Cm 99K Y,(1.1)\\nthat maps an arrangement X ∈ Xof 3D features and a tuple (C1, . . . , Cm) ∈ Cm of\\ncameras to the m 2D images of X taken by the cameras. For instance in Figure 1c,\\nthe joint camera map becomes\\nΦ :\\n\\x00\\nR3\\x015\\n× C2 99K\\n\\x00\\nR2\\x015\\n×\\n\\x00\\nR2\\x015\\n.(1.2)\\nA full specification of the joint camera map requires a choice of camera model.\\nThe simplest model is a pinhole camera; see Figure 2. Such a camera simply takes'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 102}, page_content='a picture of a point in space by projecting it onto a plane. A pinhole camera in\\nstandard position is typically assumed to be centered at the origin such that its'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 103}, page_content='4 JOE KILEEL AND KATHL ´EN KOHN\\nimage plane is H = {(x, y, z) ∈ R2 | z = 1 }. In these coordinates, the pinhole\\ncamera is the map\\nR3 99K H, (x, y, z) 7−→(x\\nz , y\\nz , 1).\\n(x, y, z)\\n(x/z, y/z, 1)\\n(0, 0, 1)\\nH\\nc\\nFigure 2. A pinhole camera in standard position is centered\\nat c = (0 , 0, 0) and maps world points ( x, y, z) to image points\\n(x\\nz , y\\nz , 1) on the image plane H.\\nOften homogeneous coordinates are used to model cameras. This means that'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 103}, page_content='each point in the image plane is identified with the light ray passing through the\\npoint and the origin. In homogeneous coordinates, the standard pinhole camera in\\nFigure 2 becomes\\nP3\\nR 99K P2\\nR, [x : y : z : w] 7−→[x : y : z].\\nThis map is defined everywhere except at the camera center [0 : 0 : 0 : 1], i.e. the\\norigin in the affine chart where w = 1.\\nThe projective geometry approach in modeling cameras is thoroughly explained'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 103}, page_content='in the textbook [ 94]. That book laid many foundations and conventions used in\\nmodern computer vision and offers a great entry point for the algebraic community\\ninto the field of computer vision. The main focus of the book [ 94] is multiview\\ngeometry, where a 3D object is viewed by several cameras, such as in Figure 1.\\nIn that setting, we cannot assume that all cameras are in standard position as\\ndescribed above. Instead, a pinhole camera is more generally given by a 3 × 4'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 103}, page_content='matrix A of rank three. The corresponding camera map\\nP3\\nR 99K P2\\nR, X 7−→AX.\\nis defined everywhere except at the camera center that is given by the kernel of A.\\nThe standard camera in Figure 2 corresponds to the matrix\\nh1 0 0 0\\n0 1 0 0\\n0 0 1 0\\ni\\n.\\nHence, when using pinhole cameras and homogeneous coordinates, the camera\\nvariety Cm in (1.1) that describes all m-tuples of such cameras is\\nCm = (PMat3×4\\n3 )m,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 104}, page_content='SNAPSHOT OF ALGEBRAIC VISION 5\\nwhere Mat3×4\\n3 ⊂ R3×4 denotes the set of 3 ×4 matrices of rank three. For instance,\\nthe joint camera map in (1.2) becomes\\nΦ :\\n\\x00\\nP3\\nR\\n\\x015\\n× (PMat3×4\\n3 )2 99K\\n\\x00\\nP2\\nR\\n\\x015\\n×\\n\\x00\\nP2\\nR\\n\\x015\\n,\\n(X1, . . . , X5, A1, A2) 7−→(A1X1, . . . , A1X5, A2X1, . . . , A2X5).\\nIn the next section, we review common camera models and highlight algebraic\\nvision articles studying camera geometry. In the remaining sections, our focus'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 104}, page_content='returns to the joint camera map in (1.1): We will see that many computer vision\\nproblems can be formulated using the joint camera map – such as understanding\\nthe image of a shape in space or reconstructing a 3D shape from several images –\\nand are thus natural to study through the algebraic lens. The recent paper [1] gives\\na similar such unifying algebro-geometric framework for computer vision problems.\\n2. Camera models\\nCalibrated cameras. The camera model described in the previous section'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 104}, page_content='is known as the projective / uncalibrated pinhole camera . The calibrated pinhole\\ncamera model assumes that every camera is obtained from the standard pinhole\\ncamera in Figure 2 by translation and rotation. This means that every camera\\nmatrix A is of the form [ R | t] where R ∈ SO(3) is the relative rotation from the\\nstandard pinhole camera to the camera with matrix A and the relative translation\\ncan be read off from the vector t ∈ R3: the camera center c, which is the origin'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 104}, page_content='in Figure 2, is now c = −R⊤t (note that the vector ( c, 1)⊤ ∈ R4 spans the kernel\\nof the camera matrix [ R | t]). In particular, every calibrated pinhole camera has 6\\ndegrees of freedom (3 for R and 3 for t), whereas a projective pinhole camera has\\n11 degrees of freedom.\\nCalibrated pinhole cameras are a commonly used model in applications, corre-\\nsponding to the case when the internal parameters of the cameras are known (such'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 104}, page_content='as from meta data stored inside the image file). There is also a variety of partly\\ncalibrated pinhole cameras, e.g. a camera with unknown focal length, that have less\\nstrict structural assumptions on the 3 ×4 camera matrices than the fully calibrated\\nmodel described above. Partly calibrated pinhole cameras are modeled as K[R | t]\\nwhere K is a 3 × 3 upper triangular calibration matrix whose entries are partially\\nknown [94, Chapter 6].'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 104}, page_content='known [94, Chapter 6].\\nDistortion. In practice, cameras are not as ideal as in the calibrated model.\\nAs seen in Figure 2, the pinhole cameras described so far assume that the world\\npoint, the camera center, and the image point are collinear. This assumption does\\nnot hold for real-life camera lenses, because they are affected by various kinds of\\ndistortion. The main factor of deviation from the idealistic pinhole camera model\\nis typically radial distortion; see Figure 3.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 104}, page_content='is typically radial distortion; see Figure 3.\\nOften, calibrated cameras are a sufficient approximation of real-life cameras.\\nHowever, sometimes the impact of radial distortion is too big, e.g., for fisheye\\ncameras. One approach to address radial distortion is to make the camera model\\nmore complicated by adding distortion parameters that have to be estimated during\\n3D reconstruction (see [94, Chapter 7.4] for an overview and [106] for an algebraic'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 104}, page_content='treatment of distortion varieties). Another approach is to simplify the camera model\\nby not estimating the radial distortion at all: Once the center of radial distortion on\\na given image is determined, we know for every 3D point onto which line through\\nthe distortion center it gets mapped by the camera (see Figure 3), although we'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 105}, page_content='6 JOE KILEEL AND KATHL ´EN KOHN\\nFigure 3. A grid affected by radial distortion. Radial lines and\\ndistortion center are blue.\\ndo not know its exact point in the image. Thus, the camera simplifies to a map\\nP3 99K P1 that sends 3D points to radial lines through the distortion center in the\\nimage. This is known as the 1D radial camera model [124, 173].\\nRolling shutter and pushbroom cameras. Many real-world cameras do\\nnot function like pinhole cameras that were described so far. For instance, smart-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 105}, page_content='phones, that produce a massive and yet growing amount of picture data, use rolling\\nshutters (see e.g., [53]). When taking a picture, these cameras scan across the scene,\\ncapturing the resulting image row by row. For static cameras taking pictures of still\\nscenes, there is no difference between rolling shutters and standard pinhole cameras\\nthat capture an entire image at the same instant (also known as global shutter).'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 105}, page_content='However, for moving cameras or objects, in particular videos, global shutter pin-\\nhole cameras are not a good approximation of rolling shutters. For example, when\\na rolling shutter camera moves with constant speed along a line, the picture of any\\nother line in three-space would typically be a conic in the image plane, whereas a\\npinhole camera maps lines to lines.\\nA related camera model is the pushbroom camera, which is common in satellites'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 105}, page_content='[94, Chapter 6.4.1]. Such a camera considers only a plane in the 3D world that\\npasses through the camera center and then projects that plane onto a single line.\\nHence, a non-moving pushbroom camera produces only an image line, not a plane.\\nTwo-dimensional images arise by moving the camera.\\nRolling shutter and pushbroom cameras have received little attention from the\\nalgebraic community so far. In particular, the theory described in Sections 3–7 is\\nlargely undeveloped for such cameras.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 105}, page_content='largely undeveloped for such cameras.\\nCongruences. Ponce, Sturmfels, and Trager [ 154] propose a general frame-\\nwork to study camera geometry that includes all camera models without distortion\\ndiscussed above. Their notion of a camera is an abstraction from the classical way\\nof modeling a camera by a map P3 99K P2. Instead of identifying a concrete im-\\nage plane with coordinates, they define a rational geometric camera to be a map'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 105}, page_content='P3 99K Gr(1, P3) from three-space into the Grassmannian of lines. The fiber over\\nany image point under a classical camera P3 99K P2 without distortion is a line,\\nnamely a viewing ray of the camera. The associated rational geometric camera\\nmaps each world point directly to its corresponding viewing ray. For instance, a\\ngeometric pinhole camera assigns to every world point distinct from the camera\\ncenter the unique line spanned by the world point and the camera center.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 105}, page_content='The image of a rational geometric camera is acongruence, i.e. a two-dimensional\\nfamily of lines. To obtain an actual photographic camera, one composes the geomet-\\nric camera map with a map from its image congruence to P2 that models the image'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 106}, page_content='SNAPSHOT OF ALGEBRAIC VISION 7\\nformation process via mapping viewing rays to image coordinates. These physical\\nrealizations of rational geometric cameras are the focus of Trager, Sturmfels, Canny,\\nHebert, and Ponce in [ 177].\\nIn [154], the authors suggest an even more general camera definition by saying\\nthat a geometric camera is an arbitrary congruence in Gr(1 , P3). The order of a\\ncongruence is the number of lines in the congruence that pass through a general'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 106}, page_content='point in P3. Hence, congruences of order one are exactly those that are the images\\nof rational geometric cameras P3 99K Gr(1, P3). Congruences of higher order are\\ngeometric cameras such that a world point typically appears several times on each\\nimage taken by the camera. This happens for instance for many moving rolling\\nshutter cameras, or for catadioptric cameras which are cameras that use curved\\nmirrors. The idea of modeling cameras by congruences had been expressed by'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 106}, page_content='computer vision researchers before [ 149, 152, 169].\\nChirality. In real life, a 3D point has to lie in front of a camera in order to be\\nseen by it. Imposing this semialgebraic constraint on a camera model is referred\\nto as chirality. The theoretical investigation of chirality was initiated by Hartley\\n[88]; see also [ 94, Chapter 21]. Further studies of this concept were for instance\\nundertaken by Laveau and Faugeras [ 125], Werner and Pajdla [ 187, 188, 185,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 106}, page_content='186], and Agarwal, Pryhuber, Sinn, and Thomas [ 4, 155].\\n3. Multiview varieties\\nGiven m fixed pinhole cameras C = (C1, . . . , Cm), where Ci : P3\\nR 99K P2\\nR, we\\ncan specialize the joint camera map Φ from (1.1) to\\nΦC : P3\\nR 99K (P2\\nR)m,(3.1)\\nsending a 3D point to its m images under the cameras. The joint image of the\\ncameras, first introduced by Triggs [ 178], is Φ C(P3\\nR). The first algebraic study of\\nthe joint image was done by Heyden and ˚Astr¨ om [96]. They called it the natural'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 106}, page_content='descriptor and observed that it is not Zariski closed. The Zariski closure MC :=\\nΦC(P3\\nR) of the joint image is called thejoint image variety [175] or multiview variety\\n[8]. It coincides with the Euclidean closure of the joint image [ 4, Theorem 4].\\n3.1. Multiview constraints. Many computer vision works have studied sys-\\ntems of polynomials that vanish on the joint image, often referred to as multiview\\nconstraints; see e.g. [ 74, 96, 73, 133, 8, 175, 5 ]. Their main motivation is that'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 106}, page_content='the joint image completely determines the cameras C1, . . . , Cm, up to changes of\\ncoordinates in P3. So one way of thinking about the 3D reconstruction pipeline\\nin Figure 1 is to first obtain point correspondences between the given images (i.e.,\\n(noisy) samples on the joint image) and then estimate sufficiently many multiview\\nconstraints from the correspondences so as to be able to reconstruct the cameras,\\nthen the 3D points.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 106}, page_content='then the 3D points.\\nWe now describe the multiview constraints studied by Heyden and ˚Astr¨ om\\n[96]. Let A1, . . . , Am be 3 × 4 matrices of rank three that define the cameras, i.e.,\\nCi : X 7→ AiX. We write xi = (xi1 : xi2 : xi3) for the homogeneous coordinates on'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 107}, page_content='8 JOE KILEEL AND KATHL ´EN KOHN\\nthe i-th image P2\\nR, and form the 3 m × (m + 4) matrix\\nMA = MA(x1, . . . , m) :=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nA1 x1 0 ··· 0\\nA2 0 x2 ··· 0\\n... ... ... ... ...\\nAm 0 0 ··· xm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb.\\nA tuple of image points ( x1, . . . , xm) ∈ (P2\\nR)m lies in the joint image if and only\\nif there is some 3D point X and non-zero scalars λi such that AiX = λixi for all\\ni = 1, . . . , m. This implies that [ X, −λ1, . . . ,−λm]⊤ is in the kernel of MA, so the'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 107}, page_content='matrix MA must be rank deficient. In fact, the multiview variety MC is cut out\\nby the maximal minors of MA:\\nMC = {(x1, . . . , xm) ∈ (P2\\nR)m | rk MA(x1, . . . , xm) < m+ 4}.(3.2)\\nThese maximal minors are multilinear, i.e. they are linear in every triplet xi of\\nvariables.\\nExample 3.1. For m = 2 cameras, MA is a 6 × 6 matrix. Its determinant\\ndefines the multiview hypersurface MC in P2\\nR × P2\\nR. This equation is a bilinear\\nform in x1, x2 represented by a matrix F ∈ R3×3, i.e. det( MA) = x⊤'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 107}, page_content='2 Fx1. Here F\\nis called the fundamental matrix for projective pinhole cameras, and the essential\\nmatrix for calibrated pinhole cameras. Longuet-Higgins’ seminal work [ 131] intro-\\nduced the essential matrix to the computer vision community along with a first 3D\\nreconstruction algorithm. We will discuss essential matrices, fundamental matrices,\\nand their generalizations to more than two cameras in Section 4.\\nFor every subset J ⊆ {1, . . . , m} of cardinality k, we consider the 3 k × (k + 4)'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 107}, page_content='submatrix MA,J of MA whose rows and columns correspond to the cameras Cj for\\nj ∈ J. As the maximal minors of MA,J are multilinear constraints in the k triplets\\nof variables {xj | j ∈ J}, we refer to the set of all maximal minors of all MA,J\\nwith |J| = k as the k-linearities. As seen in (3.2), the multiview variety MC of m\\ncameras is cut out by the m-linearities. Heyden and ˚Astr¨ om [96] showed that MC\\nis in fact cut out by the 2-linearities if the m camera centers are not coplanar.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 107}, page_content='An ideal-theoretic treatment of the multiview variety was first done by Aholt,\\nSturmfels, and Thomas [8], and later continued by Agarwal, Pryhuber, and Thomas\\n[5]. They study the vanishing ideal of MC, called the multiview ideal.\\nTheorem 3.2. (1) If the m camera centers are pairwise distinct, the mul-\\ntiview ideal is generated by the 2- and 3-linearities [5, Theorem 3.7].\\n(2) If all 4 × 4 minors of the 4 × 3m matrix [A⊤\\n1 A⊤\\n2 ··· A⊤\\nm] are non-zero,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 107}, page_content='1 A⊤\\n2 ··· A⊤\\nm] are non-zero,\\nthen the 2-, 3-, and 4-linearities form a universal Gr¨ obner basis of the\\nmultiview ideal [8, Theorem 2.1].\\nA nice overview of the joint image and its Zariski closure (equivalently, Eu-\\nclidean closure) is presented in the joint image handbook by Trager, Hebert, and\\nPonce [175]. They provide an explicit description of the actual joint image (instead\\nof its closure) as a constructible set, and discuss sets of k-linearities that cut out'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 107}, page_content='the multiview variety as well as sets of k-linearities that are not sufficient to define\\nMC but uniquely determine the cameras C1, . . . , Cm, up to coordinates changes\\nin P3\\nR.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 108}, page_content='SNAPSHOT OF ALGEBRAIC VISION 9\\n3.2. Euclidean distance degree and triangulation. Triangulation refers\\nto the problem of reconstructing the 3D coordinates of a point from 2D images\\nwhen the cameras are known. For pinhole cameras C = (C1, . . . , Cm), this means\\nto compute the fiber of the specialized joint camera map Φ C in (3.1). In theory,\\ntriangulation is trivial to solve for m ≥ 2 cameras, since the fiber under each\\ncamera Ci : P3\\nR 99K P2'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 108}, page_content='camera Ci : P3\\nR 99K P2\\nR is a line and so the desired 3D point is the intersection\\nof these m lines in P3\\nR. However, in practice, the image measurements are noisy,\\nwhich means that the given image points ( x1, . . . , xm) ∈ (P2\\nR)m do not lie on the\\nmultiview variety MC. Thus, to solve triangulation, one first needs to denoise the\\ngiven data, for instance by finding a nearby point on the multiview variety MC to\\nthe given data point x = (x1, . . . , xm) ∈ (P2\\nR)m.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 108}, page_content='R)m.\\nIn practical applications, the given image points are finite, which means that\\nafter a choice of affine chart R2 of each P2\\nR we may assume that x ∈ R2m. In this\\nsetting, we want to find the closest point x on the restriction of MC to the chosen\\naffine chart, that is, on the affine multiview variety :\\nMα\\nC = MC ∩ R2m.\\nTypically, distance is measured via the squared Euclidean distance on R2m. Re-\\ngarding Mα\\nC as a complex variety in C2m, for almost all data points x ∈ C2m,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 108}, page_content='this optimization problem has a constant number of complex critical points [ 61].\\nThat number is known as the Euclidean distance degree (ED degree for short) of\\nthe variety Mα\\nC. ED degree measures the algebraic complexity of the triangulation\\nproblem, in the sense that it provides an upper bound for the number of real critical\\npoints for generic data points x ∈ R2m.\\nHartley and Sturm [ 92] showed that the ED degree of the affine multiview\\nvariety Mα'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 108}, page_content='variety Mα\\n(C1,C2) for two cameras is 6. Stew´ enius and Nist´ er computed the ED\\ndegree of Mα\\nC for m ≤ 7 cameras and observed that it grows cubically [ 90]. The\\nproblem of determining the ED degree of the affine multiview variety actually mo-\\ntivated the general introduction of Euclidean distance degrees of algebraic varieties\\nby Draisma, Horobet ¸, Ottaviani, Sturmfels, and Thomas [61, Example 3.3], as well\\nas follow-up work on orthogonally invariant matrix sets [ 63, 62]. In [ 61, Conjec-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 108}, page_content='ture 3.4], a precise cubic formula for the ED degree of Mα\\nC for an arbitrary number\\nof cameras was conjectured. Harris and Lowengrub [ 86] provided an upper bound\\nfor the ED degree, showing that it is indeed bounded by some cubic polynomial in\\nthe number of cameras, before [ 61, Conjecture 3.4] was finally proven by Maxim,\\nRodriguez, and Wang:\\nTheorem 3.3 ([ 136]). When the m ≥ 2 cameras are in general position, the\\nEuclidean distance degree of the affine multiview variety Mα\\nC is\\n9\\n2m3 − 21'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 108}, page_content='C is\\n9\\n2m3 − 21\\n2 m2 + 8m − 4.\\nInformally, the theorem means triangulation with a large number of cameras may\\nbe quite difficult: the number of critical points grows cubically with m.\\n3.3. Extensions. There are several extensions of multiview varieties which\\nincorporate additional constraints or apply to different models. We discuss some\\nrecent extensions below.\\nChirality. The discussion of multiview varieties and triangulation above ig-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 108}, page_content='nores the constraint of chirality, i.e., that the 3D point must lie in front of each'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 109}, page_content='10 JOE KILEEL AND KATHL ´EN KOHN\\ncamera in order to be seen by it. Agarwal, Pryhuber, Sinn, and Thomas [4] provide\\nsemialgebraic descriptions (using polynomial equalities and inequalities) of both the\\nchiral domain, that is the set of points inP3\\nR that lie in front of each of them pinhole\\ncameras Ci, and the chiral joint image , i.e. the image of the chiral domain under\\nthe specialized joint camera map Φ C in (3.1). The chiral joint image is a subset of\\nTriggs’ joint image ΦC(P3'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 109}, page_content='Triggs’ joint image ΦC(P3\\nR) [178] and describes the true image of the 3D world as\\nseen by the cameras C = (C1, . . . , Cm). Hence, the semialgebraic description of the\\nchiral joint image from [ 4] is a refinement of the multiview constraints discussed\\nabove.\\nCongruences. Multiview varieties can be naturally generalized to other cam-\\nera models (besides pinhole cameras as described above). Recall that a rational\\ngeometric camera is a map\\nP3 99K Σ ⊂ Gr(1, P3)'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 109}, page_content='geometric camera is a map\\nP3 99K Σ ⊂ Gr(1, P3)\\nthat maps a world point X to a line passing through X. The Zariski closure of the\\nimage of the map is a congruence Σ, i.e. a surface in the Grassmannian Gr(1 , P3)\\nof lines. The map is defined everywhere except at its focal locus, that is, the set of\\npoints X ∈ P3 that belong to more than one line on Σ.\\nGiven m rational geometric cameras C = ( C1, . . . , Cm), Ci : P3 99K Σi ⊂'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 109}, page_content='Gr(1, P3), the multi-image variety MC is the Zariski closure of the image of\\nΦC : P3 99K Σ1 × . . .× Σm ⊂ Gr(1, P3)m, X 7−→(C1(X), . . . , Cm(X)) .\\nPonce, Sturmfels, and Trager [ 154] showed that the multi-image variety often has\\nan easy description in terms of the concurrent lines variety Vm ⊂ Gr(1, P3)m that\\nconsists of all m-tuples of lines that meet in a common point.\\nTheorem 3.4 ([154, Theorem 5.1]). If the m focal loci of the camerasC1, . . . , Cm'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 109}, page_content='are pairwise disjoint, then MC = Vm ∩ (Σ1 × . . .× Σm).\\nThe authors also describe a minimal set of generators and a reduced Gr¨ obner\\nbases for the prime ideal of the concurrent lines variety Vm in [154, Theorem 3.1].\\nMoreover, using the Pl¨ ucker embedding Gr(1, P3) ⊂ P5, they conjectured a formula\\nfor the multidegree of Vm in (P5)m, which was proven by Escobar and Knutson [68,\\nTheorem 2.4]. The latter theorem also provides the multidegree of the multi-image'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 109}, page_content='variety MC in (P5)m under the assumption that the m focal loci of the cameras\\nC1, . . . , Cm are pairwise disjoint.\\n3D data. Another way to generalize multiview varieties is by not only consid-\\nering the image of a single 3D point under the given cameras, but rather instead\\nconsidering the image of more complex 3D objects. The line multiview variety, i.e.,\\nthe Zariski closure of the image of the map Gr(1 , P3) 99K (Gr(1, P2))m that sends a'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 109}, page_content='3D line to its images under m pinhole cameras, is described in [ 35, 74]. Multiview\\nvarieties for more than one point are studied in [ 102]. For instance, the authors\\nthere consider the set of point pairs in R3 × R3 that are a Euclidean distance of 1\\napart from each other. Let X ⊂P3\\nR × P3\\nR be the Zariski closure of said set. Given\\nm pinhole cameras C = (C1, . . . , Cm), the authors restrict the joint camera map\\nfrom (1.1) to:\\nX 99K (P2\\nR)m × (P2\\nR)m, (X1, X2) 7−→(ΦC(X1), ΦC(X2)) ,(3.3)'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 109}, page_content='R)m, (X1, X2) 7−→(ΦC(X1), ΦC(X2)) ,(3.3)\\nand define the rigid multiview variety as the Zariski closure of the image of this\\nmap. They derive a set of polynomial equations cutting out the rigid multiview'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 110}, page_content='SNAPSHOT OF ALGEBRAIC VISION 11\\nvariety and give a conjecture for a minimal generating set of its prime ideal. The\\nauthors of [ 102] also describe defining polynomial equations for other variations\\nof multiview varieties, e.g. when X in (3.3) is instead taken to be an arbitrary\\nhypersurface in P3\\nR × P3\\nR, or when X is in ( P3\\nR)3 and consists of triples of points\\nwith fixed pairwise Euclidean distances, or when X is in ( P3\\nR)4 and consists of all'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 110}, page_content='R)4 and consists of all\\n4-tuples of points that lie on a common plane in P3\\nR.\\nHigher dimensions. Several works have studied higher-dimensional gener-\\nalizations of the specialized joint camera map in (3.1), i.e., maps from a projec-\\ntive space onto a product of projective spaces, all of arbitrary dimension, e.g.\\n[128, 100, 49 ]. The study of higher-dimensional pinhole cameras is motivated\\nby capturing dynamic scenes . We will sketch this application and some higher-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 110}, page_content='dimensional extensions of multiview varieties in Section 4.3.5.\\n4. Understanding camera tuples\\n4.1. Symmetry of joint camera map. The joint camera map Φ : X ×\\nCm 99K Y often carries a symmetry: depending on the specific camera models and\\n3D object types, there exists a group G acting on the fibers of Φ. This symmetry\\narises by simultaneously acting on the cameras and the 3D objects via 3D change\\nof coordinates which preserve the camera model and 3D objects. This has the'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 110}, page_content='important implication that in 3D reconstruction (such as structure-from-motion)\\nwe cannot hope to precisely recover a world scene ( X, (C1, . . . , Cm)) from its 2D\\nimage data Φ( X, (C1, . . . , Cm)). Instead the best we can reconstruct is the world\\nscene up the action of G.\\nFor example, if Cm = ( PMat3×4\\n3 )m consists of tuples of projective pinhole\\ncameras and X = (P3\\nR)n consists of tuples of points in 3D projective space, then'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 110}, page_content='the group G is the projective general linear group G = PGL(4, R). Indeed, note\\nAx = (Ag)(g−1x)\\nfor each A ∈ Mat3×4\\n3 , x ∈ P3 and g ∈ PGL(4, R). In this case, we can only aim to\\nrecover the world scene up to a projective transformation.\\nAs another example, suppose that that Cm consists of tuples of calibrated cam-\\neras ([R1|t1], . . . ,[Rm|tm]) where Ri ∈ SO(3) and ti ∈ R3. Take X = (P3)n again\\nto consist of tuples of 3D projective points. Now the relevant group is\\nG =\\n\\x1a\\ng ∈ GL(4, R) : g =\\n\\x14R t'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 110}, page_content='G =\\n\\x1a\\ng ∈ GL(4, R) : g =\\n\\x14R t\\n0 λ\\n\\x15\\nfor some R ∈ SO(3), t∈ R3, λ∈ R \\\\ {0}\\n\\x1b\\n,\\n(4.1)\\ni.e., the scaled special Euclidean group of R3 (also known as the similarity group).\\nThis is because G is the largest subgroup of 3D changes of coordinates which maps\\ncalibrated cameras to calibrated cameras. the world scene is ambiguous up to a\\nproper rigid motion R, and a central scale λ, t So when we know the cameras are\\ncalibrated, the world scene is ambiguous up to a proper rigid motion ( R, t) and a'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 110}, page_content='central scale (λ).\\nA novel approach to detect all symmetries in a given 3D reconstruction problem\\nhas been recently developed by Duff, Korotynskiy, Pajdla, and Regan [ 66]. Their\\napproach is based on based on Galois / monodromy groups , and uses numerical\\nhomotopy continuation computations (cf. Section 6.2 below).'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 111}, page_content='12 JOE KILEEL AND KATHL ´EN KOHN\\n4.2. Camera configurations. To cope with such ambiguities, it has been\\nuseful in computer vision to consider camera tuples modulo the relevant symme-\\ntries. Working out camera tuples modulo symmetries can be viewed as one of the\\naccomplishments of multiview geometry [ 94]. From a mathematical standpoint,\\nAholt and Oeding observed in [7] that formally we can regard the space of possible\\ncamera configurations as the GIT quotient:\\nCm/ /G.(4.2)'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 111}, page_content='Cm/ /G.(4.2)\\nHere GIT stands for geometric invariant theory, developed by Mumford [141]. The\\nGIT quotient Cm/ /Gis an abstract, non-embedded scheme or stack. While this GIT\\ndescription can be made precise to the abstract algebraic geometer, it would not be\\nuseable in 3D reconstruction problems. The perspective does tell us that multiview\\ngeometry found certain explicit birational models for these GIT quotients, which\\nare useful for 3D reconstruction.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 111}, page_content='are useful for 3D reconstruction.\\n4.3. Multifocal tensors. Camera configurations are represented by multi-\\nfocal tensors in multiview geometry. We present them for configurations of two,\\nthree and four cameras, and then give a higher-dimensional construction called\\nGrassmann tensors [91] motivated by dynamic scenes.\\n4.3.1. Two projective cameras: fundamental matrices . Fix two projec-\\ntive pinhole cameras C1, C2 : P3 99K P2 represented by matrices A1, A2. Consider'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 111}, page_content='the set of corresponding point pairs defined as the multiview variety:\\nMC =\\n\\x08\\n(x1, x2) ∈ P2 × P2 : ∃X ∈ P3 s.t. C1(X) = x1, C2(X) = x2\\n\\t\\n.\\nThis set is invariant under the action of G = PGL(4 , R) on C = ( C1, C2), i.e.\\nit depends only on the configuration C modulo G. On the other hand, MC is\\ngenerically a hypersurface of bidegree (1 , 1) as explained in Example 3.1:\\nMC =\\n\\x1a\\n(x1, x2) : ∃X ∈ P3 ∃λ1, λ2 ∈ R s.t.\\n\\x14\\nA1 x1 0\\nA2 0 x2\\n\\x15\" X\\n−λ1\\n−λ2\\n#\\n= 0\\n\\x1b\\n=\\n\\x1a\\n(x1, x2) : det\\n\\x14\\nA1 x1 0\\nA2 0 x2\\n\\x15\\n= 0\\n\\x1b\\n.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 111}, page_content='\\x1b\\n=\\n\\x1a\\n(x1, x2) : det\\n\\x14\\nA1 x1 0\\nA2 0 x2\\n\\x15\\n= 0\\n\\x1b\\n.\\nThis equation for MC may be written as x⊤\\n2 Fx1 = 0 where for 1 ≤ i, j≤ 3,\\nFij = (−1)i+j det\\n\\x14A1,ˆj\\nA2,ˆi\\n\\x15\\n,\\nwhere A1,ˆj denotes A1 with the jth row dropped and likewise for A2,ˆi. One regards\\nF as in P(Mat3×3), that is, defined only up to nonzero scale. In multiview geometry,\\nF is called the fundamental matrix for the pair of cameras C. It may be shown\\nthat F generically determines C modulo G [94], thus F furnishes an explicit model'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 111}, page_content='for the configuration. The collection of all F arising in this way is exactly the\\nset of rank 2 matrices, P(Mat3×3\\n2 ), with Zariski closure given by the determinantal\\nvariety {F ∈ P(Mat3×3) : det(F) = 0}. The construction is practically useful in 3D\\nreconstruction because point pairs in MC can be detected in real-life 2D images.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 112}, page_content='SNAPSHOT OF ALGEBRAIC VISION 13\\n4.3.2. Two calibrated cameras: Essential matrices . We can restrict the\\nfundamental matrix construction to the case of two calibrated cameras. The restric-\\ntion yields a subset of the fundamental matrices, whose points are called essential\\nmatrices and customarily denoted by E in computer vision. Demazure [ 57] deter-\\nmined that the resulting subvariety of the determinantal variety is 5 dimensional,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 112}, page_content='degree 10, with ideal minimally generated by the following ten cubic equations:\\n(4.3) det( E) = 0 and EE⊤E − 1\\n2 trace(EE⊤)E = 0.\\nAnother description is that essential matrices E ∈ P(Mat3×3) are those with rank 2\\nwhose top singular values are equal, i.e. σ1(E) = σ2(E) > σ3(E) = 0. Yet another\\ndescription is that, after taking the complex Zariski closure, the variety of essential\\nmatrices can be realized as a complex hyperplane section of the determinantal'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 112}, page_content='variety of rank ≤ 2 symmetric 4 × 4 matrices [77].\\nHere E only depends on C modulo the relevant group, i.e. the scaled special\\nEuclidean group G (4.1). In this case however, the association of a calibrated\\nconfiguration to an essential matrix is generically two-to-one (cf. twisted pairs in\\n[94]). Thus the GIT quotient (4.2) gives a degree-2 cover of the variety of essential\\nmatrices. The advantage of essential matrices is that they capture corresponding'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 112}, page_content='image point pairs which can be detected in real-life images.\\n4.3.3. Three cameras: Trifocal tensors . Given three projective cameras\\nC1, C2, C3 : P3 99K P2 represented by matrices A1, A2, A3, consider the set of\\ncorresponding point-line-line triples:\\nΨC =\\n\\x08\\n(x1, ℓ2, ℓ3) ∈ P2 × (P2)∗ × (P2)∗ :\\n∃X ∈ P3 s.t. C1(X) = x1, C2(X) ∈ ℓ2, C3(X) ∈ ℓ3\\n\\t\\n,\\nwhere (P2)∗ is the dual projective plane consisting of lines in P2. It then holds\\nΨC =\\n\\x1a\\n(x1, ℓ2, ℓ3) : ∃X ∈ P3 ∃λ1 ∈ R s.t.\\n\\uf8ee\\n\\uf8f0\\nA1 x1\\nℓ⊤\\n2 A2 0\\nℓ⊤'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 112}, page_content='\\uf8ee\\n\\uf8f0\\nA1 x1\\nℓ⊤\\n2 A2 0\\nℓ⊤\\n3 A3 0\\n\\uf8f9\\n\\uf8fb\\n\\x14\\nX\\n−λ1\\n\\x15\\n= 0\\n\\x1b\\n=\\n\\x1a\\n(x1, ℓ2, ℓ3) : det\\n\\uf8ee\\n\\uf8f0\\nA1 x1\\nℓ⊤\\n2 A2 0\\nℓ⊤\\n3 A3 0\\n\\uf8f9\\n\\uf8fb = 0\\n\\x1b\\n,\\nwhere we have identified ℓi with its normal vector of 3 homogeneous coordinates.\\nThe equation for Ψ C may be written using tensor contraction as T(x1, ℓ2, ℓ3) = 0\\nwhere T ∈ P(R3×3×3) is given by\\nTijk = ( −1)i+j+k det\\n\\uf8ee\\n\\uf8f0\\nA1,i\\nA2,j\\nA3,ˆk\\n\\uf8f9\\n\\uf8fb\\nfor 1 ≤ i, j, k≤ 3. Here A1,i denotes the ith row of A1, likewise for A2,j, and A3,ˆk'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 112}, page_content='denotes A3 with the kth row removed. In multiview geometry T is known as the\\ntrifocal tensor associated with the camera triple C. The map from ( C1, C2, C3)\\nmodulo PGL(4 , R) to T is generically one-to-one, so the Zariski closure of the\\nset of trifocal tensors is a birational model for configurations of three (projective)\\ncameras. As proven in [ 7], the trifocal tensors variety has dimension 18, degree 297\\nand an ideal minimally generated by 10 polynomials in degree 3, 81 polynomials'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 113}, page_content='14 JOE KILEEL AND KATHL ´EN KOHN\\nin degree 5, and 1980 polynomials in degree 6. Trifocal tensors are relevant to\\n3D reconstruction, because of their relation to image correspondence data. They\\ncapture point-line-line correspondences by definition via tensor-vector contraction.\\nHowever it is also true that trifocal tensors capture other correspondence types (e.g.,\\npoint-point-point) via other multilinear algebraic operations, see [ 94, Part III] for\\ndetails.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 113}, page_content='details.\\nIf we restrict ( C1, C2, C3) to calibrated pinhole cameras, then we obtain only\\ncertain trifocal tensors and a subvariety called the calibrated trifocal variety in\\n[105]. The calibrated trifocal variety is a birational model for the GIT quotient of\\ntriples of calibrated cameras modulo the scaled special Euclidean group G (4.1).\\nCurrently the ideal defining the variety is partially understood [ 135].\\n4.3.4. Four cameras: Quadrifocal tensors . A similar construction can be'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 113}, page_content='carried out for four projective pinhole cameras C1, C2, C3, C4 based on correspon-\\ndences between quadruples of image lines:\\n\\x08\\n(ℓ1, ℓ2, ℓ3, ℓ4) ∈ (P2)∗ × (P2)∗ × (P2)∗ × (P2)∗ :\\n∃X ∈ P3 s.t. C1(X) ∈ ℓ1, C2(X) ∈ ℓ2, C3(X) ∈ ℓ3, C4(X) ∈ ℓ4\\n\\t\\n.\\nSimilarly to the above, this is a hypersurface defined by Q(ℓ1, ℓ2, ℓ3, ℓ4) = 0 where\\nQ ∈ P(R3×3×3×3) is given by\\nQijkl = ( −1)i+j+k+l det\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nA1,i\\nA2,j\\nA3,k\\nA4,l\\n\\uf8f9\\n\\uf8fa\\uf8fb\\nThe tensor Q is known as the quadrifocal tensor in multiple view geometry. The'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 113}, page_content='ideal of the Zariski closure of quadrifocal tensors was studied up to degree 9 in\\n[145]. Quadrifocal tensors for calibrated cameras have not been investigated.\\n4.3.5. Dynamic scenes. The fundamental matrix and the trifocal and quadri-\\nfocal tensors (and their calibrated counterparts) are used for the reconstruction of\\nstatic scenes. There are many approaches for the reconstruction of dynamic scenes,\\ni.e., where the 3D points undergo different motions. Here we highlight two algebraic\\nideas.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 113}, page_content='ideas.\\nWolf and Shashua [ 189] and Huang, Fossum and Ma [ 99] argue that many\\nmotions of 3D points can be modeled by embedding them into a larger dimensional\\nspace such that the camera map becomes a projection of the form PN 99K P2. This\\nmotivated Hartley and Schaffalitzky [91] to construct the Grassmann tensor, which\\nwe explain in Section 4.3.6 below. For instance, a 3D point X = (X1, X2, X3, 1)⊤\\nthat moves linearly in an affine chart of P3 with constant velocity in the direction'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 113}, page_content='V = (V1, V2, V3, 0)⊤ can be embedded into P6 as follows: ¯X = (V1 : V2 : V3 : X1 :\\nX2 : X3 : 1)⊤. If Ai denotes the 3 ×4 matrix representing a moving pinhole camera\\nat time i, the image taken of the moving point X at time i is Ai(X + iV ). Writing\\nA′\\ni for the left-most 3 × 3 submatrix of Ai and ¯Ai = [ iA′\\ni | Ai] ∈ R3×7, we can\\nexpress the image as Ai(X + iV ) = ¯Ai ¯X. Hence, the camera map is a projection\\nP6 99K P2.\\nAnother algebraic approach to dynamic scenes was suggested by Vidal and'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 113}, page_content='coauthors [ 184, 72 ]. Suppose that a scene observed by a pinhole camera has\\nn independently moving objects and that we take two pictures of the scene at\\ntwo different times. For each of the n objects, we have a fundamental matrix Fi'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 114}, page_content='SNAPSHOT OF ALGEBRAIC VISION 15\\ndescribing the relative pose of the object at the two instants of time. Any pair of\\nimage points ( x1, x2) coming from a common 3D point must lie on one of the n\\nobjects and thus satisfies the equation (x⊤\\n2 F1x1) ··· (x⊤\\n2 Fnx1) = 0. As this equation\\nis homogeneous of degreen in both x1 and x2, we can apply the Veronese embedding\\nνn : P2 → PN , N=\\n\\x00n+2\\n2\\n\\x01\\n−1, to obtain the following bilinear equation in ν(x1) and'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 114}, page_content='ν(x2): ν(x2)⊤Fν (x1) = 0, where F ∈ RN×N is called the multibody fundamental\\nmatrix. This construction has been generalized, e.g. to multibody trifocal tensors\\n[93] and other motion models [182]. Finally, we note that 3D motion segmentation\\nfrom m views, i.e., the problem to determine the number n of moving objects\\nand which given m-tuples of image points belong to which object, can also be\\nsolved with generalized principal component analysis (GPCA). GPCA is a method'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 114}, page_content='considered by Vidal, Ma, and Sastry to fit an unknown number of linear subspaces\\nof unknown dimensions to given data points in a real vector space [ 183]. Other\\nalgebraic methods for GPCA include [ 180].\\n4.3.6. Grassmann tensors. Here we review the Grassmann tensor construc-\\ntion of Hartley and Schaffalitzky [ 91] that unifies multifocal tensors with higher-\\ndimensional generalizations. Consider PN , noting that N >3 can model dynamic'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 114}, page_content='scenes as in Section 4.3.5. Let Ci : PN 99K Pni , X 7→ AiX be m surjective linear\\nprojections. Assume m ≤ N + 1 ≤ Pm\\ni=1 ni. Then we can fix integers c1, . . . , cm\\nsatisfying 1 ≤ ci ≤ ni and c1 + . . .+ cm = N + 1. The special cases ni = 2 and\\nni = 1 appear in pinhole cameras and 1D radial cameras [ 124, 173], respectively.\\nWrite Gr(n−c, Pn) for the Grassmannian of linear subspaces of Pn of codimen-\\nsion c. The main object is the set of “corresponding subspace tuples”:'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 114}, page_content='(4.4) {(L1, . . . , Lm) ∈ Gr(n1 − c1, Pn1 ) × . . .× Gr(nm − cm, Pnm ) :\\n∃X ∈ PN s.t. C1(X) ∈ L1, . . . , Cm(X) ∈ Lm}.\\nThe authors of [ 91] proved that (4.4) is an algebraic hypersurface; moreover its\\ndefining equation is multilinear in the Pl¨ ucker coordinates for the Grassmannians.\\nThus, the coefficients of defining equation form an m-way tensor\\nT = T(C1,...,Cm),(c1,...,cm)\\nof size\\n\\x00n1+1\\nc1\\n\\x01\\n× . . .×\\n\\x00nm+1\\ncm\\n\\x01\\n. One calls T the Grassmann tensor for the projec-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 114}, page_content='tions (C1, . . . , Cm) with respect to the codimensions (c1, . . . , cm) [91] . The authors\\nproved that T depends only on ( C1, . . . , Cm) modulo PGL( N + 1, R). Further, if\\nat least ni strictly exceeds 1 then ( C1, . . . , Cm) modulo PGL(N + 1) is generically\\nuniquely determined by T. Fundamental matrices, trifocal tensors and quadrifo-\\ncal tensors correspond to N = 3 and ( c1, c2) = (2 , 2), ( c1, c2, c3) = (2 , 1, 1) and\\n(c1, c2, c3, c4) = (1, 1, 1, 1) respectively.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 114}, page_content='(c1, c2, c3, c4) = (1, 1, 1, 1) respectively.\\nThe construction of T can be derived here. For Li ∈ Gr(ni −ci, Pni ) we choose\\na basis of the dual space L∗\\ni ⊆ (Pni )∗ comprised of the hyperplanes containing Li;\\nlet the basis be\\nVi = {vi1, . . . , vici } ⊆(Pni )∗.\\nThe dual linear map C∗\\ni : (Pni )∗ → (PN )∗ pulls Vi back to (PN )∗ to give\\nA⊤\\ni Vi = {A⊤\\ni vi1, . . . , A⊤\\ni vici } ⊆(PN )∗.\\nThe requirement on X ∈ PN in (4.4) is ⟨AiX, vij⟩ = 0 for each i, j, or equivalently\\n⟨X, A⊤'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 114}, page_content='⟨X, A⊤\\ni vij⟩ = 0. Hence we require A⊤\\n1 V1 ∪ . . .∪ A⊤\\nmVm to not span ( PN )∗, i.e.\\n(4.5)\\n\\x02\\nA⊤\\n1 v11 . . . A⊤\\n1 v1c1 . . . . . . A⊤\\nmvm1 . . . A⊤\\nmvmcm\\n\\x03\\n∈ R(N+1)×(N+1).'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 115}, page_content='16 JOE KILEEL AND KATHL ´EN KOHN\\nis rank-deficient. The matrix (4.5) factors as\\n(4.6)\\n\\x02\\nA⊤\\n1 A⊤\\n2 . . . A⊤\\nm\\n\\x03\\n| {z }\\n=A⊤\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nV1 0 . . . 0\\n0 V2 . . . 0\\n... ... ... ...\\n0 0 . . .Vm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n| {z }\\n=V\\n.\\nWe require the determinant of (4.6) to vanish, which by the Cauchy-Binet formula\\nis X\\nI⊆[(n1+1)+...+(nm+1)]\\n#I=N+1\\ndet\\n\\x00\\nA⊤[:, I]\\n\\x01\\ndet(V[I, :]) .(4.7)\\nHere A⊤[:, I] denotes the ( N + 1) × (N + 1) submatrix consisting of the columns'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 115}, page_content='indexed by I, etc. Recalling that the rows in V containing Vi ∈ R(ni+1)×ci have\\nrank ci and that c1 + . . .+ cm = N + 1, we can restrict the sum in (4.7) as follows:\\nX\\nI1⊆[n1+1]\\n#I1=c1\\n. . .\\nX\\nIm⊆[nm+1]\\n#Im=cm\\ndet\\n\\x00\\nA⊤\\n1 [:, I1] . . . A⊤\\nm[:, Im]\\n\\x01 mY\\ni=1\\ndet (Vi[Ii, :]) .\\nAt this point, note det (Vi[Ii, :]) for Ii ⊆ [ni+1] are the dual Pl¨ ucker coordinates\\nof Li ∈ Gr(ni − ci, Pni ). These depend on the choice of basis Vi only through a'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 115}, page_content='global nonzero scale, and (after sign flips) equal the primal Pl¨ ucker coordinates of\\nLi up to scale. That is, letting Ui = {ui1, . . . , ui,ni−ci+1} ⊆Pni be a basis for Li\\none has\\n(4.8)\\n\\x10\\ndet(Vi[Ii, :]) | Ii ⊆ [ni + 1], #Ii = ci\\n\\x11\\n∝\\n\\x10\\n(−1)ci+PIi det(Ui[Ic\\ni , :]) | Ii ⊆ [ni + 1], #Ii = ci\\n\\x11\\n,\\nwhere Ic\\ni = [ni + 1] \\\\ Ii and PIi = P\\nℓ∈Ii ℓ.\\nIn view of this discussion, we define the entries of the Grassmann tensor as\\n(4.9) TI1,...,Im = ( −1)N+1+PI1+...+PIm det\\n\\x00\\nA⊤\\n1 [:, I1] . . . A⊤\\nm[:, Im]\\n\\x01'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 115}, page_content='\\x00\\nA⊤\\n1 [:, I1] . . . A⊤\\nm[:, Im]\\n\\x01\\n,\\nand conclude the following result:\\nTheorem 4.1 ([ 91]). Given surjective linear projections Ci : PN 99K Pni for\\ni = 1, . . . , mand ci ∈ Z such that 1 ≤ ci ≤ ni and c1 + . . .+ cm = N + 1, define the\\nset of corresponding subspaces by (4.4). Up to Zariski closure, this is an algebraic\\nhypersurface cut out in primal Pl¨ ucker coordinates byT in (4.9).\\n4.3.7. Beyond four cameras . When N = 3 and there are m > 4 pinhole'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 115}, page_content='cameras, a tensor-based model for camera configurations is no longer possible since\\nthere is no suitable tuple of codimensions ( c1, . . . , cm) for the Grassmann tensor\\nconstruction.\\nThe issue is that various embedded subvarieties that one may associate with the\\ncameras are not hypersurfaces, and thus not specified by a single equation. How-\\never models for camera configurations still exist inside appropriate Hilbert schemes.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 115}, page_content='Roughly speaking, a Hilbert scheme is the set of ideals of a fixed polynomial ring\\nwith a prescribed number of equations in each degree. In particular, one can asso-\\nciate (C1, . . . , Cm) modulo G to the ideal of its multiview variety MC, viewed as a'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 116}, page_content='SNAPSHOT OF ALGEBRAIC VISION 17\\npoint in a suitable Hilbert scheme [8]. Aholt, Thomas and Sturmfels [8] proved that\\nthis association is generically one-to-one if m >2. The Hilbert scheme approach\\nwas recently extended by Lieblich and Van Meter [ 130] via functorial methods.\\nTheir work has the benefit of incorporating calibrated cameras as well.\\nA rather different line of work encodes the configuration of m cameras through'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 116}, page_content='their viewing graph. The graph has m vertices, one for each camera. There is an\\nedge between two vertices if the relative position of the two corresponding cameras\\nis known. The edge is then labeled with the fundamental matrix of the camera pair.\\nIn most practical scenarios with many cameras, the viewing graph is incomplete.\\nThe structure of the graph then determines if the given fundamental matrices are\\ngenerically sufficient to recover the global camera configuration [ 176, 11].'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 116}, page_content='5. 3D reconstruction\\nIn this section, we discuss the role that multiview geometry plays in 3D re-\\nconstruction algorithms and how zero-dimensional polynomial systems arise. The\\nnext section will focus on methods that are used to solve these polynomials with\\nsufficient speed.\\n5.1. Structure-from-motion pipeline. We have introduced the structure-\\nfrom-motion problem already in Section 1. Here, we give more detailed algorithmic'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 116}, page_content='steps to solve this 3D reconstruction problem. Figure 4 overviews the usual incre-\\nmental structure-from-motion pipeline; see [26] for a full description.\\nImages Feature\\nextraction\\nFeature\\nmatching\\nGeometric\\nverification\\nReconstruction\\ninitialization\\nImage\\nregistration\\nTriangulation\\nBundle\\nadjustment\\nCorrespondence\\nsearch\\nIncremental\\nreconstruction\\nReconstructed features \\nand camera poses\\nFigure 4. Incremental structure-from-motion pipeline.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 116}, page_content='Feature extraction. The first step in the pipeline is to detect distinguished\\npoints or lines (or possibly other information such as conics) in each given image.\\nA standard algorithm is the Scale-Invariant Feature Transform (SIFT) due to Lowe\\n[132].\\nFeature matching. Next, the detected features are matched across different\\nimages; cf. Figure 1b. In particular, feature matching identifies which images show\\na common part of the scene.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 116}, page_content='a common part of the scene.\\nGeometric verification. Typically, not all feature matches from the feature\\nmatching step will be correct; many will be outliers. For two (or more) images'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 117}, page_content='18 JOE KILEEL AND KATHL ´EN KOHN\\nwith matched features, the geometric verification step tries to compute a geometric\\ntransformation that correctly maps sufficiently many features between the images.\\nIf such a transformation can be found, the pair (or tuple) of images and their\\ncommon features that could be mapped are considered to be geometrically verified.\\nFor instance, the fundamental (see Section 4.3.1) or essential matrix (see Section'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 117}, page_content='4.3.2) encode the relative pose of two pinhole cameras and thus provide the desired\\ntransformation between their two images. The output of the geometric verification\\nstep is the viewing graph whose nodes are images and whose (hyper-)edges connect\\ngeometrically verified image pairs (or tuples). The (hyper-)edges are labeled with\\nthe computed geometric transformations. See Section 4.3.7 for more on the viewing\\ngraph.\\nSince geometric verification forgets the original images and only works with'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 117}, page_content='the feature matches from feature matching (see Figure 1c), its core is an algebro-\\ngeometric problem that amounts to solving systems of polynomial equations. How-\\never, due to the presence of outliers, this step must use robust estimation techniques,\\nusually RANSAC (Random Sample Consensus). More details are in Section 5.2.\\nReconstruction initialization. This step picks a starting pair (or tuple)\\nof geometrically verified images (typically, in a dense area of the viewing graph),'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 117}, page_content='marks them as registered, and reconstructs the 3D coordinates of their matched\\nfeatures (as shown in Figure 1c). Note that the relative pose of the cameras was\\nalready precomputed in the previous step.\\nImage registration. This is the first step of the incremental reconstruction\\nphase. It registers one new image that is connected with previously registered\\nimages in the viewing graph by (re-)calculating its camera pose relative to the re-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 117}, page_content='constructed camera poses of the registered images. This can be either done from\\nmatched 2D features (as in the geometric verification step) or from 2D-3D cor-\\nrespondences if some of the already reconstructed 3D features are detected 2D\\nfeatures on the new image. Both ways solve a system of polynomial equations, but\\nthe latter problem – known as Perspective-n-Point (PnP) if all detected features\\nare points – can be typically solved faster.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 117}, page_content='are points – can be typically solved faster.\\nTriangulation. This step reconstructs the 3D coordinates of all features that\\nthe newly registered image has in common with any of the previously registered im-\\nages (except, of course, for those features whose 3D coordinates were reconstructed\\nbefore); see Section 3.2.\\nBundle adjustment. This step refines the 3D features and camera poses\\nreconstructed so far to minimize the accumulation of errors in the incremental'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 117}, page_content='reconstruction phase; see [ 179] for more details. Afterward, a new image will\\nbe registered and the incremental reconstruction phase repeats until (almost) all\\ncameras and sufficiently many 3D features are reconstructed to approximate the\\n3D scene. Bundle adjustment is a very important step in the pipeline, but a fuller\\naccount falls outside our scope. See [147] for another survey article that emphasizes\\nnumerical optimization aspects in structure-from-motion more ours does.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 117}, page_content='5.2. Geometric verification. The algebro-geometric problems within the\\nstructure-from-motion pipeline aim to reconstruct the relative camera positions\\nlocally. That is, we seek the relative pose for only a few (e.g., two) of the given\\nimages at a time, based on feature matches between the images. A salient issue is\\nthat some of the purported feature matches will be completely erroneous. Indeed\\nfeature extraction/matching methods applied to real images invariably produce a'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 118}, page_content='SNAPSHOT OF ALGEBRAIC VISION 19\\nnontrivial fraction of incorrect matches; and the problem becomes severe for scenes\\nwith repeated textures (e.g., a brick wall), among others. Therefore reconstruction\\nmethods must be robust to outliers. A general paradigm for estimation in the pres-\\nence of outliers was introduced by Fischler and Bolles [ 76], who were motivated by\\nthe computer vision setting. Their Random Sample Consensus (RANSAC) and its'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 118}, page_content='subsequent variants remain popular today for computing local reconstructions.\\nTo illustrate RANSAC and show how it motivates solving zero-dimensional\\npolynomial systems in vision, we discuss essential matrix estimation. In this exam-\\nple, data consists of several purported point matches\\n(x1, y1), . . . ,(xn, yn) ∈ P2\\nR × P2\\nR.\\nLet E ⊂P(Mat3×3) be the variety of essential matrices defined by (4.3). We seek\\nE ∈ Ewhich best fits the veridical point pairs amongst the data. By construction,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 118}, page_content='if (xi, yi) is a correct (and noiseless) image point pair and E is the ground-truth\\nessential matrix, then\\ny⊤\\ni Exi = 0.\\nNotice that this constraint on E is linear and of codimension 1. Thus given\\ndim(E) = 5 such constraints, we expect up to deg( E) = 10 real solutions for E.\\nHowever it is unknown which of the purported points pairs are veridical. RANSAC\\nachieves robust estimation by randomly sampling a subset I ⊆[n] = {1, . . . , n} of'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 118}, page_content='five point pairs and fitting to them exactly by solving the polynomial system:\\n(5.1) det( E) = 0, EE ⊤E − 1\\n2 trace(EE⊤)E = 0, y ⊤\\ni Exi = 0 ( i ∈ I).\\nLetting E1, . . . , Es be the real solutions, RANSAC checks if there exists a consensus\\namongst the other n − 5 point pairs as to whether any of the solutions agree with\\nmost of the other point pairs. RANSAC re-samplesI until a consensus is found. We\\nnote that this involves a choice of error metric to measure the agreement between'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 118}, page_content='Ek and ( xi, yi) for i ∈ I. A simple choice is ( y⊤\\ni Ekxi)2 often called algebraic\\nerror, but often the geometric reprojection error or its first-order approximation\\ncalled Sampson error are preferred [ 94]. We also remark that popular variants of\\nRANSAC, like LO-RANSAC [48], polish the winning Ek by using all point pairs in\\n[n] that agree with Ek to form a non-linear least squares cost which is minimized\\nstarting from Ek.\\n5.3. Minimal problems. The polynomial system (5.1) is called a minimal'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 118}, page_content='problem in the vision literature, reflecting the fact that (5.1) uses the minimal\\namount of data such that the solution is uniquely determined up to finitely many\\nsolutions. More generally, minimal problems are 3D reconstruction problems such\\nthat random input instances have a finite positive number of solutions. Formally,\\na structure-from-motion problem is minimal if the associated joint camera map\\nΦ : X × Cm 99K Y over the complex numbers has generically finite non-empty'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 118}, page_content='fibers, modulo the group G that acts on the fibers. In other words, for a generic\\n(complex) y ∈ Y, the fiber Φ −1(y) is non-empty and consists of finitely many\\nG-orbits.\\nBy choosing appropriate coordinates for camera configurations modulo the\\ngroup G (for example, by using the multifocal tensors in Section 4.3), a mini-\\nmal problem can be cast as a zero-dimensional parameterized polynomial system.\\nThus one expects that a minimal problem can be solved efficiently, as long as its'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 118}, page_content='generic number of complex solutions is sufficiently small. This generic number of'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 119}, page_content='20 JOE KILEEL AND KATHL ´EN KOHN\\ncomplex solutions – known as the algebraic degree of the minimal problem – is a\\nmeasure of the minimal problem’s intrinsic difficulty.\\nExample 5.1. Reconstructing five points in general position observed by two\\ncalibrated pinhole cameras (see Figure 1c) is a minimal problem. This is because,\\ngenerically, the system (5.1) has 10 complex solutions (i.e., essential matrices), so\\n20 complex pairs of cameras (modulo the scaled special Euclidean group), and for'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 119}, page_content='each camera pair there are five unique 3D points mapping to the given image points.\\nNote that the number of real solutions to (5.1) depends on the image data. The real\\ncount is generically amongst{0, 2, 4, 6, 8, 10}, but varies with the specific five points.\\nIn particular, understanding when there exists at least a real solution is subtle [ 3].\\nAn even more difficult problem is to determine when there is at least one chiral'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 119}, page_content='reconstruction, i.e. a real solution where the 3D points lie in front of the cameras\\n[155]. If we are given six points in two calibrated views, the system (5.1) no longer\\ncorresponds to a minimal problem, as generically there is no complex solution. We\\nneed the image data to lie on the Chow hypersurface in order for (5.1) to be soluble;\\nsee [77] for an explicit Pfaffian formula defining the Chow hypersurface obtained'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 119}, page_content='using Ulrich sheaves, or [29] for a simpler construction based on Jordan algebras.\\nThe literature on minimal problems is vast. Many minimal problems have been\\ndescribed, solvers have been developed for them, and new minimal problems are\\nconstantly appearing; see e.g. [ 101, 146, 119, 144, 167, 156, 67, 115, 117,\\n181, 163, 126, 43, 14, 13, 139, 192, 118, 69 ].\\nThere are also many variations of the above definition. For instance, when'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 119}, page_content='solving 3D reconstruction by RANSAC as described in Section 5.2, one often aims to\\nrecover the camera parameters only. Hence, often a structure-from-motion problem\\nis referred to as minimal if for a generic image tuple y ∈ Y, the projection of the\\nfiber Φ −1(y) ⊂ X × Cm onto the space Cm of cameras is non-empty and finite\\nmodulo G. In the following, we call such problems camera-minimal and reserve\\nthe term minimal for when we aim to recover the 3D structure in X as well (as in'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 119}, page_content='the definition before Example 5.1). Note that every minimal problem is camera-\\nminimal but not vice versa.\\nExample 5.2. The following problem is camera-minimal (shown in [105] where\\nit is called 3PPP + 1PPL): Reconstruct four points X1, X2, X3, X4 ∈ P3 and a line\\nL ⊂ P3 incident to X4 that are observed by three calibrated pinhole cameras such\\nthat the first two cameras see only the four points and the third camera only sees'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 119}, page_content='X1, X2, X3 and the line L; see Figure 5. In fact, it typically has 272 complex\\nsolutions in terms of camera parameters. However, this problem is not minimal\\nsince the line L cannot be recovered uniquely when the three cameras are known:\\nThere is a one-dimensional family of lines that yield the same image line in the\\nthird view.\\nIn standard structure-from-motion reconstructions, (camera-)minimal prob-\\nlems need to be solved many times to ensure RANSAC finds a solution that is'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 119}, page_content='not corrupted by outliers. Thus, minimal solvers need to be fast. Furthermore,\\nthey should be accurate, since numerical errors could jeopardize RANSAC’s suc-\\ncess as well. We also would like to handle various nearly-degenerate geometries,\\nsuch as near coplanarities amongst the five point pairs in (5.1), as such situations\\ndo arise in natural images. In Section 6, we review the symbolic and numerical\\nminimal solvers that are used to meet these needs.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 120}, page_content='SNAPSHOT OF ALGEBRAIC VISION 21\\nFigure 5. A camera-minimal problem with algebraic degree 272.\\nThe large zoo of minimal problems in the literature is motivated by the quest to\\nfind reconstruction problems that 1) admit a feasible minimal solver as described\\nabove, 2) their necessary input data can be detected by state-of-the-art feature\\nextraction methods, and 3) cover the wide variety of reconstruction applications in'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 120}, page_content='computer vision. The latter includes variations of the camera model (see Section\\n2), of the parameters to be reconstructed, or of the input data (also beyond the\\nstructure-from-motion problem we focus on here, e.g. camera pose estimation from\\nknown 3D and 2D data). We list some of the most classical minimal problems in\\nTable 1.\\nto be reconstructed minimal data degree references\\nessential matrix 5 point pairs 10 [143, 57]\\nfundamental matrix 7 point pairs 3\\n[87]\\nalready known in 19th'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 120}, page_content='[87]\\nalready known in 19th\\ncentury [95, 170]\\nrelative pose of 2 calibrated\\ncameras with unknown\\ncommon focal length\\n6 point pairs 15 [167]\\nabsolute pose of 1\\ncalibrated camera\\n(P3P, image registration)\\n3 world-image point pairs 4\\n[85]\\nalready known in 19th\\ncentury [84]\\nplanar homography 4 point pairs 1\\ncalibrated planar homography 4 point pairs 12 [64, 75]\\ntrifocal tensor 9 line triples 36 [122]\\ncalibrated trifocal tensor 3 point triples\\n+1 line triple 216 [105]\\nrelative pose of 2 projective'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 120}, page_content='relative pose of 2 projective\\ncameras with unkown\\nradial lens distortion\\n8 point pairs 16 [119]\\nworld point under noise known cameras with:\\n(triangulation, • 1 point pair • 6 [92]\\nreprojection error) • 1 point triple • 47 [168]\\nTable 1.Some representative minimal problems. Several of these\\nminimal problems are classical and integrated in vision pipelines.\\nEfforts have been undertaken to provide complete catalogs of some classes of'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 120}, page_content='minimal problems. Kileel [ 105] describes all 66 camera-minimal problems (incl.\\ntheir algebraic degrees) for three calibrated pinhole cameras that can be formulated\\nusing linear constraints on the trifocal tensor. Duff, Kohn, Leykin, and Pajdla [ 64]'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 121}, page_content='22 JOE KILEEL AND KATHL ´EN KOHN\\ncharacterize all 30 minimal problems (including their algebraic degrees) of recon-\\nstructing points, lines and their incidences for an arbitrary number of calibrated\\npinhole cameras under the assumption that all points and lines are observed by\\nevery camera. In a follow-up work allowing partial visibility [ 65], they classify all\\ncamera-minimal and minimal problems of three calibrated pinhole cameras observ-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 121}, page_content='ing point-line arrangements such that each line is incident to at most one point.\\nIn particular, there are 759 (equivalence classes of) camera-minimal problems with\\nalgebraic degree less than 300.\\n6. Polynomial solvers for minimal problems\\nIn this section, we discuss symbolic and numerical methods to construct solvers\\nfor minimal problems in computer vision, e.g. the minimal problems in Table 1.\\nAlgebraically, we model a minimal problem as azero-dimensional parameterized\\npolynomial system:'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 121}, page_content='polynomial system:\\n(6.1)\\n\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f3\\nf1(x1, . . . , xn; θ1, . . . , θt) = 0\\n...\\nfk(x1, . . . , xn; θ1, . . . , θt) = 0.\\nHere x = ( x1, . . . , xn) are variables, and θ = ( θ1, . . . , θt) are parameters (e.g.,\\ncoming from image point correspondences). The equations fi are polynomials in\\nboth x and θ, typically with integer coefficients (e.g., the epipolar constraints (5.1)).\\nWrite I for the ideal generated by the equations\\nI = {p1f1 + . . .+ pkfk | pi ∈ R[x1, . . . , xn; θ1, . . . , θt]}.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 121}, page_content='Let |θ denotes specialization to concrete parameter values θ ∈ Rt. Minimality of\\nthe problem translates to the following two assumptions:\\nA1) For generic θ, the ideal I|θ is zero-dimensional (i.e., has finitely many\\nroots);\\nA2) For generic θ, the ideal I|θ is radical (i.e., has no repeated roots)\\nThe conditions imply the existence of a positive integer NC = NC(I) such that\\nfor generic parameters θ, the number of C-solutions to (6.1) is NC. Furthermore,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 121}, page_content='each root has multiplicity 1. Note that NC is the algebraic degree of the minimal\\nproblem discussed in Section 5.3.\\nBy definition, a solver is meant to receive values for the parameter θ ∈ Rt, and\\nthen output the set of corresponding real solutions VR(I|θ) ⊂ Rn to (6.1). A salient\\nissue to bear in mind is the online vs. offline distinction in computer vision: When\\ngiven a specific parameter value θ, for many vision tasks we must run the solver in'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 121}, page_content='real-time. Therefore, online computations must be fast. On the other hand, when\\ndeveloping the solver, we are free to dedicate far more time and computational\\nresources offline.\\n6.1. Symbolic method: Elimination templates. Currently a symbolic\\nmethod is the most popular one to build minimal solvers in vision. At the heart\\nof the method is a classical fact that zero-dimensional polynomial systems can be\\nconverted to eigenvector problems.1'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 121}, page_content='converted to eigenvector problems.1\\n1A different symbolic approach, based instead on resultants, is being developed by Bhayani,\\nHeikkil¨ a and Kukelova [24, 25].'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 122}, page_content='SNAPSHOT OF ALGEBRAIC VISION 23\\nLemma 6.1. Let θ ∈ Rt, and the corresponding solutions to (6.1) be VC(I|θ) =\\n{v1, . . . , vNC } ⊂Cn. Let P ∈ R[x1, . . . , xn] be a polynomial separating the solutions,\\nthat is, P(vi) ̸= P(vj) for all i ̸= j. Let B = {b1, . . . , bNC } ⊆R[x1, . . . , xn] be\\nsuch that its reduction modulo I|θ is a vector space basis B = {b1, . . . ,bNC } of the\\nquotient ring R[x1, . . . , xn]/I|θ. Multiplication by P is a linear endomorphism on'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 122}, page_content='R[x1, . . . , xn]/I|θ, represented with respect to B by a matrix\\n(6.2) [mult( P)|θ]B ∈ RNC×NC , Pbj :=\\nNCX\\ni=1\\n([mult(P)|θ]B)ij bi.\\nThen [mult(P)|θ]B is diagonalizable with eigenvalue/left eigenvectors pairs given by\\n(P(vi), B(vi)) ∈ C × CNC for i = 1, . . . , NC.\\nIn [52], Cox gives an interesting history of Lemma 6.1, which is often (wrongly)\\nattributed to Stickelberger. We call P the action polynomial and [mult(P)|θ]B the'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 122}, page_content='action matrix. Notice that if the action matrix is available, then Lemma 6.1 shows\\nhow the solutions vi to (6.1) may be computed: from the eigenvectors wi of the\\naction matrix we require B(vi) to be proportional to wi. To make the recovery of\\nvi easy, typically B is chosen to consist of monomials.\\nThus the main challenge is to obtain the action matrix online. In principle, it\\nrequires computing identities of the form\\n(6.3) Pbj =\\nNCX\\ni=1\\n([mult(P)|θ]B)ij bi +\\nkX\\ni=1\\nqijfi|θ ∈ R[x1, . . . , xn],'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 122}, page_content='kX\\ni=1\\nqijfi|θ ∈ R[x1, . . . , xn],\\nfor j = 1 , . . . , NC where qij ∈ R[x1, . . . , xn] and ([mult( P)|θ]B)ij ∈ R. Gr¨ obner\\nbasis computations are too slow to be performed online, and not viable in floating\\npoint arithmetic anyways. Instead, the main tool is elimination templates.\\nFix choices of B ⊂R[x1, . . . , xn] and P ∈ R[x1, . . . , xn] such that:\\nA3) For generic θ, B ⊂R[x1, . . . , xn]/I|θ is a vector space basis;\\nA4) For generic θ, P separates VC(I|θ).'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 122}, page_content='A4) For generic θ, P separates VC(I|θ).\\nIn the present setting, elimination templates are defined as follows.\\nDefinition 6.2. A set E = {e1, . . . , eT } ⊂ I ⊆R[x1, . . . , xn; θ1, . . . , θt] is an\\nelimination template if A5) For generic θ, we have P · B ⊂span(B ∪ E|θ).\\nOften E is chosen to be x-monomials multiplied by f1, . . . , fk. By Definition 6.2,\\n(6.4) Pbj =\\nNCX\\ni=1\\n([mult(P)|θ]B)ij bi +\\nTX\\nℓ=1\\n⋆ eℓ|θ ∈ R[x1, . . . , xn]'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 122}, page_content='TX\\nℓ=1\\n⋆ eℓ|θ ∈ R[x1, . . . , xn]\\nfor j = 1, . . . , NC and scalars ⋆. Computing the action matrix thus becomes a linear\\nsystem. Specifically, fix C = {c1, . . . , cM } ⊂R[x1, . . . , xn] such that\\nA6) For generic θ, E|θ ⊂ span(B ∪ C).\\nOften C is all x-monomials appearing in any eℓ ∈ Ewith nonzero coefficient in\\nR[θ1, . . . , θt], not already in B. Form a matrix called the elimination template\\nmatrix, denoted [elim( P)|θ]B,E,C ∈ R(T+NC)×(NC+M), with rows indexed by E|θ ∪'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 122}, page_content='P · Band columns indexed by B ∪ C:'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 123}, page_content='24 JOE KILEEL AND KATHL ´EN KOHN\\n(6.5)\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nb1 . . . bNC c1 . . . cM\\ne1|θ\\n...\\neT |θ\\n⋆ ⋆\\nPb1\\n...\\nPbNC\\n⋆ ⋆\\n\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\n,\\nwhere each entry is the coefficient of the column in the row. Thanks to (6.4),\\nelementary row operations can reduce the elimination template matrix to the form\\n(6.6)\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nb1 . . . bNC c1 . . . cM\\ne1|θ\\n...\\neT |θ\\n⋆ ⋆\\nPb1\\n...\\nPbNC\\n[mult( P )|θ ]B 0\\n\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\n.\\nThe bottom-left block of (6.6) is the desired action matrix.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 123}, page_content='The eigenvectors and elimination templates method is summarized in Algo-\\nrithm 1. In the online part, the most costly step is computing the pseudo-inverse\\nof the top-right block of the elimination template matrix. In the offline phase, we\\nseek B, P,E, C such that the size of the elimination template matrix is as small as\\npossible.\\nFor the offline procedure, full details are beyond our scope but we mention some\\nof the highlights. A breakthrough was obtained by Kukelova, Bujnak and Pajdla'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 123}, page_content='in 2008, who showed how find elimination templates automatically, as opposed\\nto by hand [ 116]. Behind their work, a principle is that “shape” of symbolic\\ncalculations should be the same at generic parameters θ, and also preserved if we\\nreduce (6.1) modulo a prime. This enables traditional Gr¨ obner basis computations\\nin the offline phase. In [ 122] Larsson, ˚Astr¨ om and Oskarsson observed thatqij in\\n(6.3) are uniquely defined only up to the first syzygy module of ( f1|θ, . . . , fk|θ).'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 123}, page_content='Syzygy-reduction [ 122, Proposition 1] was shown to result in choices of E with\\nsubstantially smaller elimination template matrices for many minimal problems.\\nIn [123], Larsson, Oskarsson, ˚Astr¨ om, Wallis, Kukelova and Pajdla leveraged the\\nfact that the basis B need not be standard monomials with respect to some Gr¨ obner\\nbasis for I|θ, only condition A3 is required (see [ 140]). The authors chose B by\\na random monomial-sampling strategy, and showed the strategy can sometimes'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 123}, page_content='produce smaller elimination template matrices than achievable anywhere on the\\nGr¨ obner fan. Martyushev, Vrablikova and Pajdla further improved the state-of-\\nthe-art with a greedy search strategy to reduce the size of C in [134]. Integrating\\nmany of these ideas, Li and Larsson released code for building solvers in [ 129].'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 124}, page_content='SNAPSHOT OF ALGEBRAIC VISION 25\\nAlgorithm 1 Offline/online solver for the parameterized system (6.1)\\n1: Offline Input\\n⟨f1, . . . , fk⟩ ⊆R[x1, . . . , xn; θ1, . . . , θt] satisfying A1-A2.\\n2: Offline Precomputations\\nFind B ⊆R[x1, . . . , xn] satisfying A3,\\nP ∈ R[x1, . . . , xn] satisfying A4,\\nE ⊆R[x1, . . . , xn; θ1, . . . , θt] satisfying A5,\\nC ⊆R[x1, . . . , xn] satisfying A6.\\nCompute the bottom NC rows of (6.5), and\\ndenote these by\\n\\x02\\nE21 E22\\n\\x03\\n∈ RNC×(NC+M).\\n3: Online Input\\nGeneric θ ∈ Rt.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 124}, page_content='\\x03\\n∈ RNC×(NC+M).\\n3: Online Input\\nGeneric θ ∈ Rt.\\n4: Online Computations\\nEvaluate the top T rows of (6.5) at θ, and\\ndenote these by\\n\\x02E11 E12\\n\\x03\\n∈ RT×(NC+M).\\nThus [elim(P)|θ]B,E,C =\\n\\x14\\nE11 E12\\nE21 E22\\n\\x15\\n.\\nCompute E21 − E22E†\\n12E11 ∈ RNC×NC .\\nThis equals [mult(P)|θ]B.\\nCompute all real eigenvalue/eigenvector pairs\\n(λ1, w1), . . . ,(λr, wr) ∈ R × RNC of [mult(P)|θ]B.\\nDetermine v1, . . . , vr ∈ Cn such that\\nP(vi) = λi and B(vi) ∝ wi.\\n5: Online Output\\nVR(I|θ) = {v1, . . . , vr} ∩Rn.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 124}, page_content='5: Online Output\\nVR(I|θ) = {v1, . . . , vr} ∩Rn.\\n6.2. Numerical method: Homotopy continuation. An alternative method\\nfor building minimal problem solver is homotopy continuation. Homotopy continua-\\ntion [166] is a path-following, ODE-based numerical method to solve parametrized\\nzero-dimensional polynomial systems of a significantly different nature than the\\neigenvectors and elimination templates method described above. It uses only floating-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 124}, page_content='point arithmetic operations, and exploits the offline vs. online paradigm in a dif-\\nferent way.\\nIn the offline phase, homotopy continuation solves one generic instance of the\\nminimal problem (i.e., the polynomial system f(x; θ∗) = 0 for a fixed choice of\\nparameters). The offline solve is done effectively from scratch. Specifically, homo-\\ntopy continuation chooses a start system g(x) = 0 of polynomials with the same'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 124}, page_content='variables and degree structure as f(x; θ∗) = 0 that is trivial to solve (e.g., the start\\nsolutions are roots of unity). Then homotopy continuation interpolates from the\\nstart system to the minimal problem instance via linear interpolation:\\nh(x; t) = (1 − t)f(x; θ∗) + tg(x),'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 125}, page_content='26 JOE KILEEL AND KATHL ´EN KOHN\\nas t goes from 1 to 0. During the interpolation, we numerically track how the solu-\\ntions to g(x) = 0 evolve [55, 56]. In general, many solution paths diverge. However\\nthe paths with finite limits give all solutions to f(x; θ∗) = 0 [166, Theorem 7.1.1].\\nThe offline procedure is costly: its complexity is driven by the number of solutions\\nto g(x) = 0, often exponential in the number of variables.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 125}, page_content='In the online phase, homotopy continuation also interpolates between polyno-\\nmial systems. But now it tracks solutions of the precomputed minimal problem\\ninstance f(x; θ∗) = 0 to the solutions of the desired instance of the minimal prob-\\nlem (i.e., f(x; θ) = 0 for a different choice of parameters). The complexity of the\\nonline phase is driven by the number of solutions to f(x; θ0) = 0, rather than the\\nnumber of solutions to g(x) = 0. Consequently the online solve is much faster than'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 125}, page_content='the offline phase. Indeed, in the online solve we track only NC many paths, which\\nis the intrinsic algebraic degree of the minimal problem. General-purpose software\\nfor homotopy continuation includes Bertini [ 15], HomotopyContinuation.jl [ 36],\\nNumericalImplicitization.m2 [45], among others.\\nIn computer vision, homotopy continuation has been used to determine the\\nalgebraic degrees of (camera-)minimal problems [105, 64, 65]. Homotopy continu-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 125}, page_content='ation was also previously suggested as a solver for such problems [97, 114], however\\nfor many years it was not seen to be competitive with the elimination templates ap-\\nproach. Recently, homotopy continuation yielded the fastest solver for the minimal\\nstructure-from-motion problem shown in Figure 6. For this problem no elimina-\\ntion template matrix of reasonable size could be found by the symbolic method\\nof Section 6.1. However, with homotopy continuation a numerically stable solver'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 125}, page_content='was built that runs on the order of a second or less [ 69]. The minimal problem\\nin Figure 6 has algebraic degree NC = 312, so homotopy continuation tracks 312\\npaths per online solve. The solver was validated on real-data scenarios where the\\nstate-of-the-art pipeline COLMAP [164] failed to find enough point-based features,\\nwhich also highlights the importance of investigating (camera-)minimal problems\\nthat involve not only points but also lines.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 125}, page_content='that involve not only points but also lines.\\nMore recently, homotopy continuation has been combined with machine learn-\\ning to build competitive solvers for minimal problems of large algebraic degree:\\nHruby, Duff, Leykin, and Pajdla [ 98] achieve fast computational speeds by first\\nusing machine learning to pick a single start solution and then tracking that solu-\\ntion to a single solution of the desired target system. The start solution does not'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 125}, page_content='come from a fixed start system, but rather determines the start system used for\\nhomotopy continuation, meaning that the start system is also learned. Another\\nimportant advance has been to combine GPU computing with homotopy continua-\\ntion. This was recently carried out by Chien, Fan, Abdelfattah, Tsigaridas, Tomov\\nand Kimia [46] on various benchmark minimal problems.\\n7. Degeneracies and discriminants\\nThis section is devoted to degenerate configurations of 3D scenes and cameras.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 125}, page_content='Section 7.1 focuses on configurations that do not admit unique recovery from image\\ndata. Degenerate views of 3D curves and surfaces are discussed in Section 7.2. Fi-\\nnally, Section 7.3 outlines the connection between degenerate instances of minimal\\nproblems and condition numbers that measure how sensitive numerical reconstruc-\\ntion algorithms are to errors in the input.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 126}, page_content='SNAPSHOT OF ALGEBRAIC VISION 27\\nFigure 6. A minimal problem with algebraic degree 312: three\\ncalibrated pinhole cameras observe three points, two of which have\\nan incident line. Homotopy continuation was used to build a solver\\nfor this minimal problem taking 1 second or less, whereas symbolic\\nmethods haven’t yielded a practical solver [ 69].\\n7.1. Critical loci. A critical configuration in a structure-from-motion prob-\\nlem is a pair ( X, C) ∈ X × Cm consisting of an ordered subset X of P3'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 126}, page_content='R and an m-\\ntuple of cameras C such that the resulting m images Φ(X, C) do not have a unique\\nreconstruction modulo the group G that acts on the fibers of the joint camera map\\nΦ. In other words, a configuration of 3D points X ∈ Xand cameras C ∈ Cm is\\ncritical if there is another configuration ( X′, C′) ∈ X × Cm that is not in the same\\nG-orbit as (X, C) and produces the same images, i.e., Φ( X, C) = Φ(X′, C′).\\nThe study of critical configurations for two pinhole cameras goes back to the'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 126}, page_content='German-Austrian literature on photogrammetry, where critical loci where known\\nas gef¨ ahrliche¨Orter: Krames [ 112] showed that an ordered set of 3D points and\\ntwo pinhole cameras form a critical configuration only if the points and the two\\ncamera centers lie on a real ruled quadric surface; see also Maybank [ 137] for a\\nmodern reference. Recall that a quadric surface in P3\\nR is said to be ruled if it\\ncontains a line. There are four projectively non-equivalent ruled quadrics. They'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 126}, page_content='can be distinguished based on the rank of a symmetric matrix S ∈ R4×4 giving a\\ndefining equation X⊤SX = 0 of the quadric: smooth ruled quadric (rank S = 4),\\ncone (rank S = 3), two planes (rank S = 2), and double plane (rank S = 1); see\\nFigure 7.\\nA first characterization of critical configurations of 3D points and an arbitrary\\nnumber of projective pinhole cameras modulo the group PGL(4 , R) was provided\\nby Hartley and Kahl [ 89]. In particular, for m = 2, they derive:'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 126}, page_content='Theorem 7.1. An ordered set of 3D points and two pinhole cameras form a\\ncritical configuration modulo PGL(4, R) if and only if the points and the two camera\\ncenters lie on a real ruled quadric surface Q, with the following two exceptions: 1)\\nQ is a cone and both camera centers lie on the same line on Q but not on its\\nvertex, and 2) Q is two planes and the camera centers do not lie on the same\\nplane. Hence, up to projective equivalence, there are eight critical quadrics with'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 126}, page_content='two marked camera centers, shown in Figure 7.\\nThe classification in [89] has some mistakes for three or more cameras. Br˚ atelund\\ndeveloped new algebraic techniques [ 33] and corrected the classification for three\\ncameras [32].'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 127}, page_content='28 JOE KILEEL AND KATHL ´EN KOHN\\nFigure 7. The eight critical quadrics (in black) for 3D points\\nand two projective pinhole cameras. The marked points are the\\ncamera centers. The exceptions in Theorem 7.1 are red. (Courtesy\\nof Martin Br˚ atelund.)\\nKahl and Hartley [103] also studied the critical configurations of 3D points and\\ncalibrated pinhole cameras, modulo the scaled special Euclidean group of R3. They\\nprovide a full characterization for two cameras and show the existence of critical'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 127}, page_content='configurations for an arbitrary number of points and cameras. It is still an open\\nproblem to derive the complete catalog of all critical configurations consisting of\\n3D points and more than two calibrated pinhole cameras.\\nRemark 7.2. Another natural notion of critical locus that is not as prominent\\nin the vision literature as the critical configurations described above is the ram-\\nification locus of the joint camera map. For instance, for a minimal problem of'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 127}, page_content='algebraic degree larger one, almost all configurations ( X, C) ∈ X × Cm are criti-\\ncal (over C). However, the ramification locus of the joint camera map of such a\\nminimal problem carries crucial meaning: configurations close to the ramification\\nlocus often correspond to ill-conditioned problem instances [ 41, 58, 59], which has\\nnegative algorithmic consequences such as numerical instability; see also Section\\n7.3.\\nCritical configurations are also understood in a variety of other settings. Crit-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 127}, page_content='ical loci for 3D lines and three pinhole cameras are for instance investigated by\\nBuchanan [ 38, 39 ], Maybank [ 138], Navab and Faugeras [ 142], and Zhao and\\nChung [193]. They are congruences in Gr(1 , P3) that can be parametrized by Bor-\\ndiga surfaces in P4. It would be interesting to generalize those results to more\\ncameras, which is currently open.\\nBuchanan [ 37] also described the critical configurations for a single pinhole'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 127}, page_content='camera, i.e., for the problem of camera calibration where both 3D and image points\\nare known and only the camera parameters are to be recovered. Here the critical\\n3D points and the camera center lie either on a (possibly reducible but connected)\\ntwisted cubic or on the union of a line and a plane such that the camera center lies\\non the line.\\n˚Astr¨ om and Kahl [12] classified the critical loci for 1D structure-from-motion'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 127}, page_content='problems where 2D points are projected via pinhole cameras of the form P2 99K P1.\\nCritical configurations of 2D points and camera centers lie on cubic curves.\\nBertolini, Turrini, and coauthors generalized several of the results above to\\nhigher dimensions, i.e., where the classical pinhole cameras P3 99K P2 are replaced\\nby projections PN 99K Pn, N > n; see e.g. [ 22, 18, 19, 16, 21 ]. As explained'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 128}, page_content='SNAPSHOT OF ALGEBRAIC VISION 29\\nat the end of Section 4.3, the study of general projections is motivated by the\\nreconstruction of dynamic scenes. Several of their works also perform simulated\\nexperiments to test numerical instability phenomena for problem instances near\\ncritical loci; see in addition [ 17, 23, 20].\\nDespite the literature on critical configurations outlined above, critical loci\\nof vision problems are still unexplored in many settings, e.g. for 3D point-line'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 128}, page_content='arrangements or for other camera models besides pinhole cameras.\\n7.2. Aspect graphs and visual events. In this section, we focus on taking\\npictures of an algebraic curve or surface X in P3 with a pinhole camera C : P3 99K\\nP2. Clearly, if X is a curve, its image is typically a plane curve and we write\\nYC(X) for its Zariski closure in P2. If X is a surface, we denote by YC(X) its image\\ncontour, also known as silhouette or outline curve, in the image plane P2. This is'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 128}, page_content='the curve that bounds the two-dimensional region that is the image of the surface\\nX taken by the camera C. In other words, it is the natural sketch one might use to\\ndepict the surface; see Figure 8 for an image contour of a torus. Formally, YC(X)\\nis the branch locus of the map C restricted to X, i.e., it is the projection of the\\ncritical points where viewing lines through the camera center are tangent to the\\nsurface X.\\nFigure 8. Image contour of a torus.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 128}, page_content='surface X.\\nFigure 8. Image contour of a torus.\\nThe reconstruction of an algebraic surface from its image contour has been\\nstudied intensively, e.g., by Zariski [191], Segre [165], Biggiogero [27, 28], Chisini\\n[47] who conjectured that a surface can be reconstructed from its image contour,\\nKulikov [120, 121] who solved Chisini’s conjecture, and Forsyth [ 79] who solved\\nthe special case of a smooth surface and a general camera. Algorithmic solutions'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 128}, page_content='are for instance provided in [ 113, 54, 80, 81 ].\\nThe aspects of a curve or surfaceX are the stable views when the camera moves,\\ni.e., where the topology and singularities of the plane curve YC(X) do not change\\nunder perturbations of the camera center. There is a finite number of aspects and\\nthey are the nodes of theaspect graph. Its edges are the visual events that transition\\nbetween different aspects. For instance, Figure 8 shows an aspect of a torus, and a'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 128}, page_content='visual event occurs when we rotate the torus until the real singularities disappear.\\nThe aspect graph was first introduced by Koenderink and van Doorn [ 109] under\\nthe name of visual potential . A detailed discussion of image contours and their\\nvisual events can be found in Koenderink’s book [ 108].\\nAlthough there has not been much use of aspect graphs in real-life applications,\\nthey have been an active research topic in the computer vision community; see e.g.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 128}, page_content='Bowyer’s and Dyer’s survey [31] or Chapter 13 in Forsyth’s and Ponce’s book [ 78].\\nIn particular, several algorithms for the computation of aspect graphs of algebraic'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 129}, page_content='30 JOE KILEEL AND KATHL ´EN KOHN\\nsurfaces were proposed, using both symbolic and numerical methods [ 153, 151,\\n159, 160, 148].\\nFrom the algebraic perspective, visual events have been studied by Petitjean\\n[150] and Kohn, Sturmfels, and Trager [111]. The visual event surface V(X) of the\\ncurve or surface X is the Zariski closure in P3 of the set of camera centers where\\na visual event occurs. For a curve X in P3, the visual event surface typically has'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 129}, page_content='three irreducible components, each representing a different type of visual event. On\\nthe image curve YC(X), these types correspond to the three Reidemeister moves\\nfrom knot theory [ 157]; see Figure 9\\nFigure 9. The three types of visual events of a space curve.\\nIf X is a surface, its visual event surface V(X) typically has five irreducible\\ncomponents that arise from six different types of visual events (here two types of'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 129}, page_content='events cannot be distinguished algebraically, i.e. they lie on the same component of\\nV(X), but they represent two distinct behaviors over the real numbers). These six\\ntypes of events correspond to the non-generic singularities from catastrophe theory\\n[174, 10].\\nIn [111], the components of the visual event surface V(X) of a curve or surface\\nX are alternatively characterized via the iterated singular loci of the higher associ-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 129}, page_content='ated hypersurfaces [82] / coisotropic hypersurfaces [110] of X. For a surface X, the\\ncoisotropic hypersurfaces are the dual surface X∨ in (P3)∗ and the Hurwitz threefold\\nHur(X) in Gr(1, P3), i.e., the Zariski closure of the set of planes (respectively lines)\\ntangent to X. Figure 10 shows the iterated singular loci of these hypersurfaces till\\nthe level of curves. The five components of V(X) are encoded by the curves in the'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 129}, page_content='last row of Figure 10: two components are the dual surfaces to the curves on the\\nleft. The remaining three components are ruled by the lines on the curves shown\\nin the right diagram.\\nExample 7.3. Consider the curve Fℓ(X) ⊂ Gr(1, P3) associated with a surface\\nX ⊂ P3 of degree at least four. The curve Fℓ(X) is the Zariski closure of the set\\nof all lines that intersect the surface X at some point with multiplicity four. Such'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 129}, page_content='lines are called flecnodal. The union of all flecnodal lines is the flecnodal surface of\\nX; it is one of the irreducible components of the visual event surface V(X). The\\nvisual event corresponding to the flecnodal surface is called swallowtail event .'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 130}, page_content='SNAPSHOT OF ALGEBRAIC VISION 31\\n(P3)∗\\nmult. 1\\nX∨\\nmult. 2\\nPp(X)\\nmult. 3\\nEp(X)\\nmult. 2 + 2\\nGr(1,P3)\\nmult. 1\\nHur(X)\\nmult. 2\\nPT(X)\\nmult. 3\\nFℓ(X)\\nmult. 4\\nBit(X)\\nmult. 2 + 2\\nCℓ(X)\\nmult. 3 + 2\\nTℓ(X)\\nmult. 2 + 2 + 2\\nFigure 10. Loci of planes and lines that meet a surface X with\\nassigned multiplicities. For instance, “mult. 2 + 2 + 2” on the\\nright-hand side refers to the locus of tritangent lines. A dashed\\n(resp. solid) edge means that the lower variety is contained in (the'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 130}, page_content='singular locus of) the upper one [ 111].\\nFor an explicit example, let us consider the quartic surface parametrized by\\n(s, t) 7→ (s, t, s2 + 3st + t4). The surface is shown in yellow in Figure 11. In Figure\\n11a, the surface’s image contour is smooth. In Figure 11b, the image contour has\\ntwo cusps and a node, as illustrated in the torus example in Figure 8. The transition\\noccurs when the camera center is located on a flecnodal line. On a general surface,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 130}, page_content='there is a one-dimensional family of points such that one of the tangent lines at\\nthat point is flecnodal. The family makes up the curve shown in red in Figure 11.\\nFor the particular surface there, every red point has two flecnodal lines. The lines\\nare shown for a particular point in Figures 11a and 11b, and for many points in\\nFigure 11c.\\n(a) Smooth image contour\\n(b) Singular image contour\\n (c) Flecnodal surface\\nFigure 11. Quartic surface parametrized by (s, t, s2 +3st+t4) (in'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 130}, page_content='yellow) with flecnodal lines that meet the surface with multiplicity\\n4 at one of the red points. A swallowtail event happens when the 2\\ncusps and the node in Figure 11b come together to a higher order\\nsingularity.\\nSimilarly, if X is a curve its coisotropic hypersurfaces are the dual surfaceX∨ in\\n(P3)∗ and the Chow threefold in Gr(1, P3), that is, the set of lines meeting X. Their\\n(iterated) singular curves give rise to the three components of V(X) analogously.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 131}, page_content='32 JOE KILEEL AND KATHL ´EN KOHN\\nThe degrees of the components of V(X) for a general curve or surfaceX of fixed\\ndegree (and genus in the curve case) are classically known; see [ 111, Theorems 3.1\\nand 4.1]. Petitjean provided degree formulas for the five components of V(X) in\\nthe surface case [ 150], but they appeared in fact already in paragraphs 597, 598,\\n599, 608 and 613 of Salmon’s 1882 book [ 162].\\n7.3. Hurwitz hypersurfaces and condition numbers. The minimal prob-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 131}, page_content='lem (5.1) of computing essential matrices consistent with five point pairs admits\\nthe following geometric interpretation: We are computing the intersection of the es-\\nsential variety E ⊂P(R3×3) with a data-dependent linear subspace L ⊂ P(R3×3) of\\ncomplementary dimension, namely L = {xiy⊤\\ni : i ∈ I}⊥. In numerical algebraic ge-\\nometry language, we are computing a witness set of E [166, Chapter 13.3]. Several,\\nthough not all, minimal problems in computer vision usefully admit such a inter-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 131}, page_content='pretation. For instance, Kileel [ 105] describes all 66 minimal problems for three\\ncalibrated pinhole cameras that arise from slicing the calibrated trifocal variety Tcal\\nwith linear spaces of complementary dimension.\\nIn the applied algebra community, this description has motivated theoretical\\nworks on intersecting a fixed variety Z ⊂ Pn with varying linear subspaces of com-\\nplementary dimension. This includes the detailed study of the Hurwitz hypersurface'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 131}, page_content='Hur(Z) by Sturmfels [ 171]. Writing c for the codimension of Z in Pn and d for\\nits degree, Hur(Z) is the Zariski closure in Gr( c, Pn) of the set of all c-dimensional\\nprojective subspaces L of Pn such that the intersection Z ∩ L does not consist of d\\nreduced points. The Hurwitz hypersurface is intimately linked with the condition\\nnumber [41] of the algebraic function L 7→ Z ∩L. That condition number measures\\nhow much the intersection Z ∩ L changes when L gets perturbed, which is cru-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 131}, page_content='cial for the understanding of how much numerical computations such as homotopy\\ncontinuation are affected by errors. B¨ urgisser showed that the set of ill-posed L,\\ni.e. those with infinite condition, is essentially the Hurwitz hypersurface Hur( Z)\\nand that (almost) all L with large condition are contained in a small tube around\\nHur(Z) [40, Corollary 1.10]. B¨ urgisser’s work follows the general idea that con-\\ndition numbers are given by the inverse distance to ill-posedness from numerical'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 131}, page_content='analysis [59, 58].\\nRecently there has been work on condition numbers of minimal problems in\\nthe more practical setting, where the dependence is in terms of the perturbation of\\nthe image data (instead of perturbation of L in the Grassmannian) by Fan, Kileel\\nand Kimia [ 70, 71]. Their work drew distinctions between criticality (Section 7.1\\nabove) and ill-posedness (when the condition number is infinite), see [71, Section 5].'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 131}, page_content='For the 5- and 7-point problems, the authors derived condition number formulas in\\nterms of certain Jacobian matrices, and they characterized ill-posed world scenes\\nand ill-posed image point pairs geometrically. Furthermore, real-data experiments\\nshowed that poor conditioning can actually plague some computer vision data sets.\\nReferences\\n1. Sameer Agarwal, Timothy Duff, Max Lieblich, and Rekha R Thomas,An atlas for the pinhole\\ncamera, Foundations of Computational Mathematics (2022), 1–51.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 131}, page_content='2. Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven M\\nSeitz, and Richard Szeliski, Building Rome in a day, Communications of the ACM54 (2011),\\nno. 10, 105–112.\\n3. Sameer Agarwal, Hon-Leung Lee, Bernd Sturmfels, and Rekha Thomas, On the existence of\\nepipolar matrices, International Journal of Computer Vision 121 (2017), no. 3, 403–415.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 132}, page_content='SNAPSHOT OF ALGEBRAIC VISION 33\\n4. Sameer Agarwal, Andrew Pryhuber, Rainer Sinn, and Rekha Thomas, The chiral domain of\\na camera arrangement, Journal of Mathematical Imaging and Vision (2022), 1–20.\\n5. Sameer Agarwal, Andrew Pryhuber, and Rekha Thomas, Ideals of the multiview variety ,\\nIEEE Transactions on Pattern Analysis and Machine Intelligence 43 (2019), no. 4, 1279–\\n1292.\\n6. Chris Aholt, Sameer Agarwal, and Rekha Thomas, A QCQP approach to triangulation ,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 132}, page_content='European Conference on Computer Vision, Springer, 2012, pp. 654–667.\\n7. Chris Aholt and Luke Oeding, The ideal of the trifocal variety, Mathematics of Computation\\n83 (2014), no. 289, 2553–2574.\\n8. Chris Aholt, Bernd Sturmfels, and Rekha Thomas, A Hilbert scheme in computer vision ,\\nCanadian Journal of Mathematics 65 (2013), no. 5, 961–988.\\n9. Christopher Aholt, Polynomials in multiview geometry , Ph.D. thesis, University of Wash-\\nington, 2012.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 132}, page_content='ington, 2012.\\n10. Vladimir I Arnol’d, Catastrophe theory, Springer-Verlag, 1984.\\n11. Federica Arrigoni, Andrea Fusiello, Elisa Ricci, and Tomas Pajdla, Viewing graph solvability\\nvia cycle consistency, Proceedings of the IEEE/CVF International Conference on Computer\\nVision, 2021, pp. 5540–5549.\\n12. Kalle ˚Astr¨ om and Fredrik Kahl,Ambiguous configurations for the 1D structure and motion\\nproblem, Journal of Mathematical Imaging and Vision 18 (2003), no. 2, 191–203.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 132}, page_content='13. Daniel Barath, Five-point fundamental matrix estimation for uncalibrated cameras, Proceed-\\nings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 235–\\n243.\\n14. Daniel Barath, Tekla Toth, and Levente Hajder, A minimal solution for two-view focal-\\nlength estimation using two affine correspondences , Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition, 2017, pp. 6003–6011.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 132}, page_content='15. Daniel J Bates, Andrew J Sommese, Jonathan D Hauenstein, and Charles W Wampler,\\nNumerically solving polynomial systems with Bertini , SIAM, 2013.\\n16. Marina Bertolini, Gian Mario Besana, Roberto Notari, and Cristina Turrini, Critical loci\\nin computer vision and matrices dropping rank in codimension one , Journal of Pure and\\nApplied Algebra 224 (2020), no. 12, 106439.\\n17. Marina Bertolini, GianMario Besana, and Cristina Turrini, Instability of projective recon-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 132}, page_content='struction from 1-view near critical configurations in higher dimensions, International Confer-\\nence on Midwest Algebra, Geometry and Their Interactions, American Mathematical Society,\\n2007, pp. 1–12.\\n18. , Critical loci for projective reconstruction from multiple views in higher dimension:\\nA comprehensive theoretical approach, Linear Algebra and its Applications 469 (2015), 335–\\n363.\\n19. Marina Bertolini and Luca Magri, Critical hypersurfaces and instability for reconstruction'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 132}, page_content='of scenes in high dimensional projective spaces , Machine Graphics and Vision 29 (2020),\\nno. 1/4, 3–20.\\n20. Marina Bertolini, Luca Magri, and Cristina Turrini, Critical loci for two views reconstruction\\nas quadratic transformations between images , Journal of Mathematical Imaging and Vision\\n61 (2019), no. 9, 1322–1328.\\n21. Marina Bertolini, Roberto Notari, and Cristina Turrini, Smooth determinantal varieties and'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 132}, page_content='critical loci in multiview geometry , Collectanea Mathematica 73 (2022), no. 3, 457–475.\\n22. Marina Bertolini and Cristina Turrini, Critical configurations for 1-view in projections from\\nPk → P2, Journal of Mathematical Imaging and Vision 27 (2007), no. 3, 277–287.\\n23. Marina Bertolini, Cristina Turrini, and GianMario Besana, Instability of projective recon-\\nstruction of dynamic scenes near critical configurations , Proceedings of the IEEE Interna-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 132}, page_content='tional Conference on Computer Vision, 2007, pp. 1–7.\\n24. Snehal Bhayani, Janne Heikkil¨ a, and Zuzana Kukelova, Sparse resultant based minimal\\nsolvers in computer vision and their connection with the action matrix , arXiv preprint\\narXiv:2301.06443 (2023).\\n25. Snehal Bhayani, Zuzana Kukelova, and Janne Heikkil¨ a,A sparse resultant based method for\\nefficient minimal solvers , Proceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition, 2020, pp. 1770–1779.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 132}, page_content='and Pattern Recognition, 2020, pp. 1770–1779.\\n26. Simone Bianco, Gianluigi Ciocca, and Davide Marelli, Evaluating the performance of struc-\\nture from motion pipelines , Journal of Imaging 4 (2018), no. 8, 98.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 133}, page_content='34 JOE KILEEL AND KATHL ´EN KOHN\\n27. G Masotti Biggiogero, La caratterizzazione della curva di diramazione dei piani tripli, ot-\\ntenuta mediante sistemi di curve pluritangenti , Ist. Lombardo Sci. Lett. Rend Cl. Sci. Mat.\\nNat 80 (1947), 151–160.\\n28. , Sulla caratterizzazione della curva di diramazione dei piani quadrupli generali , Ist.\\nLombardo Sci. Lett. Rend Cl. Sci. Mat. Nat 80 (1947), 269–280.\\n29. Arthur Bik, Henrik Eisenmann, and Bernd Sturmfels, Jordan algebras of symmetric matrices,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 133}, page_content='Le Matematiche 76 (2021), no. 2, 337–353.\\n30. Mireille Boutin and Pierre-Louis Bazin, Structure from motion: A new look from the point of\\nview of invariant theory, SIAM Journal on Applied Mathematics64 (2004), no. 4, 1156–1174.\\n31. Kevin W Bowyer and Charles R Dyer, Aspect graphs: An introduction and survey of recent\\nresults, International Journal of Imaging Systems and Technology 2 (1990), no. 4, 315–328.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 133}, page_content='32. Martin Br˚ atelund, Critical configurations for three projective views , arXiv preprint\\narXiv:2112.05478 (2021).\\n33. , Critical configurations for two projective views, a new approach, Journal of Symbolic\\nComputation 120 (2024), 102226.\\n34. Paul Breiding, T¨ urk¨ u¨Ozl¨ um C ¸ elik, Timothy Duff, Alexander Heaton, Aida Maraj, Anna-\\nLaura Sattelberger, Lorenzo Venturello, and O˘ guzhan Y¨ ur¨ uk,Nonlinear algebra and appli-\\ncations, Numerical Algebra, Control and Optimization (2021).'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 133}, page_content='35. Paul Breiding, Felix Rydell, Elima Shehu, and Ang´ elica Torres, Line multiview varieties ,\\nSIAM Journal on Applied Algebra and Geometry 7 (2023), no. 2, 470–504.\\n36. Paul Breiding and Sascha Timme, HomotopyContinuation.jl: A package for homotopy\\ncontinuation in Julia , International Congress on Mathematical Software, Springer, 2018,\\npp. 458–465.\\n37. Thomas Buchanan, The twisted cubic and camera calibration , Computer Vision, Graphics,\\nand Image Processing 42 (1988), no. 1, 130–132.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 133}, page_content='and Image Processing 42 (1988), no. 1, 130–132.\\n38. , Critical sets for 3D reconstruction using lines , European Conference on Computer\\nVision, Springer, 1992, pp. 730–738.\\n39. , On the critical set for photogrammetric reconstruction using line tokens in P3(C),\\nGeometriae Dedicata 44 (1992), no. 2, 223–232.\\n40. Peter B¨ urgisser,Condition of intersecting a projective variety with a varying linear subspace,\\nSIAM Journal on Applied Algebra and Geometry 1 (2017), no. 1, 111–125.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 133}, page_content='41. Peter B¨ urgisser and Felipe Cucker, Condition: The geometry of numerical algorithms ,\\nGrundlehren der mathematischen Wissenschaften, vol. 349, Springer Science & Business\\nMedia, 2013.\\n42. Eugenio Calabi, Peter J Olver, Chehrzad Shakiban, Allen Tannenbaum, and Steven Haker,\\nDifferential and numerically invariant signature curves applied to object recognition , Inter-\\nnational Journal of Computer Vision 26 (1998), 107–135.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 133}, page_content='43. Federico Camposeco, Torsten Sattler, and Marc Pollefeys, Minimal solvers for generalized\\npose and scale estimation from two rays and one point , European Conference on Computer\\nVision, Springer, 2016, pp. 202–218.\\n44. Luca Carlone, Estimation contracts for outlier-robust geometric perception, Foundations and\\nTrends® in Robotics 11 (2023), no. 2-3, 90–224.\\n45. Justin Chen and Joe Kileel, Numerical implicitization, Journal of Software for Algebra and\\nGeometry 9 (2019), no. 1, 55–63.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 133}, page_content='Geometry 9 (2019), no. 1, 55–63.\\n46. Chiang-Heng Chien, Hongyi Fan, Ahmad Abdelfattah, Elias Tsigaridas, Stanimire Tomov,\\nand Benjamin Kimia, GPU-based homotopy continuation for minimal problems in computer\\nvision, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-\\nnition, 2022, pp. 15765–15776.\\n47. Oscar Chisini, Sulla identit` a birazionale di due funzioni algebriche di pi` u variabili, dotate di'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 133}, page_content='una medesima variet` a di diramazione, Ist. Lombardo Sci. Lett. Rend. Cl. Sci. Mat. Nat.,(3)\\n80 (1947), 3–6.\\n48. Ondˇ rej Chum, Jiˇ r´ ı Matas, and Josef Kittler,Locally optimized RANSAC , Joint Pattern\\nRecognition Symposium, Springer, 2003, pp. 236–243.\\n49. Yairon Cid-Ruiz, Oliver Clarke, and Fatemeh Mohammadi, A study of nonlinear multiview\\nvarieties, Journal of Algebra 620 (2023), 363–391.\\n50. Diego Cifuentes, Sameer Agarwal, Pablo Parrilo, and Rekha Thomas, On the local stability'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 133}, page_content='of semidefinite relaxations , Mathematical Programming 193 (2022), no. 2, 629–663.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 134}, page_content='SNAPSHOT OF ALGEBRAIC VISION 35\\n51. David Cox, John Little, and Donal O’Shea, Ideals, varieties, and algorithms: An introduction\\nto computational algebraic geometry and commutative algebra , Springer Science & Business\\nMedia, 2013.\\n52. David A Cox, Stickelberger and the eigenvalue theorem , Commutative Algebra, Springer,\\n2021, pp. 283–298.\\n53. Yuchao Dai, Hongdong Li, and Laurent Kneip, Rolling shutter camera relative pose: Gen-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 134}, page_content='eralized epipolar geometry , Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition, 2016, pp. 4132–4140.\\n54. Jean d’Almeida, Courbe de ramification de la projection sur P2 d’une surface de P3, Duke\\nMathematical Journal 65 (1992), no. 2, 229–233.\\n55. D F Davidenko, On a new method of numerical solution of systems of nonlinear equations\\n(in Russian), Doklady Akademii Nauk SSSR, vol. 88, 1953, pp. 601–602.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 134}, page_content='56. , On the approximate solution of systems of nonlinear equations (in Russian) ,\\nUkrains’kyi Matematychnyi Zhurnal 5 (1953), no. 2, 196–206.\\n57. Michel Demazure, Sur deux problemes de reconstruction , Tech. Report 882, INRIA, 1988.\\n58. James Weldon Demmel, On condition numbers and the distance to the nearest ill-posed\\nproblem, Numerische Mathematik 51 (1987), no. 3, 251–289.\\n59. , The geometry of ill-conditioning , Journal of Complexity 3 (1987), no. 2, 201–229.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 134}, page_content='60. Tim Dobbert, Matchmoving: The invisible art of camera tracking , Sybex, 2005.\\n61. Jan Draisma, Emil Horobet ¸, Giorgio Ottaviani, Bernd Sturmfels, and Rekha Thomas, The\\nEuclidean distance degree of an algebraic variety, Foundations of Computational Mathemat-\\nics 16 (2016), no. 1, 99–149.\\n62. Dmitriy Drusvyatskiy, Hon-Leung Lee, Giorgio Ottaviani, and Rekha Thomas, The Eu-\\nclidean distance degree of orthogonally invariant matrix varieties , Israel Journal of Mathe-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 134}, page_content='matics 221 (2017), no. 1, 291–316.\\n63. Dmitriy Drusvyatskiy, Hon-Leung Lee, and Rekha Thomas, Counting real critical points of\\nthe distance to orthogonally invariant matrix sets , SIAM Journal on Matrix Analysis and\\nApplications 36 (2015), no. 3, 1360–1380.\\n64. Timothy Duff, Kathl´ en Kohn, Anton Leykin, and Tomas Pajdla, PLMP – Point-line mini-\\nmal problems in complete multi-view visibility , Proceedings of the IEEE/CVF International\\nConference on Computer Vision, 2019, pp. 1675–1684.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 134}, page_content='65. , PL1P – Point-line minimal problems under partial visibility in three views , Euro-\\npean Conference on Computer Vision, Springer, 2020, pp. 175–192.\\n66. Timothy Duff, Viktor Korotynskiy, Tomas Pajdla, and Margaret H Regan, Ga-\\nlois/monodromy groups for decomposing minimal problems in 3D reconstruction , SIAM\\nJournal on Applied Algebra and Geometry 6 (2022), no. 4, 740–772.\\n67. Ali Elqursh and Ahmed Elgammal, Line-based relative pose estimation , Proceedings of the'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 134}, page_content='IEEE Conference on Computer Vision and Pattern Recognition, 2011, pp. 3049–3056.\\n68. Laura Escobar and Allen Knutson, The multidegree of the multi-image variety , Combinato-\\nrial Algebraic Geometry, Springer, 2017, pp. 283–296.\\n69. Ricardo Fabbri, Timothy Duff, Hongyi Fan, Margaret H Regan, David da Costa de Pinho,\\nElias Tsigaridas, Charles W Wampler, Jonathan D Hauenstein, Peter J Giblin, Benjamin\\nKimia, et al., TRPLP – Trifocal relative pose from lines at points , Proceedings of the'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 134}, page_content='IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 12073–\\n12083.\\n70. Hongyi Fan, Joe Kileel, and Benjamin Kimia, On the instability of relative pose estimation\\nand RANSAC’s role , Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, 2022, pp. 8935–8943.\\n71. , Condition numbers in multiview geometry, instability in relative pose estimation,\\nand RANSAC, arXiv preprint arXiv:2310.02719 (2023).'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 134}, page_content='72. Xiaodong Fan and Ren´ e Vidal,The space of multibody fundamental matrices: Rank, geom-\\netry and projection, Dynamical Vision, Springer, 2006, pp. 1–17.\\n73. Olivier Faugeras and Quang-Tuan Luong, The geometry of multiple images: The laws that\\ngovern the formation of multiple images of a scene and some of their applications , MIT\\npress, 2001.\\n74. Olivier Faugeras and Bernard Mourrain, On the geometry and algebra of the point and'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 134}, page_content='line correspondences between n images, Proceedings of IEEE International Conference on\\nComputer Vision, 1995, pp. 951–956.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 135}, page_content='36 JOE KILEEL AND KATHL ´EN KOHN\\n75. Olivier D Faugeras and Francis Lustman, Motion and structure from motion in a piecewise\\nplanar environment, International Journal of Pattern Recognition and Artificial Intelligence\\n2 (1988), no. 03, 48–508.\\n76. Martin A Fischler and Robert C Bolles, Random sample consensus: a paradigm for model\\nfitting with applications to image analysis and automated cartography , Communications of\\nthe ACM 24 (1981), no. 6, 381–395.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 135}, page_content='the ACM 24 (1981), no. 6, 381–395.\\n77. Gunnar Fløystad, Joe Kileel, and Giorgio Ottaviani, The Chow form of the essential variety\\nin computer vision , Journal of Symbolic Computation 86 (2018), 97–119.\\n78. David Forsyth and Jean Ponce, Computer vision: A modern approach , 2 ed., Pearson, 2012.\\n79. David A Forsyth, Recognizing algebraic surfaces from their outlines , International Journal\\nof Computer Vision 18 (1996), no. 1, 21–40.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 135}, page_content='of Computer Vision 18 (1996), no. 1, 21–40.\\n80. Matteo Gallet, Niels Lubbes, Josef Schicho, and Jan Vrsek, Reconstruction of surfaces with\\nordinary singularities from their silhouettes , SIAM Journal on Applied Algebra and Geom-\\netry 3 (2019), no. 3, 472–506.\\n81. Matteo Gallet, Niels Lubbes, Josef Schicho, and Jan Vrˇ sek,Reconstruction of rational ruled\\nsurfaces from their silhouettes , Journal of Symbolic Computation 104 (2021), 366–380.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 135}, page_content='82. Israel M Gelfand, Mikhail M Kapranov, and Andrei V Zelevinsky, Discriminants, resultants,\\nand multidimensional determinants , Birkh¨ auser, 1994.\\n83. Giorgio Grisetti, Rainer K¨ ummerle, Cyrill Stachniss, and Wolfram Burgard, A tutorial on\\ngraph-based SLAM, IEEE Intelligent Transportation Systems Magazine 2 (2010), no. 4, 31–\\n43.\\n84. Johann August Grunert, Das Pothenot’sche problem in erweiterter gestalt; nebst bemerkun-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 135}, page_content='gen ¨ uber seine anwendung in der geod¨ asie, Grunerts Archiv fur Mathematik und Physik\\n(1841), 238–248.\\n85. Bert M Haralick, Chung-Nan Lee, Karsten Ottenberg, and Michael N¨ olle,Review and analy-\\nsis of solutions of the three point perspective pose estimation problem , International Journal\\nof Computer Vision 13 (1994), no. 3, 331–356.\\n86. Corey Harris and Daniel Lowengrub, The Chern-Mather class of the multiview variety , Com-\\nmunications in Algebra 46 (2018), no. 6, 2488–2499.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 135}, page_content='87. Richard Hartley, Projective reconstruction and invariants from multiple images, IEEE Trans-\\nactions on Pattern Analysis and Machine Intelligence 16 (1994), no. 10, 1036–1041.\\n88. , Chirality, International Journal of Computer Vision 26 (1998), 41––61.\\n89. Richard Hartley and Fredrik Kahl, Critical configurations for projective reconstruction from\\nmultiple views, International Journal of Computer Vision 71 (2007), no. 1, 5–47.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 135}, page_content='90. , Optimal algorithms in multiview geometry , Asian Conference on Computer Vision,\\nSpringer, 2007, pp. 13–34.\\n91. Richard Hartley and Frederik Schaffalitzky, Reconstruction from projections using Grass-\\nmann tensors, International Journal of Computer Vision 83 (2009), no. 3, 274–293.\\n92. Richard Hartley and Peter Sturm, Triangulation, Computer Vision and Image Understanding\\n68 (1997), no. 2, 146–157.\\n93. Richard Hartley and Ren´ e Vidal,The multibody trifocal tensor: Motion segmentation from'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 135}, page_content='3 perspective views, Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, 2004.\\n94. Richard Hartley and Andrew Zisserman, Multiple view geometry in computer vision , 2nd\\ned., Cambridge, 2003.\\n95. Otto Hesse, Die cubische gleichung, von welcher die L¨ osung des problems der homographie\\nvon M. Chasles abh¨ angt., Journal f¨ ur die reine und angewandte Mathematik62 (1863).\\n96. Anders Heyden and Kalle ˚Astr¨ om,Algebraic properties of multilinear constraints , Mathe-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 135}, page_content='matical Methods in the Applied Sciences 20 (1997), no. 13, 1135–1162.\\n97. Berthold KP Horn, Relative orientation revisited, Journal of the Optical Society of America\\nA 8 (1991), no. 10, 1630–1638.\\n98. Petr Hruby, Timothy Duff, Anton Leykin, and Tomas Pajdla, Learning to solve hard mini-\\nmal problems, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, 2022, pp. 5532–5542.\\n99. Kun Huang, Robert Fossum, and Yi Ma, Generalized rank conditions in multiple view ge-'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 135}, page_content='ometry with applications to dynamical scenes , European Conference on Computer Vision,\\nSpringer, 2002, pp. 201–216.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 136}, page_content='SNAPSHOT OF ALGEBRAIC VISION 37\\n100. Atsushi Ito, Makoto Miura, and Kazushi Ueda, Projective reconstruction in algebraic vision,\\nCanadian Mathematical Bulletin 63 (2020), no. 3, 592–609.\\n101. Bj¨ orn Johansson, Magnus Oskarsson, and Kalle ˚Astr¨ om,Structure and motion estimation\\nfrom complex features in three views , Indian Conference on Computer Vision, Graphics and\\nImage Processing, 2002.\\n102. Michael Joswig, Joe Kileel, Bernd Sturmfels, and Andr´ e Wagner, Rigid multiview varieties ,'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 136}, page_content='International Journal of Algebra and Computation 26 (2016), no. 04, 775–788.\\n103. Fredrik Kahl and Richard Hartley, Critical curves and surfaces for Euclidean reconstruction,\\nEuropean Conference on Computer Vision, Springer, 2002, pp. 447–462.\\n104. Fredrik Kahl and Didier Henrion, Globally optimal estimates for geometric reconstruction\\nproblems, International Journal of Computer Vision 74 (2007), no. 1, 3–15.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 136}, page_content='105. Joe Kileel, Minimal problems for the calibrated trifocal variety , SIAM Journal on Applied\\nAlgebra and Geometry 1 (2017), no. 1, 575–598.\\n106. Joe Kileel, Zuzana Kukelova, Tomas Pajdla, and Bernd Sturmfels,Distortion varieties, Foun-\\ndations of Computational Mathematics 18 (2018), no. 4, 1043–1071.\\n107. Midori Kitagawa and Brian Windsor, MoCap for artists: Workflow and techniques for mo-\\ntion capture, Focal Press, 2008.\\n108. Jan J Koenderink, Solid shape, MIT press, 1990.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 136}, page_content='109. Jan J Koenderink and Andrea J Van Doorn, The internal representation of solid shape with\\nrespect to vision, Biological Cybernetics 32 (1979), no. 4, 211–216.\\n110. Kathl´ en Kohn and James C Mathews Jr, Isotropic and coisotropic subvarieties of Grass-\\nmannians, Advances in Mathematics 377 (2021), 107492.\\n111. Kathl´ en Kohn, Bernd Sturmfels, and Matthew Trager, Changing views on curves and sur-\\nfaces, Acta Mathematica Vietnamica 43 (2018), no. 1, 1–29.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 136}, page_content='112. Josef Krames, Zur ermittlung eines objektes aus zwei perspektiven. (Ein beitrag zur theorie\\nder “gef¨ ahrlichen ¨ orter”.), Monatshefte f¨ ur Mathematik und Physik49 (1941), no. 1, 327–\\n354.\\n113. David J Kriegman and Jean Ponce, On recognizing and positioning curved 3-D objects from\\nimage contours, IEEE Transactions on Pattern Analysis and Machine Intelligence12 (1990),\\nno. 12, 1127–1137.\\n114. , Geometric modeling for computer vision , Curves and Surfaces in Computer Vision'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 136}, page_content='and Graphics II, vol. 1610, International Society for Optics and Photonics, 1992, pp. 250–260.\\n115. Yubin Kuang and Kalle ˚Astr¨ om,Pose estimation with unknown focal length using points,\\ndirections and lines, Proceedings of the IEEE International Conference on Computer Vision,\\n2013, pp. 529–536.\\n116. Zuzana Kukelova, Martin Bujnak, and Tomas Pajdla, Automatic generator of minimal prob-\\nlem solvers, European Conference on Computer Vision, Springer, 2008, pp. 302–315.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 136}, page_content='117. Zuzana Kukelova, Jan Heller, Martin Bujnak, Andrew Fitzgibbon, and Tomas Pajdla, Ef-\\nficient solution to the epipolar geometry for radially distorted cameras , Proceedings of the\\nIEEE International Conference on Computer Vision, 2015, pp. 2309–2317.\\n118. Zuzana Kukelova, Joe Kileel, Bernd Sturmfels, and Tomas Pajdla, A clever elimination\\nstrategy for efficient minimal solvers , Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition, 2017, pp. 4912–4921.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 136}, page_content='119. Zuzana Kukelova and Tomas Pajdla, A minimal solution to the autocalibration of radial dis-\\ntortion, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\\n2007, pp. 1–7.\\n120. Victor Stepanovich Kulikov, On Chisini’s conjecture , Izvestiya Rossiiskoi Akademii Nauk.\\nSeriya Matematicheskaya 763 (1999), 83–116.\\n121. , On Chisini’s conjecture. II, Izvestiya Rossiiskoi Akademii Nauk. Seriya Matematich-\\neskaya 72 (2008), 63–76.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 136}, page_content='eskaya 72 (2008), 63–76.\\n122. Viktor Larsson, Kalle ˚Astr¨ om, and Magnus Oskarsson,Efficient solvers for minimal problems\\nby syzygy-based reduction , Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition, 2017, pp. 820–829.\\n123. Viktor Larsson, Magnus Oskarsson, Kalle ˚Astr¨ om, Alge Wallis, Zuzana Kukelova, and Tomas\\nPajdla, Beyond Gr¨ obner bases: Basis selection for minimal solvers, Proceedings of the IEEE'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 136}, page_content='Conference on Computer Vision and Pattern Recognition, 2018, pp. 3945–3954.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 137}, page_content='38 JOE KILEEL AND KATHL ´EN KOHN\\n124. Viktor Larsson, Nicolas Zobernig, Kasim Taskin, and Marc Pollefeys, Calibration-free\\nstructure-from-motion with calibrated radial trifocal tensors, European Conference on Com-\\nputer Vision, Springer, 2020, pp. 382–399.\\n125. St´ ephane Laveau and Olivier Faugeras, Oriented projective geometry for computer vision ,\\nEuropean Conference on Computer Vision, Springer, 1996, pp. 147–156.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 137}, page_content='126. Gim Hee Lee, A minimal solution for non-perspective pose estimation from line correspon-\\ndences, European Conference on Computer Vision, Springer, 2016, pp. 170–185.\\n127. Gilad Lerman and Yunpeng Shi, Robust group synchronization via cycle-edge message pass-\\ning, Foundations of Computational Mathematics (2021), 1–77.\\n128. Binglin Li, Images of rational maps of projective spaces, International Mathematics Research\\nNotices 2018 (2018), no. 13, 4190–4228.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 137}, page_content='Notices 2018 (2018), no. 13, 4190–4228.\\n129. Bo Li and Viktor Larsson, Gaps: Generator for automatic polynomial solvers , arXiv preprint\\narXiv:2004.11765 (2020).\\n130. Max Lieblich and Lucas Van Meter, Two Hilbert schemes in computer vision , SIAM Journal\\non Applied Algebra and Geometry 4 (2020), no. 2, 297–321.\\n131. H Christopher Longuet-Higgins, A computer algorithm for reconstructing a scene from two\\nprojections, Nature 293 (1981), no. 5828, 133–135.'),\n",
              " Document(metadata={'source': '/Users/pranavsuresh/Desktop/RESEARCH_BOT/arxiv_pdfs/Computer_Vision.pdf', 'page': 137}, page_content='132. David G Lowe, Distinctive image features from scale-invariant keypoints, International Jour-\\nnal of Computer Vision 60 (2004), no. 2, 91–110.\\n133. Yi Ma, Stefano Soatto, Jana Koˇ seck´ a, and Shankar Sastry, An invitation to 3-D vision:\\nFrom images to geometric models, Interdisciplinary Applied Mathematics, vol. 26, Springer,\\n2004.\\n134. Evgeniy Martyushev, Jana Vrablikova, and Tomas Pajdla, Optimizing elimination templates'),\n",
              " ...]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lF8jA6xZ0Hm",
        "outputId": "f52c114c-231a-4d77-aeef-285a041dcd07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3406\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def split_docs(documents,chunk_size=500,chunk_overlap=50):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "  docs = text_splitter.split_documents(documents)\n",
        "  return docs\n",
        "\n",
        "docs = split_docs(documents)\n",
        "print(len(docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVcFjgHJZ8S9",
        "outputId": "9b14917f-0822-44a2-bda8-47faa5894173"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computer Stereo Vision for Autonomous Driving 3\n",
            "Cameras capture 2D images, by collecting light reﬂected on 3D objects. Im-\n",
            "ages captured from diﬀerent views can be utilized to reconstruct the 3D driving\n",
            "scene geometry. Most autonomous car perception tasks, such as visual semantic\n",
            "driving scene segmentation and object detection/recognition, are developed for\n",
            "images. In Sec. 3, we provide readers with a comprehensive overview of these\n"
          ]
        }
      ],
      "source": [
        "print(docs[8].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "\n",
        "OPENAI_API_KEY = st.secrets[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857,
          "referenced_widgets": [
            "59baeec6d783438e8a798959fc6885c2",
            "f059f038342e430586d71dec52194793",
            "a437b3c6371f4303bcc8b5245fac08ea",
            "56f90d69416d4aa1a8b9c8daa016ba6a",
            "e0ab43141a5d44278cba23259229db0d",
            "bb8b910963f642829f20d1683a067abf",
            "1067f06e2aac41c894e6d619a1ad4e2b",
            "979a4dffb5364142b3c0142ef5ba5680",
            "5e378e4ea15646b2b55f67ece88a230b",
            "bb735781bd5e406b89bafdb01c0270a6",
            "98f220c88b1b494690b61d891c40dd4c",
            "3ec2b16cdaca440a96d3f2316d5c8504",
            "3d302f3a55b7469b9b6c8f29566e5917",
            "28f66c5017f648e886c8cb34ff96d2da",
            "f5740146f8cf490590bdcba943096ac8",
            "10dac229dff549c5b01f4e7dd9b129ca",
            "d7c9d379e72445d5a343f08d3a56ee76",
            "2b74c3090dad4a58b8549926fad86271",
            "3626ac95bc944c0ab056f33f7c64ca8c",
            "820df94b12cd4f2da911b89271980b9b",
            "5dae446e7a1d4b34962de9ac4456e014",
            "b7d02009e2074ba399cf2bb47946f083",
            "3b29d6cb19d84a13b697b90647075cb9",
            "86075f014ff346cdb926459e1b3db711",
            "a065eb2b40b34dd3a3ea1b65ce67002d",
            "57142f73a2a941c9a28fa8747a3d98d0",
            "9ff11a6f4ea445548739785086c5d669",
            "b0318f9195624f0987b07d8fa7b10a6b",
            "2cc9dea0129542f582f6c7d9877cc1c8",
            "248d087c8e414fe584227814160a7f2b",
            "0c003ee77ba44f13a3fae4a34c1328f6",
            "411eb90269d442f09ee052032cee09a5",
            "49b6d94c2eab42a99b695419074f8958",
            "9fb430b5ba8e4003a95c43c00d4e2d00",
            "25d3948447bd438d904efdfccc3d7ea0",
            "cabb9f7137f042859b32af306f34549e",
            "7b31d104d2fe4170849de99e81d400f4",
            "4f08da63072a4898b1d521fb4a876f0d",
            "21215f2fa6bb4b5982ddf0198a98e9d4",
            "98fda223455743638835b78eabd1d4b5",
            "4d8df2c34fca4eeeb87670fe25d29a8a",
            "7c0b9ac6cd5f4498b810f0b0beb802bb",
            "822d81dc5fdf49fe9f950bf839a7de2f",
            "aed9acfd105c44f09c2b9675457a0689",
            "549062259cb64f81a979d8700345f8e9",
            "b1dfa23ff85a4f2f828c900476ef0804",
            "adc53ec8fb6d48b59998093ee1c52b66",
            "2a8ed199e7e840fdae0f99d378ee9644",
            "dbcfcc38507c4df8b28599c3e7420350",
            "c7b2af652a0c41278e62fd447331acd6",
            "6bd26ba594fd4f0e87c4d9659483b233",
            "919e05f8e0af4b25b55986409dc1f2ef",
            "e1f8bde65bc84ce99902ce5de17ccb0e",
            "54b4391624a34d3fafa7cba61bffa269",
            "1667053ee9d349dca8c2170016a431cb",
            "f3285ec1908d48f295b63b7349967959",
            "ed1df9840ebb43edbd6d95b4fa1e3e04",
            "fd08f2fb701d456b89050272ca9bd20d",
            "c35cdfd8fac24ad0b5f07e76004525cc",
            "c254856a8a524e70a54fcfc1e389306d",
            "f91e3c066389464b986306eaf5db61c8",
            "8d27d15788a14d90ba20be096a46c442",
            "3d288674e7b140cf85ebff0b86662045",
            "c612d8951e2c4e279cf263cf2b2b70bc",
            "df9cf009019849839111ad0f22195e3d",
            "0c375fec344a4d109cd9d869430bb999",
            "cf85d1be377e47fdb11fd8f654a2d2c8",
            "e8154cdd6da945be90467e689f53c21f",
            "4bf75d64c602432a9c9ccf62f4454f5c",
            "4c31196cd8684de283a1544fac8b0c94",
            "3655e618833c4ac0b3b303a54c9c2c62",
            "f7a527e85b5246fc8eeb73b5cdc6f128",
            "8454c711aa4c42f1992bd81597293808",
            "deeea70f2a654af58073c63429472657",
            "b867fa2c3e424ad388d566a5bd5c580e",
            "4974341254624b059e9b4c9739c4acd4",
            "c974169e945e48528316b6d33902b8f8",
            "158f3842044a4c49bfb2edceb616f98f",
            "c9b848dea9eb4c52a447529c818b29be",
            "ce3d41add0c14319b4d20de160a9259f",
            "b04f1b4d17b3403894893087ec523a2b",
            "067f757f43db45cc8022e8fba5c9c817",
            "1686038625114cc48d8942f3d4893de9",
            "85e89982b0df473dba7649e3d464bae7",
            "b0cfa0cf699c457b9b8d8bacb9b98794",
            "71723a2d3a384acba891a37f2e5f48c8",
            "c8d53bc71965475498f147a5f8a908b7",
            "a22492795b384194879c071a2f452233",
            "e419f8bfded94d9b8c2b326af0dc6e70",
            "30e1579510dd478a8e397cc56fd3fc89",
            "67cd54af507240679e1a3596685d05f7",
            "b4da1de31cc84107ab3429bc62037381",
            "c62e0de5b14e403a88840d74fd642966",
            "111d8d310add429f9603fbadeac20cf5",
            "2c475b6f31c144019d72a429891b1f89",
            "3ef4dbff55a942efaf0f6582baf4c8da",
            "347d180f557d41dbad1ac2332c729271",
            "df1372bd5cb040ac85141d5f11e13343",
            "6a41612156be4c688425dd6a8e2d425c",
            "e2655b00362a46238665bb973c18bff8",
            "59b6ac5d644048faa53103b561462eaa",
            "a2058ae051734f299da6e4aa329a4844",
            "4b33b4a1c9344044a6fc5d7e00438f5d",
            "63ecc46d58c84800be60d0bafe968ad1",
            "5cb8d31e4ea7496cb1b7be636ff1f0f6",
            "8101ef7678fb4535a82401039495efa3",
            "bc80761af4bd4506a7dce043b5bb1dcb",
            "aada1ceacb4949c290acdf68b561da8c",
            "6f633d3909424c8d9ea3c353f69b2534",
            "1012292cb6284d8b8b7dbe6da8089d03",
            "79029f8c0c9f4376af59bca97fc5bc83",
            "69c672cd74674eda925ab87e41360ae2",
            "8e89291c024044f3a21bfffb45ae1f4e",
            "b5873d590a154464894239be04937aea",
            "c4585a9ad4e0444f881a8c262766eace",
            "bc5fb63da52b4d6ea5279a371780c0e9",
            "3869c5c49f244907bfcdf3e862cddb34",
            "49a2b91eb9504c2781fb7fce431cbd18",
            "a530fc9783be4a7fb2dae89166d5df2b",
            "5c9496b5fa4540369265d9c11c3d6ca9",
            "698cbb3b46724f079132d9108e89e305",
            "ef5fbedad6724d8bb2c95a294c047880",
            "cae253847f634c359b844bbce9790286",
            "637ee292775f4d8e9e8e9fbeddedc9ab",
            "b21ab6efb2a846e59fb89ae16bc3c9c2",
            "deafe043ea334e60ad5dd87fcb9d3e51",
            "5c48fe79b60340e5bd5b55740cf1b732",
            "31074b283ec14133a7c9bada6aea11ce",
            "6d0545256e1b4893b50623facbfb5696",
            "2402bbbdd88a48bfa4ce51a12d25a3f2",
            "a1a5aa9bc8534451aa449405fe61d912",
            "802c1c2babca4409b7972b0cea66c78f",
            "14c27fddbbb3405e95f03eadf4b586be",
            "1f700c80bdbe4fc2a44826fa07874e7d",
            "a6c4eb8aa80b4adb8596ee26f6dac38e",
            "b3e7b167c30040518ad37a3993984843",
            "d8badcd24a4e489b83692dac0d3d8e8a",
            "01ab4a50d1cb4e4b810428cacc47cc33",
            "16fced5e75c245e3aaf9b90a1e12c70e",
            "325f251b981f4cd484cb67a0a77d9f31",
            "e5472172f7aa4a55806e39dfec56745b",
            "34b398d4010b40eca0c00193a3be877d",
            "820f869f678b44778fa4985cfd494832",
            "98557d5b7c184e5585cc393462bf968e",
            "e4f3845cf6ae4e4e962b04bd9a31942c",
            "2dcb289429e14663aeca16ae696a0018",
            "86b442b4220f4137b9d782cfc83db705",
            "de3524aeca7b4c41a044460c16897711",
            "7a921e69de4d49b6a8e09e9d1886878e",
            "ab97349ea06948db8eeac8125c901a0f",
            "18498a3349384437b2b59fbeffec1d67",
            "0c3f924672e64932b8c03f89bbfad8e8",
            "112ee3c1fe3b4729aba2bb3518a7973d",
            "c6f7b982fd7f40f7ae8000816f235200"
          ]
        },
        "id": "F5GY9voPa0av",
        "outputId": "5516d2a2-91d8-40b8-b0cd-dad0434e78c0"
      },
      "outputs": [],
      "source": [
        "# import openai\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
        "# from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "# embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s2p3rUwvOvW",
        "outputId": "f47e5661-04ae-4e12-843d-42f64a29d6ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query_result = embeddings.embed_query(\"Hello world\")\n",
        "len(query_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vySq5oI5sU5V"
      },
      "source": [
        "https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/pinecone.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "hfIpYLV-acks"
      },
      "outputs": [],
      "source": [
        "from pinecone.grpc import PineconeGRPC as Pinecone\n",
        "from pinecone import ServerlessSpec\n",
        "import os\n",
        "\n",
        "pc = Pinecone(api_key=\"7d4786c3-fc91-4605-b8e5-1f4807bcf148\")\n",
        "\n",
        "index_name = \"transformers\"\n",
        "\n",
        "index = pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=1536,\n",
        "    metric=\"cosine\",\n",
        "    spec=ServerlessSpec(\n",
        "        cloud=\"aws\",\n",
        "        region=\"us-east-1\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "93"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs = documents\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_pinecone import PineconeVectorStore  \n",
        "docsearch = PineconeVectorStore(index=index, embedding=embeddings,\n",
        "                                pinecone_api_key=\"7d4786c3-fc91-4605-b8e5-1f4807bcf148\",\n",
        "                                index_name=index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "i = docsearch.add_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='e6e22c35-b85a-4726-b06f-fcc19f9667bb', metadata={'page': 12.0, 'source': '/Users/pranavsuresh/Desktop/LANGCHAIN CHATBOT/Attention Transormers.pdf'}, page_content='.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
              " Document(id='12ed28b3-72a0-472b-9593-5d7193a8d2c2', metadata={'page': 14.0, 'source': '/Users/pranavsuresh/Desktop/LANGCHAIN CHATBOT/Attention Transormers.pdf'}, page_content='but\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15'),\n",
              " Document(id='5d4a68ac-f45f-4ffe-bd30-194f385630f9', metadata={'page': 1.0, 'source': '/Users/pranavsuresh/Desktop/LANGCHAIN CHATBOT/Attention Transormers.pdf'}, page_content='tion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.'),\n",
              " Document(id='06d7da69-f27b-4506-b86c-5f4c6d2d9d95', metadata={'page': 1.0, 'source': '/Users/pranavsuresh/Desktop/LANGCHAIN CHATBOT/Attention Transormers.pdf'}, page_content='reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been')]"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docsearch.search(\"Attention mechanism\", search_type='similarity')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_query(query,k=1):\n",
        "    matching_results=docsearch.similarity_search(query,k=k)\n",
        "    return matching_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm=OpenAI(model_name=\"gpt-3.5-turbo-instruct\",temperature=0.8, openai_api_key =  OPENAI_API_KEY)\n",
        "chain=load_qa_chain(llm,chain_type=\"stuff\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_answers(query):\n",
        "    doc_search = retrieve_query(query)\n",
        "    print(doc_search)\n",
        "    response = chain.run(input_documents = doc_search, question=query)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5r7YLpbchAD",
        "outputId": "5afe195d-df66-4a88-ff24-daeb6981f516"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='12ed28b3-72a0-472b-9593-5d7193a8d2c2', metadata={'page': 14.0, 'source': '/Users/pranavsuresh/Desktop/LANGCHAIN CHATBOT/Attention Transormers.pdf'}, page_content='but\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_similiar_docs(query,k=1,score=False):\n",
        "  if score:\n",
        "    similar_docs = docsearch.similarity_search_with_score(query,k=k)\n",
        "  else:\n",
        "    similar_docs = docsearch.similarity_search(query,k=k)\n",
        "  return similar_docs\n",
        "\n",
        "query = \"Attention Mechanism?\"\n",
        "similar_docs = get_similiar_docs(query)\n",
        "similar_docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(id='06d7da69-f27b-4506-b86c-5f4c6d2d9d95', metadata={'page': 1.0, 'source': '/Users/pranavsuresh/Desktop/LANGCHAIN CHATBOT/Attention Transormers.pdf'}, page_content='reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been')]\n",
            " Attention mechanism is a method used in natural language processing to compute a representation of a sequence by relating different positions of the sequence. It is also known as self-attention or intra-attention. This method involves reducing the number of operations while still maintaining effective resolution, and is counteracted with Multi-Head Attention.\n"
          ]
        }
      ],
      "source": [
        "our_query = \"What is attention mechanism?\"\n",
        "answer = retrieve_answers(our_query)\n",
        "print(answer)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01ab4a50d1cb4e4b810428cacc47cc33": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "067f757f43db45cc8022e8fba5c9c817": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c003ee77ba44f13a3fae4a34c1328f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c375fec344a4d109cd9d869430bb999": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c3f924672e64932b8c03f89bbfad8e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1012292cb6284d8b8b7dbe6da8089d03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1067f06e2aac41c894e6d619a1ad4e2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10dac229dff549c5b01f4e7dd9b129ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "111d8d310add429f9603fbadeac20cf5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "112ee3c1fe3b4729aba2bb3518a7973d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14c27fddbbb3405e95f03eadf4b586be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f700c80bdbe4fc2a44826fa07874e7d",
              "IPY_MODEL_a6c4eb8aa80b4adb8596ee26f6dac38e",
              "IPY_MODEL_b3e7b167c30040518ad37a3993984843"
            ],
            "layout": "IPY_MODEL_d8badcd24a4e489b83692dac0d3d8e8a"
          }
        },
        "158f3842044a4c49bfb2edceb616f98f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c9b848dea9eb4c52a447529c818b29be",
              "IPY_MODEL_ce3d41add0c14319b4d20de160a9259f",
              "IPY_MODEL_b04f1b4d17b3403894893087ec523a2b"
            ],
            "layout": "IPY_MODEL_067f757f43db45cc8022e8fba5c9c817"
          }
        },
        "1667053ee9d349dca8c2170016a431cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1686038625114cc48d8942f3d4893de9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16fced5e75c245e3aaf9b90a1e12c70e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18498a3349384437b2b59fbeffec1d67": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f700c80bdbe4fc2a44826fa07874e7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01ab4a50d1cb4e4b810428cacc47cc33",
            "placeholder": "​",
            "style": "IPY_MODEL_16fced5e75c245e3aaf9b90a1e12c70e",
            "value": "Downloading (…)7e55de9125/vocab.txt: 100%"
          }
        },
        "21215f2fa6bb4b5982ddf0198a98e9d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2402bbbdd88a48bfa4ce51a12d25a3f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "248d087c8e414fe584227814160a7f2b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25d3948447bd438d904efdfccc3d7ea0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21215f2fa6bb4b5982ddf0198a98e9d4",
            "placeholder": "​",
            "style": "IPY_MODEL_98fda223455743638835b78eabd1d4b5",
            "value": "Downloading (…)55de9125/config.json: 100%"
          }
        },
        "28f66c5017f648e886c8cb34ff96d2da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3626ac95bc944c0ab056f33f7c64ca8c",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_820df94b12cd4f2da911b89271980b9b",
            "value": 190
          }
        },
        "2a8ed199e7e840fdae0f99d378ee9644": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54b4391624a34d3fafa7cba61bffa269",
            "placeholder": "​",
            "style": "IPY_MODEL_1667053ee9d349dca8c2170016a431cb",
            "value": " 116/116 [00:00&lt;00:00, 4.00kB/s]"
          }
        },
        "2b74c3090dad4a58b8549926fad86271": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c475b6f31c144019d72a429891b1f89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2cc9dea0129542f582f6c7d9877cc1c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2dcb289429e14663aeca16ae696a0018": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18498a3349384437b2b59fbeffec1d67",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c3f924672e64932b8c03f89bbfad8e8",
            "value": 349
          }
        },
        "30e1579510dd478a8e397cc56fd3fc89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_111d8d310add429f9603fbadeac20cf5",
            "placeholder": "​",
            "style": "IPY_MODEL_2c475b6f31c144019d72a429891b1f89",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "31074b283ec14133a7c9bada6aea11ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "325f251b981f4cd484cb67a0a77d9f31": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "347d180f557d41dbad1ac2332c729271": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "34b398d4010b40eca0c00193a3be877d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3626ac95bc944c0ab056f33f7c64ca8c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3655e618833c4ac0b3b303a54c9c2c62": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3869c5c49f244907bfcdf3e862cddb34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b29d6cb19d84a13b697b90647075cb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_86075f014ff346cdb926459e1b3db711",
              "IPY_MODEL_a065eb2b40b34dd3a3ea1b65ce67002d",
              "IPY_MODEL_57142f73a2a941c9a28fa8747a3d98d0"
            ],
            "layout": "IPY_MODEL_9ff11a6f4ea445548739785086c5d669"
          }
        },
        "3d288674e7b140cf85ebff0b86662045": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d302f3a55b7469b9b6c8f29566e5917": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7c9d379e72445d5a343f08d3a56ee76",
            "placeholder": "​",
            "style": "IPY_MODEL_2b74c3090dad4a58b8549926fad86271",
            "value": "Downloading (…)_Pooling/config.json: 100%"
          }
        },
        "3ec2b16cdaca440a96d3f2316d5c8504": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d302f3a55b7469b9b6c8f29566e5917",
              "IPY_MODEL_28f66c5017f648e886c8cb34ff96d2da",
              "IPY_MODEL_f5740146f8cf490590bdcba943096ac8"
            ],
            "layout": "IPY_MODEL_10dac229dff549c5b01f4e7dd9b129ca"
          }
        },
        "3ef4dbff55a942efaf0f6582baf4c8da": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "411eb90269d442f09ee052032cee09a5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4974341254624b059e9b4c9739c4acd4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49a2b91eb9504c2781fb7fce431cbd18": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49b6d94c2eab42a99b695419074f8958": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b33b4a1c9344044a6fc5d7e00438f5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f633d3909424c8d9ea3c353f69b2534",
            "placeholder": "​",
            "style": "IPY_MODEL_1012292cb6284d8b8b7dbe6da8089d03",
            "value": " 466k/466k [00:00&lt;00:00, 2.27MB/s]"
          }
        },
        "4bf75d64c602432a9c9ccf62f4454f5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_deeea70f2a654af58073c63429472657",
            "max": 90888945,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b867fa2c3e424ad388d566a5bd5c580e",
            "value": 90888945
          }
        },
        "4c31196cd8684de283a1544fac8b0c94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4974341254624b059e9b4c9739c4acd4",
            "placeholder": "​",
            "style": "IPY_MODEL_c974169e945e48528316b6d33902b8f8",
            "value": " 90.9M/90.9M [00:01&lt;00:00, 92.5MB/s]"
          }
        },
        "4d8df2c34fca4eeeb87670fe25d29a8a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f08da63072a4898b1d521fb4a876f0d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "549062259cb64f81a979d8700345f8e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b1dfa23ff85a4f2f828c900476ef0804",
              "IPY_MODEL_adc53ec8fb6d48b59998093ee1c52b66",
              "IPY_MODEL_2a8ed199e7e840fdae0f99d378ee9644"
            ],
            "layout": "IPY_MODEL_dbcfcc38507c4df8b28599c3e7420350"
          }
        },
        "54b4391624a34d3fafa7cba61bffa269": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56f90d69416d4aa1a8b9c8daa016ba6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb735781bd5e406b89bafdb01c0270a6",
            "placeholder": "​",
            "style": "IPY_MODEL_98f220c88b1b494690b61d891c40dd4c",
            "value": " 1.18k/1.18k [00:00&lt;00:00, 68.5kB/s]"
          }
        },
        "57142f73a2a941c9a28fa8747a3d98d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_411eb90269d442f09ee052032cee09a5",
            "placeholder": "​",
            "style": "IPY_MODEL_49b6d94c2eab42a99b695419074f8958",
            "value": " 10.6k/10.6k [00:00&lt;00:00, 406kB/s]"
          }
        },
        "59b6ac5d644048faa53103b561462eaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cb8d31e4ea7496cb1b7be636ff1f0f6",
            "placeholder": "​",
            "style": "IPY_MODEL_8101ef7678fb4535a82401039495efa3",
            "value": "Downloading (…)e9125/tokenizer.json: 100%"
          }
        },
        "59baeec6d783438e8a798959fc6885c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f059f038342e430586d71dec52194793",
              "IPY_MODEL_a437b3c6371f4303bcc8b5245fac08ea",
              "IPY_MODEL_56f90d69416d4aa1a8b9c8daa016ba6a"
            ],
            "layout": "IPY_MODEL_e0ab43141a5d44278cba23259229db0d"
          }
        },
        "5c48fe79b60340e5bd5b55740cf1b732": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c9496b5fa4540369265d9c11c3d6ca9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cb8d31e4ea7496cb1b7be636ff1f0f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dae446e7a1d4b34962de9ac4456e014": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e378e4ea15646b2b55f67ece88a230b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "637ee292775f4d8e9e8e9fbeddedc9ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d0545256e1b4893b50623facbfb5696",
            "max": 13156,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2402bbbdd88a48bfa4ce51a12d25a3f2",
            "value": 13156
          }
        },
        "63ecc46d58c84800be60d0bafe968ad1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67cd54af507240679e1a3596685d05f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ef4dbff55a942efaf0f6582baf4c8da",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_347d180f557d41dbad1ac2332c729271",
            "value": 112
          }
        },
        "698cbb3b46724f079132d9108e89e305": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69c672cd74674eda925ab87e41360ae2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc5fb63da52b4d6ea5279a371780c0e9",
            "placeholder": "​",
            "style": "IPY_MODEL_3869c5c49f244907bfcdf3e862cddb34",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "6a41612156be4c688425dd6a8e2d425c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6bd26ba594fd4f0e87c4d9659483b233": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d0545256e1b4893b50623facbfb5696": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f633d3909424c8d9ea3c353f69b2534": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71723a2d3a384acba891a37f2e5f48c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79029f8c0c9f4376af59bca97fc5bc83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_69c672cd74674eda925ab87e41360ae2",
              "IPY_MODEL_8e89291c024044f3a21bfffb45ae1f4e",
              "IPY_MODEL_b5873d590a154464894239be04937aea"
            ],
            "layout": "IPY_MODEL_c4585a9ad4e0444f881a8c262766eace"
          }
        },
        "7a921e69de4d49b6a8e09e9d1886878e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b31d104d2fe4170849de99e81d400f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_822d81dc5fdf49fe9f950bf839a7de2f",
            "placeholder": "​",
            "style": "IPY_MODEL_aed9acfd105c44f09c2b9675457a0689",
            "value": " 612/612 [00:00&lt;00:00, 25.6kB/s]"
          }
        },
        "7c0b9ac6cd5f4498b810f0b0beb802bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "802c1c2babca4409b7972b0cea66c78f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8101ef7678fb4535a82401039495efa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "820df94b12cd4f2da911b89271980b9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "820f869f678b44778fa4985cfd494832": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "822d81dc5fdf49fe9f950bf839a7de2f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8454c711aa4c42f1992bd81597293808": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85e89982b0df473dba7649e3d464bae7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86075f014ff346cdb926459e1b3db711": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0318f9195624f0987b07d8fa7b10a6b",
            "placeholder": "​",
            "style": "IPY_MODEL_2cc9dea0129542f582f6c7d9877cc1c8",
            "value": "Downloading (…)7e55de9125/README.md: 100%"
          }
        },
        "86b442b4220f4137b9d782cfc83db705": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_112ee3c1fe3b4729aba2bb3518a7973d",
            "placeholder": "​",
            "style": "IPY_MODEL_c6f7b982fd7f40f7ae8000816f235200",
            "value": " 349/349 [00:00&lt;00:00, 9.86kB/s]"
          }
        },
        "8d27d15788a14d90ba20be096a46c442": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e89291c024044f3a21bfffb45ae1f4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49a2b91eb9504c2781fb7fce431cbd18",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a530fc9783be4a7fb2dae89166d5df2b",
            "value": 350
          }
        },
        "919e05f8e0af4b25b55986409dc1f2ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "979a4dffb5364142b3c0142ef5ba5680": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98557d5b7c184e5585cc393462bf968e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4f3845cf6ae4e4e962b04bd9a31942c",
              "IPY_MODEL_2dcb289429e14663aeca16ae696a0018",
              "IPY_MODEL_86b442b4220f4137b9d782cfc83db705"
            ],
            "layout": "IPY_MODEL_de3524aeca7b4c41a044460c16897711"
          }
        },
        "98f220c88b1b494690b61d891c40dd4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98fda223455743638835b78eabd1d4b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fb430b5ba8e4003a95c43c00d4e2d00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_25d3948447bd438d904efdfccc3d7ea0",
              "IPY_MODEL_cabb9f7137f042859b32af306f34549e",
              "IPY_MODEL_7b31d104d2fe4170849de99e81d400f4"
            ],
            "layout": "IPY_MODEL_4f08da63072a4898b1d521fb4a876f0d"
          }
        },
        "9ff11a6f4ea445548739785086c5d669": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a065eb2b40b34dd3a3ea1b65ce67002d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_248d087c8e414fe584227814160a7f2b",
            "max": 10610,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c003ee77ba44f13a3fae4a34c1328f6",
            "value": 10610
          }
        },
        "a1a5aa9bc8534451aa449405fe61d912": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2058ae051734f299da6e4aa329a4844": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc80761af4bd4506a7dce043b5bb1dcb",
            "max": 466247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aada1ceacb4949c290acdf68b561da8c",
            "value": 466247
          }
        },
        "a22492795b384194879c071a2f452233": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a437b3c6371f4303bcc8b5245fac08ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_979a4dffb5364142b3c0142ef5ba5680",
            "max": 1175,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5e378e4ea15646b2b55f67ece88a230b",
            "value": 1175
          }
        },
        "a530fc9783be4a7fb2dae89166d5df2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a6c4eb8aa80b4adb8596ee26f6dac38e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_325f251b981f4cd484cb67a0a77d9f31",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e5472172f7aa4a55806e39dfec56745b",
            "value": 231508
          }
        },
        "aada1ceacb4949c290acdf68b561da8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ab97349ea06948db8eeac8125c901a0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "adc53ec8fb6d48b59998093ee1c52b66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_919e05f8e0af4b25b55986409dc1f2ef",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e1f8bde65bc84ce99902ce5de17ccb0e",
            "value": 116
          }
        },
        "aed9acfd105c44f09c2b9675457a0689": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0318f9195624f0987b07d8fa7b10a6b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b04f1b4d17b3403894893087ec523a2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8d53bc71965475498f147a5f8a908b7",
            "placeholder": "​",
            "style": "IPY_MODEL_a22492795b384194879c071a2f452233",
            "value": " 53.0/53.0 [00:00&lt;00:00, 1.73kB/s]"
          }
        },
        "b0cfa0cf699c457b9b8d8bacb9b98794": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1dfa23ff85a4f2f828c900476ef0804": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7b2af652a0c41278e62fd447331acd6",
            "placeholder": "​",
            "style": "IPY_MODEL_6bd26ba594fd4f0e87c4d9659483b233",
            "value": "Downloading (…)ce_transformers.json: 100%"
          }
        },
        "b21ab6efb2a846e59fb89ae16bc3c9c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1a5aa9bc8534451aa449405fe61d912",
            "placeholder": "​",
            "style": "IPY_MODEL_802c1c2babca4409b7972b0cea66c78f",
            "value": " 13.2k/13.2k [00:00&lt;00:00, 654kB/s]"
          }
        },
        "b3e7b167c30040518ad37a3993984843": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34b398d4010b40eca0c00193a3be877d",
            "placeholder": "​",
            "style": "IPY_MODEL_820f869f678b44778fa4985cfd494832",
            "value": " 232k/232k [00:00&lt;00:00, 11.7MB/s]"
          }
        },
        "b4da1de31cc84107ab3429bc62037381": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df1372bd5cb040ac85141d5f11e13343",
            "placeholder": "​",
            "style": "IPY_MODEL_6a41612156be4c688425dd6a8e2d425c",
            "value": " 112/112 [00:00&lt;00:00, 4.47kB/s]"
          }
        },
        "b5873d590a154464894239be04937aea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c9496b5fa4540369265d9c11c3d6ca9",
            "placeholder": "​",
            "style": "IPY_MODEL_698cbb3b46724f079132d9108e89e305",
            "value": " 350/350 [00:00&lt;00:00, 14.0kB/s]"
          }
        },
        "b7d02009e2074ba399cf2bb47946f083": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b867fa2c3e424ad388d566a5bd5c580e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb735781bd5e406b89bafdb01c0270a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb8b910963f642829f20d1683a067abf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc5fb63da52b4d6ea5279a371780c0e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc80761af4bd4506a7dce043b5bb1dcb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c254856a8a524e70a54fcfc1e389306d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c35cdfd8fac24ad0b5f07e76004525cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df9cf009019849839111ad0f22195e3d",
            "placeholder": "​",
            "style": "IPY_MODEL_0c375fec344a4d109cd9d869430bb999",
            "value": " 39.3k/39.3k [00:00&lt;00:00, 608kB/s]"
          }
        },
        "c4585a9ad4e0444f881a8c262766eace": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c612d8951e2c4e279cf263cf2b2b70bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c62e0de5b14e403a88840d74fd642966": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6f7b982fd7f40f7ae8000816f235200": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7b2af652a0c41278e62fd447331acd6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8d53bc71965475498f147a5f8a908b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c974169e945e48528316b6d33902b8f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9b848dea9eb4c52a447529c818b29be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1686038625114cc48d8942f3d4893de9",
            "placeholder": "​",
            "style": "IPY_MODEL_85e89982b0df473dba7649e3d464bae7",
            "value": "Downloading (…)nce_bert_config.json: 100%"
          }
        },
        "cabb9f7137f042859b32af306f34549e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d8df2c34fca4eeeb87670fe25d29a8a",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c0b9ac6cd5f4498b810f0b0beb802bb",
            "value": 612
          }
        },
        "cae253847f634c359b844bbce9790286": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c48fe79b60340e5bd5b55740cf1b732",
            "placeholder": "​",
            "style": "IPY_MODEL_31074b283ec14133a7c9bada6aea11ce",
            "value": "Downloading (…)9125/train_script.py: 100%"
          }
        },
        "ce3d41add0c14319b4d20de160a9259f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0cfa0cf699c457b9b8d8bacb9b98794",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71723a2d3a384acba891a37f2e5f48c8",
            "value": 53
          }
        },
        "cf85d1be377e47fdb11fd8f654a2d2c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8154cdd6da945be90467e689f53c21f",
              "IPY_MODEL_4bf75d64c602432a9c9ccf62f4454f5c",
              "IPY_MODEL_4c31196cd8684de283a1544fac8b0c94"
            ],
            "layout": "IPY_MODEL_3655e618833c4ac0b3b303a54c9c2c62"
          }
        },
        "d7c9d379e72445d5a343f08d3a56ee76": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8badcd24a4e489b83692dac0d3d8e8a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbcfcc38507c4df8b28599c3e7420350": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de3524aeca7b4c41a044460c16897711": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "deafe043ea334e60ad5dd87fcb9d3e51": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "deeea70f2a654af58073c63429472657": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df1372bd5cb040ac85141d5f11e13343": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df9cf009019849839111ad0f22195e3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0ab43141a5d44278cba23259229db0d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1f8bde65bc84ce99902ce5de17ccb0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e2655b00362a46238665bb973c18bff8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59b6ac5d644048faa53103b561462eaa",
              "IPY_MODEL_a2058ae051734f299da6e4aa329a4844",
              "IPY_MODEL_4b33b4a1c9344044a6fc5d7e00438f5d"
            ],
            "layout": "IPY_MODEL_63ecc46d58c84800be60d0bafe968ad1"
          }
        },
        "e419f8bfded94d9b8c2b326af0dc6e70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_30e1579510dd478a8e397cc56fd3fc89",
              "IPY_MODEL_67cd54af507240679e1a3596685d05f7",
              "IPY_MODEL_b4da1de31cc84107ab3429bc62037381"
            ],
            "layout": "IPY_MODEL_c62e0de5b14e403a88840d74fd642966"
          }
        },
        "e4f3845cf6ae4e4e962b04bd9a31942c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a921e69de4d49b6a8e09e9d1886878e",
            "placeholder": "​",
            "style": "IPY_MODEL_ab97349ea06948db8eeac8125c901a0f",
            "value": "Downloading (…)5de9125/modules.json: 100%"
          }
        },
        "e5472172f7aa4a55806e39dfec56745b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8154cdd6da945be90467e689f53c21f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7a527e85b5246fc8eeb73b5cdc6f128",
            "placeholder": "​",
            "style": "IPY_MODEL_8454c711aa4c42f1992bd81597293808",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "ed1df9840ebb43edbd6d95b4fa1e3e04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f91e3c066389464b986306eaf5db61c8",
            "placeholder": "​",
            "style": "IPY_MODEL_8d27d15788a14d90ba20be096a46c442",
            "value": "Downloading (…)125/data_config.json: 100%"
          }
        },
        "ef5fbedad6724d8bb2c95a294c047880": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cae253847f634c359b844bbce9790286",
              "IPY_MODEL_637ee292775f4d8e9e8e9fbeddedc9ab",
              "IPY_MODEL_b21ab6efb2a846e59fb89ae16bc3c9c2"
            ],
            "layout": "IPY_MODEL_deafe043ea334e60ad5dd87fcb9d3e51"
          }
        },
        "f059f038342e430586d71dec52194793": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb8b910963f642829f20d1683a067abf",
            "placeholder": "​",
            "style": "IPY_MODEL_1067f06e2aac41c894e6d619a1ad4e2b",
            "value": "Downloading (…)e9125/.gitattributes: 100%"
          }
        },
        "f3285ec1908d48f295b63b7349967959": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed1df9840ebb43edbd6d95b4fa1e3e04",
              "IPY_MODEL_fd08f2fb701d456b89050272ca9bd20d",
              "IPY_MODEL_c35cdfd8fac24ad0b5f07e76004525cc"
            ],
            "layout": "IPY_MODEL_c254856a8a524e70a54fcfc1e389306d"
          }
        },
        "f5740146f8cf490590bdcba943096ac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5dae446e7a1d4b34962de9ac4456e014",
            "placeholder": "​",
            "style": "IPY_MODEL_b7d02009e2074ba399cf2bb47946f083",
            "value": " 190/190 [00:00&lt;00:00, 9.38kB/s]"
          }
        },
        "f7a527e85b5246fc8eeb73b5cdc6f128": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f91e3c066389464b986306eaf5db61c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd08f2fb701d456b89050272ca9bd20d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d288674e7b140cf85ebff0b86662045",
            "max": 39265,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c612d8951e2c4e279cf263cf2b2b70bc",
            "value": 39265
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
